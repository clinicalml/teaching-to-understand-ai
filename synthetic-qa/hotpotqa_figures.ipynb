{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "hotpotqa_figures.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "UP1CACu3irWL",
        "5WSnSiY1dF2E",
        "AK9JLhs-97wO",
        "UpcZrrMcwNWP",
        "VYBacIDTwNWV",
        "k-L_wImhCL-o",
        "rBHrWz39CjhT",
        "L3oEF1rEnXKB",
        "C2Ch46jJttQL",
        "MSkPt-33wSAp",
        "ZosFDIVUqNbA",
        "7VhWuzn6CYyT",
        "oTR95IRRnEM8",
        "rHmJJilnnEM_",
        "lOTIYC9C6K15"
      ],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlRavDDk6maE"
      },
      "source": [
        "This notebook can be run immediately on google colab to replicate figure 2 and table 1 in the paper. We recommend running for only 1 trial, as running for 10 trials can take significantly longer with the code version below not optimized for parallel computations. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP1CACu3irWL"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZx4UvXO5OLg"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentence_transformers\n",
        "!pip install datasets\n",
        "!pip install sentencepiece\n",
        "!pip install scikit-learn-extra\n",
        "!pip install lime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "meSB2v0g0EH2"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRNFoaJXvO4x"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import argparse\n",
        "from __future__ import print_function\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import time\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "from tqdm import tqdm\n",
        "import torch.utils.data as data\n",
        "from torchvision.datasets.utils import download_url, check_integrity\n",
        "import sys\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "from sklearn.gaussian_process.kernels import RBF\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from scipy.stats import multivariate_normal\n",
        "import  scipy.stats as st\n",
        "from matplotlib import cm\n",
        "import torch.optim as optim\n",
        "from __future__ import print_function\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW, AutoModel\n",
        "import nltk\n",
        "from torch.utils.data import DataLoader\n",
        "import pickle\n",
        "import spacy\n",
        "from gensim import corpora, models, similarities\n",
        "import gensim\n",
        "from spacy.lang.en import English\n",
        "from nltk.corpus import wordnet as wn\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.cluster import KMeans\n",
        "import pickle\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "import matplotlib\n",
        "from sklearn.metrics.pairwise import rbf_kernel\n",
        "import copy\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "plt.rc('text', usetex=False)\n",
        "plt.rc('font', family='serif')\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stopwords = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WSnSiY1dF2E"
      },
      "source": [
        "# HotpotQA\n",
        "Load the hotpotQA dataset with our modifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E0eakH8dHfv"
      },
      "source": [
        "from datasets import load_dataset\n",
        "DISTRACTORS_PARS_LEN = 0\n",
        "dataset = load_dataset(\"hotpot_qa\", 'distractor')\n",
        "train_dataset = dataset['train']\n",
        "validation_dataset = dataset['validation']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqzhee23lkrz"
      },
      "source": [
        "def get_token_lenght(context, question):\n",
        "    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    return len(inputs['input_ids'][0])\n",
        "\n",
        "def remove_distractors(example, n_distractors = 0):\n",
        "    all_pars = example['context']['title']\n",
        "    gold_pars = list(set(example['supporting_facts']['title']))\n",
        "    distractor_pars = list(set(all_pars) - set(gold_pars))\n",
        "    # get indices to keep from disractors\n",
        "    if len(distractor_pars) == 0 or n_distractors == 0:\n",
        "        distract_indices = []\n",
        "    else:\n",
        "        distract_indices = random.sample(range(len(distractor_pars)), n_distractors)\n",
        "    distractor_pars = [distractor_pars[idx] for idx in distract_indices]\n",
        "    keep_pars = gold_pars + distractor_pars\n",
        "    keep_pars_indices = [all_pars.index(keep_par) for keep_par in keep_pars]\n",
        "    example['context']['title'] = [example['context']['title'][idx] for idx in keep_pars_indices]\n",
        "    example['context']['sentences'] = [example['context']['sentences'][idx] for idx in keep_pars_indices]\n",
        "    \n",
        "    sentences_par = [\"\".join(example['context']['sentences'][idx]) for idx in range(len(example['context']['sentences']))]\n",
        "    example['intros'] = \" \".join([example['context']['sentences'][idx][0] for idx in range(len(example['context']['sentences']))])\n",
        "    example['passage'] = \" #$ \".join(sentences_par)\n",
        "    return example\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unz9KJ9qofO3"
      },
      "source": [
        "train_dataset = train_dataset.map(remove_distractors)\n",
        "validation_dataset = validation_dataset.map(remove_distractors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1AI46NXwNV-"
      },
      "source": [
        "all_indices = set(list(range(len(validation_dataset))))\n",
        "yes_no_indices = set([i  for i in range(len(validation_dataset)) if validation_dataset[i]['answer'] in [\"yes\",\"no\"] ])\n",
        "free_indices_val = all_indices - yes_no_indices\n",
        "all_indices = set(list(range(len(train_dataset))))\n",
        "yes_no_indices_train = set([i  for i in range(len(train_dataset)) if (train_dataset[i]['answer'] in [\"yes\",\"no\"]) or (train_dataset[i]['level'] in ['easy','medium']) ])\n",
        "free_indices_train = all_indices - yes_no_indices_train\n",
        "print(len(free_indices_val))\n",
        "print(len(free_indices_train))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6fP4_P2wNWA"
      },
      "source": [
        "hard_dataset = [train_dataset[i] for i in free_indices_train ]\n",
        "hard_dataset += [validation_dataset[i] for i in free_indices_val]\n",
        "dataset_indices =  set(list(range(len(hard_dataset))))\n",
        "VAL_START = len([train_dataset[i] for i in free_indices_train ])\n",
        "len(hard_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AK9JLhs-97wO"
      },
      "source": [
        "## Utils\n",
        "Utilities for evaluating QA models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40Mqih9Z8rTx"
      },
      "source": [
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_truth_pred(truths, preds):\n",
        "    '''\n",
        "    truths, preds: matched arrays of ground truth answers and predictions\n",
        "    '''\n",
        "    f1 = exact_match = total = 0\n",
        "    for i in range(len(truths)):\n",
        "        total += 1\n",
        "        if truths[i] in ['yes', \"no\"]:\n",
        "            continue\n",
        "        ground_truths = [truths[i]]\n",
        "        prediction = preds[i]\n",
        "        exact_match += metric_max_over_ground_truths(\n",
        "            exact_match_score, prediction, ground_truths)\n",
        "        f1 += metric_max_over_ground_truths(\n",
        "            f1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / (total+ 0.00000000001)\n",
        "    f1 = 100.0 * f1 / (total+ 0.00000000001)\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1}\n",
        "\n",
        "\n",
        "def get_answer( model, tokenizer, context, question):\n",
        "    # 1. TOKENIZE THE INPUT\n",
        "    # note: if you don't include return_tensors='pt' you'll get a list of lists which is easier for\n",
        "    # exploration but you cannot feed that into a model.\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(device)\n",
        "    # 2. OBTAIN MODEL SCORES\n",
        "    # the AutoModelForQuestionAnswering class includes a span predictor on top of the model.\n",
        "    # the model returns answer start and end scores for each word in the text\n",
        "    answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)\n",
        "    answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
        "    # 3. GET THE ANSWER SPAN\n",
        "    # once we have the most likely start and end tokens, we grab all the tokens between them\n",
        "    # and convert tokens back to words!\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
        "    return answer\n",
        "# https://huggingface.co/transformers/migration.html\n",
        "def model_evaluation(model, tokenizer, questions, contexts, answers, to_print = False):\n",
        "    preds = []\n",
        "    my_list = list(range(len(questions)))\n",
        "    with tqdm(total=len(my_list)) as pbar:\n",
        "        for ex in range(len(questions)):\n",
        "            answer = get_answer(model, tokenizer, contexts[ex],questions[ex])\n",
        "            preds.append(answer)\n",
        "            if ex % 100 == 0 and to_print:\n",
        "                print(\"context \" +contexts[ex] )\n",
        "                print(\"quest \" +questions[ex] )\n",
        "                print(\"truth \" +answers[ex]['text'] )\n",
        "                print(\"pred \" + answer)\n",
        "            pbar.update(1)\n",
        "    truths = [answers[i]['text'] for i in range(len(answers))]\n",
        "    scores = evaluate_truth_pred(truths, preds)\n",
        "    print(scores)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def kernel_similarity(x, y):\n",
        "        kernel_dist = rbf_kernel(x.reshape(1, -1),y.reshape(1, -1))\n",
        "        return kernel_dist[0][0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpcZrrMcwNWP"
      },
      "source": [
        "# FakeAI model\n",
        "The simulatedAI model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbyNu1-WwNWQ"
      },
      "source": [
        "class FakeAI:\n",
        "    r\"\"\"\n",
        "    FakeAI that is incorrect in random clusters in the input space\n",
        "    \n",
        "    Args:\n",
        "        train_passages: list of embedded passages\n",
        "        train_questions: list of embedded questions\n",
        "        n_clus_p: number of centroids in kmeans for passages\n",
        "        n_clus_q: number of centroids in kmeans for questions\n",
        "        clust_err_p: clusters where AI makes error on passages\n",
        "        clust_err_q: clusters where AI makes error on questions\n",
        "    \"\"\"\n",
        "    def __init__(self, train_passages, train_questions, n_clus_p, n_clus_q, beta_params = (1,1), kmeans = None, err_p = None):\n",
        " \n",
        "        self.train_passages = train_passages\n",
        "        self.train_questions = train_questions\n",
        "        self.n_clus_p = n_clus_p\n",
        "        self.n_clus_q = n_clus_q\n",
        "        if err_p == None:\n",
        "            self.clust_err_p = np.random.beta(beta_params[0],beta_params[1],self.n_clus_p)\n",
        "        else:\n",
        "            self.clust_err_p = err_p\n",
        "        self.clust_err_q = np.random.beta(0.2,0.8,self.n_clus_p)#np.random.rand(self.n_clus_q)#/2 + 0.5\n",
        "        if kmeans == None:\n",
        "            self.build_kmeans()\n",
        "        else:\n",
        "            self.kmeans_pass = kmeans\n",
        "            self.kmeans_quest = kmeans\n",
        "\n",
        "        \n",
        "    def build_kmeans(self):\n",
        "        # Builds kmeans for passages and questions\n",
        "        self.kmeans_quest = KMeans(n_clusters=self.n_clus_q, max_iter = 100000).fit(self.train_questions)\n",
        "        self.kmeans_pass = KMeans(n_clusters=self.n_clus_p, max_iter = 100000).fit(self.train_passages)\n",
        "\n",
        "    def predict_right_wrong(self, passages, questions):\n",
        "        '''\n",
        "        Args:\n",
        "            passages: list of embedded passages\n",
        "            questions: list of embedded questions (same size as passages)\n",
        "        Returns:\n",
        "            preds: binary array indicating if AI is right (1) or wrong (0)\n",
        "        '''\n",
        "        clusts_p = self.kmeans_pass.predict(passages)\n",
        "        clusts_q = self.kmeans_quest.predict(questions)\n",
        "        preds = []\n",
        "        for i in range(len(clusts_p)):\n",
        "            coin = random.random() # random number between [0,1]\n",
        "            if coin >= self.clust_err_p[clusts_p[i]]:\n",
        "                preds.append(0)\n",
        "            else:\n",
        "                preds.append(1)\n",
        "        return preds\n",
        "\n",
        "    \n",
        "    def predict_raw_probas(self, passages, questions):\n",
        "        '''\n",
        "        Args:\n",
        "            passages: list of embedded passages\n",
        "            questions: list of embedded questions (same size as passages)\n",
        "        Returns:\n",
        "            preds: binary array indicating if AI is right (1) or wrong (0)\n",
        "        '''\n",
        "        clusts_p = self.kmeans_pass.predict(passages)\n",
        "        clusts_q = self.kmeans_quest.predict(questions)\n",
        "        preds = []\n",
        "        for i in range(len(clusts_p)):\n",
        "            preds.append(self.clust_err_p[clusts_p[i]])\n",
        "        return preds\n",
        "    \n",
        "    def prior_rejector(self, passages, questions, epsilon_reject):\n",
        "        '''\n",
        "        rejector defined as : 1{prob_of_correct <= epsilon_reject}\n",
        "        '''\n",
        "        clusts_p = self.kmeans_pass.predict(passages)\n",
        "        clusts_q = self.kmeans_quest.predict(questions)\n",
        "        preds = []\n",
        "        for i in range(len(clusts_p)):\n",
        "            coin = random.random() # random number between [0,1]\n",
        "            if  self.clust_err_p[clusts_p[i]] <= epsilon_reject:\n",
        "                preds.append(1)\n",
        "            else:\n",
        "                preds.append(0)\n",
        "        return preds\n",
        "\n",
        "    def predict_proba(self, raw_passages):\n",
        "        embed_ps = model.encode(raw_passages)\n",
        "        embed_ps = [embed_ps[i] for i in range(len(embed_ps))]\n",
        "        clusts_p = self.kmeans_pass.predict(embed_ps)\n",
        "        preds = []\n",
        "        for i in range(len(clusts_p)):\n",
        "            coin = random.random() # random number between [0,1]\n",
        "            if coin >= self.clust_err_p[clusts_p[i]]:\n",
        "                preds.append([0.0,1.0])\n",
        "            else:\n",
        "                preds.append([1.0,0.0])\n",
        "        return np.asarray(preds)\n",
        "\n",
        "    def predict_proba_encoded(self, passages):\n",
        "        clusts_p = self.kmeans_pass.predict(np.array(passages, dtype=np.float32))\n",
        "        preds = []\n",
        "        for i in range(len(clusts_p)):\n",
        "            coin = random.random() # random number between [0,1]\n",
        "            if coin >= self.clust_err_p[clusts_p[i]]:\n",
        "                preds.append([0.0,1.0])\n",
        "            else:\n",
        "                preds.append([1.0,0.0])\n",
        "        return np.asarray(preds)\n",
        "\n",
        "\n",
        "def is_a_stopword(feature, weight):\n",
        "    split_words = feature.split(' ')\n",
        "    for word in split_words:\n",
        "        if word in stopwords:\n",
        "            return False\n",
        "    return True\n",
        "'''\n",
        "def get_highlighted_p(ai_model, paragraph):\n",
        "    '''\n",
        "    from https://eli5.readthedocs.io/en/latest/tutorials/black-box-text-classifiers.html#lime-tutorial\n",
        "    '''\n",
        "    \n",
        "    te = TextExplainer(random_state=42)\n",
        "    te.fit(paragraph, ai_model.predict_proba)\n",
        "    show_pred = te.show_prediction(target_names=['correct','false'], top=(8,0), feature_filter = is_a_stopword)\n",
        "    correct = False\n",
        "    if show_pred.data.find(\"y=correct\") != -1:\n",
        "        correct = True\n",
        "    b = show_pred.data.split('<p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">')\n",
        "    to_show = '<p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">' +b[-1]\n",
        "    if not correct:\n",
        "        to_show = to_show.replace(\"hsl(120,\",\"hsl(0,\")\n",
        "    return to_show\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jN6a89iFAk25"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1') #distilbert-base-nli-stsb-mean-tokens\n",
        "# https://www.sbert.net/docs/pretrained_models.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYBacIDTwNWV"
      },
      "source": [
        "# Teaching Prelims\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-L_wImhCL-o"
      },
      "source": [
        "## Human classes and utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYUTZTA3wNWW"
      },
      "source": [
        "class HumanLearner:\n",
        "    def __init__(self, kernel):\n",
        "        '''\n",
        "        kernel: function that takes two inputs and returns a similarity\n",
        "        prior rejector: returns rejector\n",
        "        '''\n",
        "        self.teaching_set = []\n",
        "        self.kernel = kernel\n",
        "    def predict(self, xs, prior_rejector_preds, to_print = False):\n",
        "        '''\n",
        "        xs: expected array of inputs\n",
        "        '''\n",
        "        preds = []\n",
        "        idx = 0\n",
        "        used_posterior = 0 \n",
        "        if to_print:\n",
        "            print(\"-- Human making reject predictions --\")\n",
        "            with tqdm(total=len(xs)) as pbar:\n",
        "                for x in xs:\n",
        "                    ball_at_x = []\n",
        "                    similarities = rbf_kernel(x.reshape(1,-1), np.asarray([self.teaching_set[kk][0] for kk in range(len(self.teaching_set))]))[0]\n",
        "                    for i in range(len(self.teaching_set)):\n",
        "                        similarity = similarities[i]\n",
        "                        if similarity >=  self.teaching_set[i][2]:\n",
        "                            ball_at_x.append(self.teaching_set[i])\n",
        "                    if len(ball_at_x) == 0: \n",
        "                        # use prior rejector\n",
        "                        preds.append(prior_rejector_preds[idx])\n",
        "                    else:\n",
        "                        used_posterior += 1\n",
        "                        ball_similarities = rbf_kernel(x.reshape(1,-1), np.asarray([ball_at_x[kk][0] for kk in range(len(ball_at_x))]))[0]\n",
        "                        normalization = np.sum([ball_similarities[i] for i in range(len(ball_at_x))])\n",
        "                        score_one = np.sum([ball_similarities[i]*ball_at_x[i][1] for i in range(len(ball_at_x))])\n",
        "                        pred = score_one / normalization\n",
        "                        if pred >= 0.5:\n",
        "                            preds.append(1)\n",
        "                        else:\n",
        "                            preds.append(0)\n",
        "                    idx += 1\n",
        "                    pbar.update(1)\n",
        "        else:\n",
        "            for x in xs:\n",
        "                ball_at_x = []\n",
        "                similarities = rbf_kernel(x.reshape(1,-1), np.asarray([self.teaching_set[kk][0] for kk in range(len(self.teaching_set))]))[0]\n",
        "                for i in range(len(self.teaching_set)):\n",
        "                    similarity = similarities[i]\n",
        "                    if similarity >=  self.teaching_set[i][2]:\n",
        "                        ball_at_x.append(self.teaching_set[i])\n",
        "                if len(ball_at_x) == 0: \n",
        "                    # use prior rejector\n",
        "                    preds.append(prior_rejector_preds[idx])\n",
        "                else:\n",
        "                    used_posterior += 1\n",
        "                    ball_similarities = rbf_kernel(x.reshape(1,-1), np.asarray([ball_at_x[kk][0] for kk in range(len(ball_at_x))]))[0]\n",
        "                    normalization = np.sum([ball_similarities[i] for i in range(len(ball_at_x))])\n",
        "                    score_one = np.sum([ball_similarities[i]*ball_at_x[i][1] for i in range(len(ball_at_x))])\n",
        "                    pred = score_one / normalization\n",
        "                    if pred >= 0.5:\n",
        "                        preds.append(1)\n",
        "                    else:\n",
        "                        preds.append(0)\n",
        "                idx += 1\n",
        "        if to_print:\n",
        "            print(f'Used posterior {used_posterior/len(xs)*100:.2f}')\n",
        "        return preds\n",
        "\n",
        "    def add_to_teaching(self, teaching_example):\n",
        "        '''\n",
        "        teaching_example: (x, label, gamma)\n",
        "        '''\n",
        "        self.teaching_set.append(teaching_example)\n",
        "\n",
        "    def remove_last_teaching_item(self):\n",
        "        self.teaching_set = self.teaching_set[:-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4329rtlwNWX"
      },
      "source": [
        "def compute_predictions_humanai(hum_preds, hum_rejector, ai_preds, data_x):\n",
        "    '''\n",
        "    hum_preds: array of human predictions\n",
        "    ai_preds: array of AI predictions\n",
        "    hum_rejector: HumanLearner\n",
        "    data_x: array of inputs\n",
        "\n",
        "    Returns array of final predictions and deferalls\n",
        "    '''\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        reject_decisions = hum_rejector(data_x)\n",
        "        for i in range(len(data_x)):\n",
        "            if reject_decisions[i] == 1:\n",
        "                # defer\n",
        "                predictions.append(ai_preds[i])\n",
        "            else:\n",
        "                predictions.append(hum_preds[i])\n",
        "    return predictions, reject_decisions\n",
        "\n",
        "def get_metrics(preds, truths):\n",
        "    # custom for each use case\n",
        "    return evaluate_truth_pred(truths, preds)\n",
        "\n",
        "def compute_metrics(human_preds, ai_preds, reject_decisions, truths, to_print = False):\n",
        "    coverage = 1 - np.sum(reject_decisions)/len(reject_decisions)\n",
        "    humanai_preds = []\n",
        "    human_preds_sys = []\n",
        "    truths_human = []\n",
        "    ai_preds_sys = []\n",
        "    truths_ai = []\n",
        "    for i in range(len(reject_decisions)):\n",
        "        if reject_decisions[i] == 1:\n",
        "            humanai_preds.append(ai_preds[i])\n",
        "            ai_preds_sys.append(ai_preds[i])\n",
        "            truths_ai.append(truths[i])\n",
        "        else:\n",
        "            humanai_preds.append(human_preds[i])\n",
        "            human_preds_sys.append(human_preds[i])\n",
        "            truths_human.append(truths[i])\n",
        "    humanai_metrics = get_metrics(humanai_preds, truths)\n",
        "    human_metrics = get_metrics(human_preds_sys, truths_human)\n",
        "    ai_metrics = get_metrics(ai_preds_sys, truths_ai)\n",
        "    if to_print:\n",
        "        print(f'Coverage is {coverage*100:.2f}')\n",
        "        print(f' metrics of system are: {humanai_metrics}')\n",
        "        print(f' metrics of human are: {human_metrics}')\n",
        "        print(f' metrics of AI are: {ai_metrics}')\n",
        "    return coverage, humanai_metrics, human_metrics, ai_metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBHrWz39CjhT"
      },
      "source": [
        "## Get predictions \n",
        "Obtain embeddings of the passages and other data used for teaching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dtK3TIrwNWY"
      },
      "source": [
        "# Define teaching and validation\n",
        "random.seed(66)\n",
        "all_passages = [hard_dataset[i]['passage'] for i in range(len(hard_dataset))]\n",
        "embeddings_passage = model.encode(all_passages)\n",
        "embeddings_passage = [embeddings_passage[i] for i in range(len(embeddings_passage))]\n",
        "\n",
        "all_questions = [hard_dataset[i]['question'] for i in range(len(hard_dataset))]\n",
        "all_answers = [hard_dataset[i]['answer'] for i in range(len(hard_dataset))]\n",
        "all_sentences =  [hard_dataset[i]['context']['sentences'] for i in range(len(hard_dataset))]\n",
        "\n",
        "train_indices = list(range(VAL_START))\n",
        "test_indices = list(range(VAL_START,len(hard_dataset)))\n",
        "\n",
        "train_passages = [embeddings_passage[i] for i in train_indices]\n",
        "#train_questions = [embeddings_question[i] for i in train_indices]\n",
        "test_passages = [embeddings_passage[i] for i in test_indices]\n",
        "#test_questions = [embeddings_question[i] for i in test_indices]\n",
        "train_sentences = [all_sentences[i] for i in train_indices]\n",
        "test_sentences = [all_sentences[i] for i in test_indices]\n",
        "\n",
        "train_answers = [all_answers[i] for i in train_indices]\n",
        "test_answers = [all_answers[i] for i in test_indices]\n",
        "train_raw_passages = [all_passages[i] for i in train_indices]\n",
        "test_raw_passages = [all_passages[i] for i in test_indices]\n",
        "train_raw_questions= [all_questions[i] for i in train_indices]\n",
        "test_raw_questions = [all_questions[i] for i in test_indices]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF5GamxWCtvI"
      },
      "source": [
        "\n",
        "teaching_embeddings = np.asarray([np.concatenate([train_passages[i]]) for i in range(len(train_passages))])\n",
        "validation_embeddings = np.asarray([np.concatenate([test_passages[i]]) for i in range(len(test_passages))])\n",
        "del embeddings_passage\n",
        "del model\n",
        "del train_passages\n",
        "del test_passages\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHpcZgQzpYQP"
      },
      "source": [
        "similarities_embeds_all = rbf_kernel(np.asarray(teaching_embeddings), np.asarray(teaching_embeddings))\n",
        "sorted_sims = []\n",
        "for i in range(len(similarities_embeds_all)):\n",
        "    sorted_sim = sorted([(similarities_embeds_all[i][k], k) for k in range(len(teaching_embeddings))], key=lambda tup: tup[0])\n",
        "    sorted_sims.append(np.asarray(sorted_sim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azMMvAQonEMo"
      },
      "source": [
        "# get optimal gammas\n",
        "def get_optimal_gammas():\n",
        "    optimal_gammas = []\n",
        "    with tqdm(total=len(teaching_embeddings)) as pbar:\n",
        "        similarities_embeds_all = rbf_kernel( np.asarray(teaching_embeddings), np.asarray(teaching_embeddings))\n",
        "        for i in range(len(teaching_embeddings)):\n",
        "            # get all similarities\n",
        "            similarities_embeds = similarities_embeds_all[i]\n",
        "            opt_defer_ex = opt_defer_teaching[i]\n",
        "            opt_gamma = 1\n",
        "            sorted_sim = sorted([(similarities_embeds[k], opt_defer_teaching[k]) for k in range(len(teaching_embeddings))], key=lambda tup: tup[0])\n",
        "            indicess = list(range(1, len(opt_defer_teaching)))\n",
        "            indicess.reverse()\n",
        "            for k in indicess:\n",
        "                if sorted_sim[k][1] == opt_defer_ex and sorted_sim[k- 1][1] != opt_defer_ex:\n",
        "                    opt_gamma = sorted_sim[k][0]\n",
        "                    break\n",
        "            optimal_gammas.append(opt_gamma)\n",
        "            pbar.update(1)\n",
        "    return optimal_gammas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3oEF1rEnXKB"
      },
      "source": [
        "# Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMCUulSHCy78"
      },
      "source": [
        "## Selection Algorithm- consistent (ours)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Czd2J20eN8VB"
      },
      "source": [
        "def get_improvement_defer(current_defer_preds, opt_defer_preds, gammas, xs, coin_prob = 0.15):\n",
        "    error_improvements = []\n",
        "    #similarities_embeds_all = rbf_kernel(np.asarray(xs), np.asarray(xs))\n",
        "    error_at_i = 0\n",
        "    for i in range(len(gammas)):\n",
        "        coin = random.random() # random number between [0,1]\n",
        "        error_at_i = 0\n",
        "        similarities_embeds = similarities_embeds_all[i]\n",
        "        for j in range(len(similarities_embeds)):\n",
        "            if similarities_embeds[j] >= gammas[i]:\n",
        "                f1_hum = hum_teaching_preds_b[j]\n",
        "                f1_ai = ai_teaching_preds_b[j]\n",
        "                if opt_defer_preds[i] == 1:\n",
        "                    if current_defer_preds[j] == 0:\n",
        "                        error_at_i += f1_ai - f1_hum\n",
        "                else:\n",
        "                    if current_defer_preds[j] == 1:\n",
        "                        error_at_i += f1_hum - f1_ai\n",
        "        error_improvements.append(error_at_i)\n",
        "\n",
        "        # get the ball for x\n",
        "        # in this ball how many does the current defer not match the optimal\n",
        "    return error_improvements\n",
        "\n",
        "\n",
        "def get_greedy_gamma(i, current_defer_preds, opt_defer_preds, gammas, xs):\n",
        "    similarities_embeds = similarities_embeds_all[i]\n",
        "    sorted_sim = sorted_sims[i]#sorted([(similarities_embeds[k], k) for k in range(len(teaching_embeddings))], key=lambda tup: tup[0])\n",
        "    indicess = list(range(1, len(opt_defer_teaching)))\n",
        "    indicess.reverse()\n",
        "    max_improve = -1000\n",
        "    gamma_value = 1\n",
        "    current_improve = 0\n",
        "    so_far = 0\n",
        "    for j in indicess:\n",
        "        so_far += 1\n",
        "        idx = int(sorted_sim[j][1])\n",
        "        #f1_hum = metric_max_over_ground_truths(f1_score, train_answers[idx], [hum_teaching_preds[idx]]) # pass as param plz\n",
        "        #f1_ai = metric_max_over_ground_truths(f1_score, train_answers[idx], [ai_teaching_preds[idx]])# pass as param plz\n",
        "        f1_hum = hum_teaching_preds_b[idx]\n",
        "        f1_ai = ai_teaching_preds_b[idx]\n",
        "        if opt_defer_preds[i] == 1:\n",
        "            if current_defer_preds[idx] == 0:\n",
        "                current_improve += f1_ai - f1_hum\n",
        "        else:\n",
        "            if current_defer_preds[idx] == 1:\n",
        "                current_improve += f1_hum - f1_ai\n",
        "\n",
        "        if current_improve >= max_improve:\n",
        "            max_improve = current_improve \n",
        "            gamma_value = sorted_sim[j][0]\n",
        "\n",
        "    return max_improve, gamma_value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4llaAF5NsL2"
      },
      "source": [
        "def teach_ours(greedy_gamma = False):\n",
        "    human_learner = HumanLearner(None)\n",
        "    errors = []\n",
        "    data_sizes  = []\n",
        "    indices_used = []\n",
        "    points_chosen = []\n",
        "    for itt in range(MAX_SIZE):\n",
        "        print(f'New size {itt}')\n",
        "        best_index = -1\n",
        "        # predict with current human learner\n",
        "        if itt == 0:\n",
        "            preds_teach = priorhum_teaching_preds\n",
        "        else:\n",
        "            preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "        error_improvements = get_improvement_defer(preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
        "        best_index = np.argmax(error_improvements)\n",
        "        indices_used.append(best_index) # add found element to set used\n",
        "        ex_embed = teaching_embeddings[best_index]\n",
        "        ex_label = opt_defer_teaching[best_index]\n",
        "\n",
        "        if greedy_gamma:\n",
        "            _, greedy_gamma = get_greedy_gamma(best_index, preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
        "            gamma = greedy_gamma\n",
        "            print(f'got improvements with max {_}')\n",
        "\n",
        "        else:\n",
        "            gamma = optimal_gammas[best_index]\n",
        "            print(f'got improvements with max {max(error_improvements)}')\n",
        "\n",
        "\n",
        "        human_learner.add_to_teaching([ex_embed, ex_label, gamma])\n",
        "\n",
        "        if False and itt % 3 == 0:\n",
        "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, train_answers, True)\n",
        "            #errors.append(metrics)   \n",
        "            print(\"##############################\")\n",
        "\n",
        "        if itt % PLOT_INTERVAL == 0:\n",
        "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner.predict(validation_embeddings, priorhum_validation_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_validation_preds, ai_validation_preds, preds_teach, test_answers, True)\n",
        "            errors.append(metrics['exact_match'])   \n",
        "            print(\"##############################\")\n",
        "    return errors, indices_used\n",
        "#errors, indices_used = teach_ours(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCa5P-rXnEMv"
      },
      "source": [
        "## Selection double greedy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5nncmzznEMv"
      },
      "source": [
        "\n",
        "indicess = list(range(1, len(teaching_embeddings)))\n",
        "indicess.reverse()\n",
        "def get_improvement_defer_greedy(current_defer_preds, opt_defer_preds, gammas, xs):\n",
        "\n",
        "    error_improvements = []\n",
        "    #similarities_embeds_all = rbf_kernel(np.asarray(xs), np.asarray(xs))\n",
        "    error_at_i = 0\n",
        "    found_gammas = []\n",
        "    for i in range(len(opt_defer_preds)):\n",
        "        coin = random.random() # random number between [0,1]\n",
        "        similarities_embeds = similarities_embeds_all[i]\n",
        "        sorted_sim = sorted_sims[i]#sorted([(similarities_embeds[k], k) for k in range(len(teaching_embeddings))], key=lambda tup: tup[0])\n",
        "\n",
        "        max_improve = -1000\n",
        "        gamma_value = 1\n",
        "        current_improve = 0\n",
        "        so_far = 0\n",
        "        for j in indicess:\n",
        "            so_far += 1\n",
        "            idx = int(sorted_sim[j][1])\n",
        "            f1_hum = hum_teaching_preds_b[idx]\n",
        "            f1_ai = ai_teaching_preds_b[idx]\n",
        "            if opt_defer_preds[i] == 1:\n",
        "                if current_defer_preds[idx] == 0:\n",
        "                    current_improve += f1_ai - f1_hum\n",
        "            else:\n",
        "                if current_defer_preds[idx] == 1:\n",
        "                    current_improve += f1_hum - f1_ai\n",
        "\n",
        "            if current_improve >= max_improve:\n",
        "                max_improve = current_improve \n",
        "                gamma_value = sorted_sim[j][0]\n",
        "            \n",
        "        error_improvements.append(max_improve)\n",
        "        found_gammas.append(gamma_value)\n",
        "    return error_improvements, found_gammas\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KZkwM5InEMx"
      },
      "source": [
        "def teach_ours_doublegreedy():\n",
        "    human_learner = HumanLearner(None)\n",
        "    errors = []\n",
        "    data_sizes  = []\n",
        "    indices_used = []\n",
        "    points_chosen = []\n",
        "    for itt in range(MAX_SIZE):\n",
        "        print(f'New size {itt}')\n",
        "        best_index = -1\n",
        "        # predict with current human learner\n",
        "        if itt == 0:\n",
        "            preds_teach = priorhum_teaching_preds\n",
        "        else:\n",
        "            preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "        error_improvements, best_gammas = get_improvement_defer_greedy(preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
        "        print(f'got improvements with max {max(error_improvements)}')\n",
        "        best_index = np.argmax(error_improvements)\n",
        "        indices_used.append(best_index) # add found element to set used\n",
        "        ex_embed = teaching_embeddings[best_index]\n",
        "        ex_label = opt_defer_teaching[best_index]\n",
        "        gamma = best_gammas[best_index] # + (np.random.rand(1)[0])*(1-optimal_gammas[best_index])-(1-optimal_gammas[best_index])/2 # random choice\n",
        "        human_learner.add_to_teaching([ex_embed, ex_label, gamma])\n",
        "\n",
        "        if False and itt % 3 == 0:\n",
        "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, train_answers, True)\n",
        "            #errors.append(metrics)   \n",
        "            print(\"##############################\")\n",
        "\n",
        "        if itt % PLOT_INTERVAL == 0:\n",
        "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner.predict(validation_embeddings, priorhum_validation_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_validation_preds, ai_validation_preds, preds_teach, test_answers, True)\n",
        "            errors.append(metrics['exact_match'])   \n",
        "            print(\"##############################\")\n",
        "    return errors, indices_used\n",
        "#errors_doublegreedy, indices_used_doublegreedy = teach_ours_doublegreedy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2Ch46jJttQL"
      },
      "source": [
        "## Random baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZKwIJsntunf"
      },
      "source": [
        "def teach_random(greedy_gamma = False):\n",
        "    human_learner_random = HumanLearner(None)\n",
        "    errors_random = []\n",
        "    data_sizes  = []\n",
        "    indices_used_random = random.sample(list(range(len(teaching_embeddings))), MAX_SIZE) # used to take gradient steps\n",
        "    points_chosen = []\n",
        "    for itt in range(MAX_SIZE):\n",
        "        print(f'New size {itt}')\n",
        "        best_index = indices_used_random[itt]\n",
        "        ex_embed = teaching_embeddings[best_index]\n",
        "        ex_label = opt_defer_teaching[best_index]\n",
        "        if greedy_gamma:\n",
        "            if itt == 0 or first_time:\n",
        "                preds_teach = priorhum_teaching_preds\n",
        "                first_time = False\n",
        "            else:\n",
        "                preds_teach = human_learner_random.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "            _, greedy_gamma = get_greedy_gamma(best_index, preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
        "            gamma = greedy_gamma\n",
        "        else:\n",
        "            gamma = optimal_gammas[best_index]\n",
        "        human_learner_random.add_to_teaching([ex_embed, ex_label, gamma])\n",
        "\n",
        "        if  False and itt % 100 == 0:\n",
        "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner_random.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, train_answers, True)\n",
        "            #errors_random.append(metrics)   \n",
        "            print(\"##############################\")\n",
        "\n",
        "        if  (itt) % PLOT_INTERVAL == 0:\n",
        "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner_random.predict(validation_embeddings, priorhum_validation_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_validation_preds, ai_validation_preds, preds_teach, test_answers, True)\n",
        "            errors_random.append(metrics['exact_match'])   \n",
        "            print(\"##############################\")\n",
        "    return errors_random, indices_used_random\n",
        "#errors_random, indices_used_random = teach_random()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSkPt-33wSAp"
      },
      "source": [
        "## Medoids Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpiWrRqwwT94"
      },
      "source": [
        "from sklearn_extra.cluster import KMedoids\n",
        "def teach_medoids(greedy_gamma = False):\n",
        "    \n",
        "    human_learner_medoid = HumanLearner(None)\n",
        "    errors_medoid = []\n",
        "    data_sizes  = []\n",
        "    indices_used_medoid = []\n",
        "    points_chosen = []\n",
        "    \n",
        "    for itt in range(MAX_SIZE):\n",
        "        if False and itt % 3 == 0:\n",
        "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner_medoid.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, train_answers, True)\n",
        "            #errors_medoid.append(metrics)   \n",
        "            print(\"##############################\")\n",
        "\n",
        "        if itt % PLOT_INTERVAL == 0:\n",
        "            human_learner_medoid = HumanLearner(None)\n",
        "            kmedoids = KMedoids(n_clusters=itt+1, init='k-medoids++').fit(teaching_embeddings)\n",
        "            teaching_indices = kmedoids.medoid_indices_\n",
        "\n",
        "            print(f'New size {itt}')\n",
        "            first_time = True\n",
        "            for teach_ex_idx in teaching_indices:\n",
        "                best_index = teach_ex_idx\n",
        "                if greedy_gamma:\n",
        "                    if itt == 0 or first_time:\n",
        "                        preds_teach = priorhum_teaching_preds\n",
        "                        first_time = False\n",
        "                    else:\n",
        "                        preds_teach = human_learner_medoid.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "                    _, greedy_gamma = get_greedy_gamma(best_index, preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
        "                    gamma = greedy_gamma\n",
        "                else:\n",
        "                    gamma = optimal_gammas[best_index]\n",
        "                ex_embed = teaching_embeddings[best_index]\n",
        "                ex_label = opt_defer_teaching[best_index]\n",
        "                 #+ (np.random.rand(1)[0])*(1-optimal_gammas[best_index])-(1-optimal_gammas[best_index])/2 # random choice\n",
        "                human_learner_medoid.add_to_teaching([ex_embed, ex_label, gamma])\n",
        "\n",
        "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner_medoid.predict(validation_embeddings, priorhum_validation_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_validation_preds, ai_validation_preds, preds_teach, test_answers, True)\n",
        "            errors_medoid.append(metrics['exact_match'])   \n",
        "            print(\"##############################\")\n",
        "    return errors_medoid, indices_used_medoid\n",
        "#errors_medoid, indices_used_medoid = teach_medoids()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZosFDIVUqNbA"
      },
      "source": [
        "## LIME baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jrHGZr_qQkx"
      },
      "source": [
        "def get_improvement_lime(features_covered, xs, indices_used_lime):\n",
        "    error_improvements = []\n",
        "    for i in range(len(xs)):\n",
        "        error_at_i = 0\n",
        "        if i in indices_used_lime:\n",
        "            error_at_i = -10000\n",
        "        for feat_id, feat_val in individual_feature_importance[i].items():\n",
        "            if features_covered[feat_id] == 0:\n",
        "                error_at_i +=  global_feature_importance[feat_id]\n",
        "                    \n",
        "        error_improvements.append(error_at_i)\n",
        "        # get the ball for x\n",
        "        # in this ball how many does the current defer not match the optimal\n",
        "    \n",
        "    best_index = np.argmax(error_improvements)\n",
        "    for feat_id, feat_val in individual_feature_importance[best_index].items():\n",
        "        if features_covered[feat_id] == 0:\n",
        "            features_covered[feat_id] = 1\n",
        "    return error_improvements, features_covered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp-wJsPyqOrW"
      },
      "source": [
        "def teach_lime(greedy_gamma = False):\n",
        "    human_learner = HumanLearner(None)\n",
        "    errors_lime = []\n",
        "    data_sizes  = []\n",
        "    indices_used_lime = {}\n",
        "    points_chosen = []\n",
        "    features_covered = {}\n",
        "    for i in range(DATA_DIM):\n",
        "        features_covered[i] = 0\n",
        "\n",
        "    for itt in range(MAX_SIZE):\n",
        "        print(f'New size {itt}')\n",
        "        best_index = -1\n",
        "        # predict with current human learner\n",
        "\n",
        "        error_improvements, features_covered = get_improvement_lime(features_covered, teaching_embeddings[:cutoff_size], indices_used_lime)\n",
        "        print(f'got improvements with max {max(error_improvements)}')\n",
        "        best_index = np.argmax(error_improvements)\n",
        "        indices_used_lime[best_index] =1 # add found element to set used\n",
        "        ex_embed = teaching_embeddings[best_index]\n",
        "        ex_label = opt_defer_teaching[best_index]\n",
        "\n",
        "        if greedy_gamma:\n",
        "            if itt == 0 :\n",
        "                preds_teach = priorhum_teaching_preds\n",
        "            else:\n",
        "                preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "                \n",
        "            _, greedy_gamma = get_greedy_gamma(best_index, preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
        "            gamma = greedy_gamma\n",
        "        else:\n",
        "            gamma = optimal_gammas[best_index]\n",
        "\n",
        "        #gamma = optimal_gammas[best_index]  #+ (np.random.rand(1)[0])*2*(1-optimal_gammas[best_index])-(1-optimal_gammas[best_index]) # random choice\n",
        "        human_learner.add_to_teaching([ex_embed, ex_label, gamma])\n",
        "\n",
        "        if False and itt % 3 == 0:\n",
        "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, train_answers, True)\n",
        "            #errors_lime.append(metrics)   \n",
        "            print(\"##############################\")\n",
        "\n",
        "        if itt % PLOT_INTERVAL == 0:\n",
        "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner.predict(validation_embeddings, priorhum_validation_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_validation_preds, ai_validation_preds, preds_teach, test_answers, True)\n",
        "            errors_lime.append(metrics['exact_match'])   \n",
        "            print(\"##############################\")\n",
        "    return errors_lime, indices_used_lime\n",
        "#errors_lime, indices_used_lime = teach_lime()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VhWuzn6CYyT"
      },
      "source": [
        "## Learn AI behavior baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZV7k42ICjxJ"
      },
      "source": [
        "from sklearn.neighbors import RadiusNeighborsClassifier, KNeighborsClassifier\n",
        "def teach_learnai(greedy_gamma = False):\n",
        "    human_learner_learnai = HumanLearner(None)\n",
        "    errors_learnai = []\n",
        "    data_sizes  = []\n",
        "    indices_used_learnai = []\n",
        "    points_chosen = []\n",
        "    set_xs = [teaching_embeddings[0]]\n",
        "    set_ys = [ai_teaching_preds_b[0]]\n",
        "    for itt in range(MAX_SIZE):\n",
        "        print(f'New size {itt}')\n",
        "        best_index = 0\n",
        "        best_value = 0\n",
        "        neigh = KNeighborsClassifier(n_neighbors = 1, weights='distance')\n",
        "        random_teach_subset = random.sample(list(range(len(teaching_embeddings))), len(teaching_embeddings)) \n",
        "        random_test_subset = random.sample(list(range(len(teaching_embeddings))), len(teaching_embeddings)) \n",
        "        for j in random_teach_subset:\n",
        "            x_try = teaching_embeddings[j]\n",
        "            y_try = ai_teaching_preds_b[j]\n",
        "            set_xs.append(x_try)\n",
        "            set_ys.append(y_try)\n",
        "            np_set_xs = np.asarray(set_xs)\n",
        "            np_set_ys = np.asarray(set_ys)\n",
        "            neigh = KNeighborsClassifier(n_neighbors = 1, weights='distance')\n",
        "            neigh.fit(np_set_xs, np_set_ys)\n",
        "            acc = neigh.score(np.asarray([teaching_embeddings[kk] for kk in random_test_subset]), np.asarray([ai_teaching_preds_b[kk] for kk in random_test_subset]))\n",
        "            if acc >= best_value:\n",
        "                best_value = acc\n",
        "                best_index = j\n",
        "            set_xs = set_xs[:-1]\n",
        "            set_ys = set_ys[:-1]\n",
        "        print(best_value)\n",
        "        indices_used_learnai.append(best_index)\n",
        "        ex_embed = teaching_embeddings[best_index]\n",
        "        ex_label = opt_defer_teaching[best_index]\n",
        "        \n",
        "        if greedy_gamma:\n",
        "            if itt == 0 :\n",
        "                preds_teach = priorhum_teaching_preds\n",
        "                first_time = False\n",
        "            else:\n",
        "                preds_teach = human_learner_learnai.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "                \n",
        "            _, greedy_gamma = get_greedy_gamma(best_index, preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
        "            gamma = greedy_gamma\n",
        "        else:\n",
        "            gamma = optimal_gammas[best_index]\n",
        "        human_learner_learnai.add_to_teaching([ex_embed, ex_label, gamma])\n",
        "\n",
        "\n",
        "        if False and itt % 3 == 0:\n",
        "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner_learnai.predict(teaching_embeddings, priorhum_teaching_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, train_answers, True)\n",
        "            #errors_learnai.append(metrics)   \n",
        "            print(\"##############################\")\n",
        "\n",
        "        if itt % PLOT_INTERVAL == 0:\n",
        "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
        "            preds_teach = human_learner_learnai.predict(validation_embeddings, priorhum_validation_preds)\n",
        "            _, metrics, __, ___ = compute_metrics(hum_validation_preds, ai_validation_preds, preds_teach, test_answers, True)\n",
        "            errors_learnai.append(metrics['exact_match'])   \n",
        "            print(\"##############################\")\n",
        "    return errors_learnai, indices_used_learnai\n",
        "#errors_learnai, indices_used_learnai = teach_learnai()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTR95IRRnEM8"
      },
      "source": [
        "# Replicate Figure 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEKVWOUPnEM8"
      },
      "source": [
        "human_predictor = FakeAI(teaching_embeddings, teaching_embeddings, 15, 15,  (2,1), kmeans = None)\n",
        "kmeans_pass = human_predictor.kmeans_pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYrWduBA5Hbb"
      },
      "source": [
        "setting = \"A\"\n",
        "greedy_gamma = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1xlqdMg0Hh0"
      },
      "source": [
        "if setting == \"A\":\n",
        "    REJECT_EPSILON = 0.1\n",
        "else:\n",
        "    REJECT_EPSILON = 0.9\n",
        "MAX_TRIALS = 10\n",
        "MAX_SIZE = 100 + 1\n",
        "PLOT_INTERVAL = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cm6xhNkunEM8",
        "scrolled": true
      },
      "source": [
        "'''\n",
        "At each trial, get new human and AI models, get optimal gammas and evaluate methods\n",
        "'''\n",
        "scores_ours = []\n",
        "scores_medoid = []\n",
        "scores_random = []\n",
        "scores_aibaseline = []\n",
        "scores_oracle = []\n",
        "scores_lime = []\n",
        "scores_human = []\n",
        "for trial in range(MAX_TRIALS):\n",
        "    # Get Human Predictions\n",
        "    print(f' \\n \\n trial {trial}  \\n \\n')\n",
        "    hum_teaching_preds = []\n",
        "    hum_validation_preds = []\n",
        "    priorhum_teaching_preds = []\n",
        "    priorhum_validation_preds = []\n",
        "    if setting == \"A\":\n",
        "        human_predictor = FakeAI(teaching_embeddings, teaching_embeddings, 15, 15,  (1,1), kmeans = kmeans_pass)\n",
        "    else:\n",
        "        human_predictor = FakeAI(teaching_embeddings, teaching_embeddings, 15, 15,  (2,1), kmeans = kmeans_pass)\n",
        "    hum_teaching_preds = []\n",
        "    hum_validation_preds = []\n",
        "    priorhum_teaching_preds = []\n",
        "    priorhum_validation_preds = []\n",
        "    hum_teaching_preds_b = human_predictor.predict_right_wrong(teaching_embeddings, teaching_embeddings)\n",
        "    hum_validation_preds_b = human_predictor.predict_right_wrong(validation_embeddings, validation_embeddings)\n",
        "    for i in range(len(hum_teaching_preds_b)):\n",
        "        if hum_teaching_preds_b[i] == 1:\n",
        "            hum_teaching_preds.append(train_answers[i])\n",
        "        else:\n",
        "            sents = nltk.sent_tokenize(train_raw_passages[i].replace(\"#$\",\"</br> </br>\"))\n",
        "            rand_sent = random.randint(0,len(sents)-1)\n",
        "            hum_teaching_preds.append(sents[rand_sent])\n",
        "    for i in range(len(hum_validation_preds_b)):\n",
        "        if hum_validation_preds_b[i] == 1:\n",
        "            hum_validation_preds.append(test_answers[i])\n",
        "        else:\n",
        "            sents = nltk.sent_tokenize(test_raw_passages[i].replace(\"#$\",\"</br> </br>\"))\n",
        "            rand_sent = random.randint(0,len(sents)-1)\n",
        "            hum_validation_preds.append(sents[rand_sent])\n",
        "    priorhum_teaching_preds = human_predictor.prior_rejector(teaching_embeddings, teaching_embeddings, REJECT_EPSILON)\n",
        "    priorhum_validation_preds = human_predictor.prior_rejector(validation_embeddings, validation_embeddings, REJECT_EPSILON)\n",
        "\n",
        "    # Get AI predictions\n",
        "    ai_teaching_preds = []\n",
        "    ai_validation_preds = []\n",
        "    if setting == \"A\":\n",
        "        fakeai = FakeAI(teaching_embeddings, teaching_embeddings, 15, 15, (2,1), kmeans = human_predictor.kmeans_pass )\n",
        "    else:\n",
        "        fakeai = FakeAI(teaching_embeddings, teaching_embeddings, 15, 15, (1,1), kmeans = human_predictor.kmeans_pass )\n",
        "    ai_teaching_preds_b = fakeai.predict_right_wrong(teaching_embeddings, teaching_embeddings)\n",
        "    ai_validation_preds_b = fakeai.predict_right_wrong(validation_embeddings, validation_embeddings)\n",
        "    ai_teaching_preds = []\n",
        "    for i in range(len(ai_teaching_preds_b)):\n",
        "        if ai_teaching_preds_b[i] == 1:\n",
        "            ai_teaching_preds.append(train_answers[i])\n",
        "        else:\n",
        "            sents = nltk.sent_tokenize(train_raw_passages[i].replace(\"#$\",\"</br> </br>\"))\n",
        "            rand_sent = random.randint(0,len(sents)-1)\n",
        "            ai_teaching_preds.append(sents[rand_sent])\n",
        "    ai_validation_preds = []\n",
        "    for i in range(len(ai_validation_preds_b)):\n",
        "        if ai_validation_preds_b[i] == 1:\n",
        "            ai_validation_preds.append(test_answers[i])\n",
        "        else:\n",
        "            sents = nltk.sent_tokenize(test_raw_passages[i].replace(\"#$\",\"</br> </br>\"))\n",
        "            rand_sent = random.randint(0,len(sents)-1)\n",
        "            ai_validation_preds.append(sents[rand_sent])\n",
        "\n",
        "    # Optimal deferall decisions\n",
        "    opt_defer_teaching = []\n",
        "    opt_defer_validation = []\n",
        "    clusts_p_ai = fakeai.kmeans_pass.predict(teaching_embeddings)\n",
        "    clusts_p_hum = human_predictor.kmeans_pass.predict(teaching_embeddings)\n",
        "    for ex in range(len(hum_teaching_preds)):\n",
        "        corr_ai = fakeai.clust_err_p[clusts_p_ai[ex]]\n",
        "        corr_hum = human_predictor.clust_err_p[clusts_p_hum[ex]]\n",
        "        if corr_ai >= corr_hum:\n",
        "            opt_defer_teaching.append(1)\n",
        "        else:\n",
        "            opt_defer_teaching.append(0)\n",
        "    clusts_p_ai = fakeai.kmeans_pass.predict(validation_embeddings)\n",
        "    clusts_p_hum = human_predictor.kmeans_pass.predict(validation_embeddings)\n",
        "    for ex in range(len(hum_validation_preds)):\n",
        "        corr_ai = fakeai.clust_err_p[clusts_p_ai[ex]]\n",
        "        corr_hum = human_predictor.clust_err_p[clusts_p_hum[ex]]\n",
        "        if corr_ai >= corr_hum:\n",
        "            opt_defer_validation.append(1)\n",
        "        else:\n",
        "            opt_defer_validation.append(0)\n",
        "\n",
        "    optimal_score = compute_metrics(hum_validation_preds, ai_validation_preds, opt_defer_validation, test_answers)\n",
        "    prior_score = compute_metrics(hum_validation_preds, ai_validation_preds, priorhum_validation_preds, test_answers)\n",
        "    prior_score = prior_score[1]['exact_match']\n",
        "\n",
        "    human_score = compute_metrics(hum_validation_preds, ai_validation_preds, [0]*len(ai_validation_preds), test_answers)\n",
        "    ai_score = compute_metrics(hum_validation_preds, ai_validation_preds, [1]*len(ai_validation_preds), test_answers)\n",
        "\n",
        "    hum = human_score[1]['exact_match']\n",
        "    scores_human.append(human_score[1]['exact_match'])\n",
        "    scores_oracle.append(optimal_score[1]['exact_match'])\n",
        "    if not greedy_gamma:\n",
        "        optimal_gammas = get_optimal_gammas()\n",
        "    else:\n",
        "        optimal_gammas = [1]*len(teaching_embeddings)\n",
        "    print(\"running our method\")\n",
        "    if greedy_gamma:\n",
        "        errors, indices_used = teach_ours_doublegreedy()\n",
        "\n",
        "    else:\n",
        "        errors, indices_used = teach_ours(False)\n",
        "               \n",
        "    print(\"running lime\")\n",
        "    DATA_DIM = teaching_embeddings.shape[1]\n",
        "    explainer = lime.lime_tabular.LimeTabularExplainer(teaching_embeddings, discretize_continuous= False)\n",
        "    global_feature_importance = {}\n",
        "    cutoff_size = len(teaching_embeddings)\n",
        "    for i in range(DATA_DIM):\n",
        "        global_feature_importance[i] = 0\n",
        "    individual_feature_importance = []\n",
        "    with tqdm(total=len(teaching_embeddings)) as pbar:\n",
        "        for ex in range(cutoff_size):\n",
        "            exp = explainer.explain_instance(teaching_embeddings[ex], fakeai.predict_proba_encoded)\n",
        "            feat_weights = exp.local_exp[1]\n",
        "            dic_feat_weights = {}\n",
        "            for j in range(len(feat_weights)):\n",
        "                dic_feat_weights[feat_weights[j][0]] = abs(feat_weights[j][1])\n",
        "                global_feature_importance[feat_weights[j][0]] += abs(feat_weights[j][1])\n",
        "            individual_feature_importance.append(dic_feat_weights)\n",
        "            pbar.update(1)\n",
        "\n",
        "    for i in range(DATA_DIM):\n",
        "        global_feature_importance[i] = math.sqrt(global_feature_importance[i])\n",
        "    errors_lime, indices_used_lime= teach_lime(greedy_gamma)\n",
        "    \n",
        "    print(\"running medoid\")\n",
        "    errors_medoid, indices_used_medoid = teach_medoids(greedy_gamma)\n",
        "    \n",
        "    print(\"running random\")\n",
        "    errors_random, indices_used_random = teach_random(greedy_gamma)\n",
        "        \n",
        "    print(\"running learnai\")\n",
        "    errors_learnai, indices_used_learnai = teach_learnai(greedy_gamma)\n",
        "\n",
        "    errors.insert(0, prior_score)\n",
        "    errors_learnai.insert(0, prior_score)\n",
        "    errors_medoid.insert(0, prior_score)\n",
        "    errors_random.insert(0, prior_score)\n",
        "    errors_lime.insert(0, prior_score)\n",
        "\n",
        "    scores_ours.append(errors)\n",
        "    scores_aibaseline.append(errors_learnai)\n",
        "    scores_medoid.append(errors_medoid)\n",
        "    scores_random.append(errors_random)\n",
        "    scores_lime.append(errors_lime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHmJJilnnEM_"
      },
      "source": [
        "## Result plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VjtB_penEM_"
      },
      "source": [
        "import matplotlib\n",
        "\n",
        "matplotlib.rcParams['pdf.fonttype'] = 42\n",
        "matplotlib.rcParams['ps.fonttype'] = 42\n",
        "plt.rc('text', usetex=False)\n",
        "plt.rc('font', family='serif')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZLLLaEtnEM_"
      },
      "source": [
        "\n",
        "teaching_sizes = [i*PLOT_INTERVAL for i in range(21)]\n",
        "actual_max_trials = len(scores_ours) \n",
        "\n",
        "avgs_rand = [np.average([ scores_oracle[triall] - scores_ours[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([scores_oracle[triall] - scores_ours[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'GREEDY-SELECT (Ours)')\n",
        "\n",
        "\n",
        "avgs_rand = [np.average([scores_oracle[triall] - scores_medoid[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([ scores_oracle[triall] -scores_medoid[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes,  avgs_rand, yerr=stds_rand, marker = \"s\",   label=f'K-Medoids')\n",
        "\n",
        "avgs_rand = [np.average([scores_oracle[triall]- scores_aibaseline[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([scores_oracle[triall] -scores_aibaseline[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes,  avgs_rand,yerr=stds_rand, marker = \"*\",   label=f'AI-Behavior')\n",
        "\n",
        "\n",
        "avgs_rand = [np.average([scores_oracle[triall] - scores_random[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([scores_oracle[triall] -scores_random[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes,  avgs_rand,yerr=stds_rand, marker = \"v\",  label=f'Random')\n",
        "\n",
        "avgs_rand = [np.average([scores_oracle[triall] - scores_lime[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([scores_oracle[triall] -scores_lime[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes,  avgs_rand,yerr=stds_rand, marker = \"+\",  label=f'LIME')\n",
        "\n",
        "avgs_rand = [np.average([scores_oracle[triall] - scores_human[triall] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([scores_oracle[triall] -scores_human[triall] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes[0],  avgs_rand[0],yerr=stds_rand[0], marker = \"x\",  label=f'Human alone {avgs_rand[0]:.2f} $\\pm$ {stds_rand[0]:.2f}')\n",
        "\n",
        "\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.get_xaxis().tick_bottom()    \n",
        "ax.get_yaxis().tick_left()   \n",
        "plt.grid()\n",
        "plt.legend(fontsize='large')\n",
        "plt.legend()\n",
        "plt.ylabel('Difference to Oracle Accuracy',  fontsize='x-large')\n",
        "plt.xlabel('Teaching set size', fontsize='x-large')\n",
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = 6\n",
        "fig_size[1] = 4\n",
        "plt.savefig(\"teaching_complexity_A_nongreedy\", dpi = 1000)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOTIYC9C6K15"
      },
      "source": [
        "# Replicate Table 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEH2e5p46Njq"
      },
      "source": [
        "human_predictor = FakeAI(teaching_embeddings, teaching_embeddings, 15, 15,  (2,1), kmeans = None)\n",
        "kmeans_pass = human_predictor.kmeans_pass\n",
        "REJECT_EPSILON = 0.9\n",
        "MAX_TRIALS = 10\n",
        "MAX_SIZE = 30 + 1\n",
        "PLOT_INTERVAL = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnVPdVsv6VNM"
      },
      "source": [
        "scores_ours = []\n",
        "scores_noisy_gamma = []\n",
        "scores_no_rej_prior = []\n",
        "scores_no_h_prior = []\n",
        "scores_all_noisy = []\n",
        "scores_oracle = []\n",
        "scores_prior = []\n",
        "for trial in range(MAX_TRIALS):\n",
        "    # Get Human Predictions\n",
        "    print(f' \\n \\n trial {trial}  \\n \\n')\n",
        "    hum_teaching_preds = []\n",
        "    hum_validation_preds = []\n",
        "    priorhum_teaching_preds = []\n",
        "    priorhum_validation_preds = []\n",
        "    human_predictor = FakeAI(teaching_embeddings, teaching_embeddings, 15, 15,  (2,1), kmeans = kmeans_pass)\n",
        "    hum_teaching_preds = []\n",
        "    hum_validation_preds = []\n",
        "    priorhum_teaching_preds = []\n",
        "    priorhum_validation_preds = []\n",
        "    hum_teaching_preds_b = human_predictor.predict_right_wrong(teaching_embeddings, teaching_embeddings)\n",
        "    hum_validation_preds_b = human_predictor.predict_right_wrong(validation_embeddings, validation_embeddings)\n",
        "    for i in range(len(hum_teaching_preds_b)):\n",
        "        if hum_teaching_preds_b[i] == 1:\n",
        "            hum_teaching_preds.append(train_answers[i])\n",
        "        else:\n",
        "            sents = nltk.sent_tokenize(train_raw_passages[i].replace(\"#$\",\"</br> </br>\"))\n",
        "            rand_sent = random.randint(0,len(sents)-1)\n",
        "            hum_teaching_preds.append(sents[rand_sent])\n",
        "    for i in range(len(hum_validation_preds_b)):\n",
        "        if hum_validation_preds_b[i] == 1:\n",
        "            hum_validation_preds.append(test_answers[i])\n",
        "        else:\n",
        "            sents = nltk.sent_tokenize(test_raw_passages[i].replace(\"#$\",\"</br> </br>\"))\n",
        "            rand_sent = random.randint(0,len(sents)-1)\n",
        "            hum_validation_preds.append(sents[rand_sent])\n",
        "    priorhum_teaching_preds = human_predictor.prior_rejector(teaching_embeddings, teaching_embeddings, REJECT_EPSILON)\n",
        "    priorhum_validation_preds = human_predictor.prior_rejector(validation_embeddings, validation_embeddings, REJECT_EPSILON)\n",
        "\n",
        "    # Get AI predictions\n",
        "    ai_teaching_preds = []\n",
        "    ai_validation_preds = []\n",
        "    fakeai = FakeAI(teaching_embeddings, teaching_embeddings, 15, 15, (1,1), kmeans = human_predictor.kmeans_pass )\n",
        "    ai_teaching_preds_b = fakeai.predict_right_wrong(teaching_embeddings, teaching_embeddings)\n",
        "    ai_validation_preds_b = fakeai.predict_right_wrong(validation_embeddings, validation_embeddings)\n",
        "    ai_teaching_preds = []\n",
        "    for i in range(len(ai_teaching_preds_b)):\n",
        "        if ai_teaching_preds_b[i] == 1:\n",
        "            ai_teaching_preds.append(train_answers[i])\n",
        "        else:\n",
        "            sents = nltk.sent_tokenize(train_raw_passages[i].replace(\"#$\",\"</br> </br>\"))\n",
        "            rand_sent = random.randint(0,len(sents)-1)\n",
        "            ai_teaching_preds.append(sents[rand_sent])\n",
        "    ai_validation_preds = []\n",
        "    for i in range(len(ai_validation_preds_b)):\n",
        "        if ai_validation_preds_b[i] == 1:\n",
        "            ai_validation_preds.append(test_answers[i])\n",
        "        else:\n",
        "            sents = nltk.sent_tokenize(test_raw_passages[i].replace(\"#$\",\"</br> </br>\"))\n",
        "            rand_sent = random.randint(0,len(sents)-1)\n",
        "            ai_validation_preds.append(sents[rand_sent])\n",
        "\n",
        "    # Optimal deferall decisions\n",
        "    opt_defer_teaching = []\n",
        "    opt_defer_validation = []\n",
        "    clusts_p_ai = fakeai.kmeans_pass.predict(teaching_embeddings)\n",
        "    clusts_p_hum = human_predictor.kmeans_pass.predict(teaching_embeddings)\n",
        "    for ex in range(len(hum_teaching_preds)):\n",
        "        corr_ai = fakeai.clust_err_p[clusts_p_ai[ex]]\n",
        "        corr_hum = human_predictor.clust_err_p[clusts_p_hum[ex]]\n",
        "        if corr_ai >= corr_hum:\n",
        "            opt_defer_teaching.append(1)\n",
        "        else:\n",
        "            opt_defer_teaching.append(0)\n",
        "    clusts_p_ai = fakeai.kmeans_pass.predict(validation_embeddings)\n",
        "    clusts_p_hum = human_predictor.kmeans_pass.predict(validation_embeddings)\n",
        "    for ex in range(len(hum_validation_preds)):\n",
        "        corr_ai = fakeai.clust_err_p[clusts_p_ai[ex]]\n",
        "        corr_hum = human_predictor.clust_err_p[clusts_p_hum[ex]]\n",
        "        if corr_ai >= corr_hum:\n",
        "            opt_defer_validation.append(1)\n",
        "        else:\n",
        "            opt_defer_validation.append(0)\n",
        "\n",
        "    optimal_gammas = [1.0]*len(teaching_embeddings)\n",
        "    \n",
        "    optimal_score = compute_metrics(hum_validation_preds, ai_validation_preds, opt_defer_validation, test_answers)\n",
        "    scores_oracle.append(optimal_score[1]['exact_match'])\n",
        "    print(optimal_score)\n",
        "    prior_score = compute_metrics(hum_validation_preds, ai_validation_preds, priorhum_validation_preds, test_answers)\n",
        "    prior_score = prior_score[1]['exact_match']\n",
        "    scores_prior.append(prior_score)\n",
        "    human_score = compute_metrics(hum_validation_preds, ai_validation_preds, [0]*len(ai_validation_preds), test_answers)\n",
        "    hum = human_score[1]['exact_match']\n",
        "    opt = optimal_score[1]['exact_match']\n",
        "\n",
        "    print(\"###########################\")\n",
        "    print(\"running our method: clean\")\n",
        "    errors, indices_used = teach_ours_doublegreedy(hum_teaching_preds_b, priorhum_teaching_preds, opt_defer_teaching, False)\n",
        "    errors.insert(0, prior_score)\n",
        "    scores_ours.append(errors)\n",
        "    print(\"###########################\")\n",
        "    print(\"running our method: noisy gamma\")\n",
        "    errors, indices_used = teach_ours_doublegreedy(hum_teaching_preds_b, priorhum_teaching_preds, opt_defer_teaching, True)\n",
        "    errors.insert(0, prior_score)\n",
        "    scores_noisy_gamma.append(errors)\n",
        "    print(\"###########################\")\n",
        "    print(\"running our method: no rej prior\")\n",
        "    random_prior = np.random.randint(2, size = len(priorhum_teaching_preds))\n",
        "    errors, indices_used = teach_ours_doublegreedy(hum_teaching_preds_b, random_prior, opt_defer_teaching, False)\n",
        "    errors.insert(0, prior_score)\n",
        "    scores_no_rej_prior.append(errors)\n",
        "    print(\"###########################\")\n",
        "    print(\"running our method: no h \")\n",
        "    opt_err_defer_teaching = []\n",
        "    clusts_p_ai = fakeai.kmeans_pass.predict(teaching_embeddings)\n",
        "    clusts_p_hum = human_predictor.kmeans_pass.predict(teaching_embeddings)\n",
        "    for ex in range(len(hum_teaching_preds)):\n",
        "        corr_ai = fakeai.clust_err_p[clusts_p_ai[ex]]\n",
        "        corr_hum = 0.5#human_predictor.clust_err_p[clusts_p_hum[ex]]\n",
        "        if corr_ai >= corr_hum:\n",
        "            opt_err_defer_teaching.append(1)\n",
        "        else:\n",
        "            opt_err_defer_teaching.append(0) \n",
        "    random_preds = np.random.randint(2, size = len(priorhum_teaching_preds))\n",
        "    errors, indices_used = teach_ours_doublegreedy(random_preds, priorhum_teaching_preds, opt_err_defer_teaching, False)\n",
        "    errors.insert(0, prior_score)\n",
        "    scores_no_h_prior.append(errors)\n",
        "    print(\"###########################\")\n",
        "    print(\"running our method: all wrong \")\n",
        "    errors, indices_used = teach_ours_doublegreedy(random_preds, random_prior, opt_err_defer_teaching, True)\n",
        "    errors.insert(0, prior_score)\n",
        "    scores_all_noisy.append(errors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS7CMzPn6cVz"
      },
      "source": [
        "\n",
        "teaching_sizes = [i*PLOT_INTERVAL for i in range(3)]\n",
        "actual_max_trials = len(scores_oracle_fixed) \n",
        "avgs_rand = [np.average([scores_oracle_fixed[triall] - scores_ours[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([scores_oracle_fixed[triall] -  scores_ours[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes,  avgs_rand, yerr=stds_rand, label=f'Ours')\n",
        "print(f'no noise {avgs_rand[-1]} and std {stds_rand[-1]} ')\n",
        "avgs_rand = [np.average([scores_oracle_fixed[triall] -  scores_noisy_gamma[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([scores_oracle_fixed[triall] -  scores_noisy_gamma[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes,  avgs_rand, yerr=stds_rand, label=f'noisy gamma')\n",
        "print(f'noise {avgs_rand[-1]} and std {stds_rand[-1]} ')\n",
        "\n",
        "avgs_rand = [np.average([scores_oracle_fixed[triall] -  scores_no_rej_prior[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([scores_oracle_fixed[triall] -  scores_no_rej_prior[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes,  avgs_rand, yerr=stds_rand, label=f'no rej prior')\n",
        "print(f'no rej prior {avgs_rand[-1]} and std {stds_rand[-1]} ')\n",
        "\n",
        "avgs_rand = [np.average([scores_oracle_fixed[triall] -  scores_no_h_prior[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([scores_oracle_fixed[triall] -  scores_no_h_prior[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes,  avgs_rand, yerr=stds_rand, label=f'no h prior')\n",
        "print(f'no h prior {avgs_rand[-1]} and std {stds_rand[-1]} ')\n",
        "\n",
        "avgs_rand = [np.average([scores_oracle_fixed[triall] -  scores_all_noisy[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([scores_oracle_fixed[triall] -  scores_all_noisy[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes,  avgs_rand, yerr=stds_rand, label=f'all noise')\n",
        "print(f'all noisy {avgs_rand[-1]} and std {stds_rand[-1]} ')\n",
        "print(f'prior {avgs_rand[0]} and std {stds_rand[0]} ')\n",
        "\n",
        "\n",
        "\n",
        "avgs_rand = [np.average([scores_oracle_fixed[triall] -  scores_human[triall] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "stds_rand = [np.std([scores_oracle_fixed[triall] -  scores_human[triall] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
        "plt.errorbar(teaching_sizes,  avgs_rand, yerr=stds_rand, label=f'all noise')\n",
        "print(f'human {avgs_rand[-1]} and std {stds_rand[-1]} ')\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "ax.spines['right'].set_visible(False)\n",
        "ax.spines['top'].set_visible(False)\n",
        "ax.get_xaxis().tick_bottom()    \n",
        "ax.get_yaxis().tick_left()   \n",
        "plt.grid()\n",
        "plt.legend(fontsize='large')\n",
        "plt.ylabel('Overall Accuracy')\n",
        "plt.xlabel('Teaching set size')\n",
        "fig_size = plt.rcParams[\"figure.figsize\"]\n",
        "fig_size[0] = 5\n",
        "fig_size[1] = 4\n",
        "plt.savefig(\"teaching_complexity_noisy.pdf\", dpi = 1000)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}