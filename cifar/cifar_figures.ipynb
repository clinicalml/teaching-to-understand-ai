{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHFgm6GQAlQ0"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17032,
     "status": "ok",
     "timestamp": 1636488326871,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "gNSNdEF8OlJr",
    "outputId": "42aa5e3f-103b-4624-9319-90c02fc52404"
   },
   "outputs": [],
   "source": [
    "!pip install scikit-learn-extra\n",
    "!pip install lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 15106,
     "status": "ok",
     "timestamp": 1636488341966,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "yktglxv5LfTV"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "from sklearn import neighbors, datasets\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import torch.nn.parallel\n",
    "from tqdm import tqdm\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "import torch.utils.data as data\n",
    "from torchvision.datasets.utils import download_url, check_integrity\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from scipy.stats import multivariate_normal\n",
    "import  scipy.stats as st\n",
    "from matplotlib import cm\n",
    "from __future__ import print_function\n",
    "from spacy.lang.en import English\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "import matplotlib\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as tfunc\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "import scipy.stats as st\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import lime\n",
    "import lime.lime_tabular\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1636488341967,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "ilvQVFbFLhZ1",
    "outputId": "81418f83-53b8-4614-d495-bb0eb4143595"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(66)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhX-OFF-LiSS"
   },
   "source": [
    "# CIFAR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1636488353001,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "x0OcndfwpMS0"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                                                                padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(int(nb_layers)):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
    "        assert ((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) / 6\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "        self.softmax = nn.Softmax()\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "    def get_repr(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1636488353553,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "Az4blvINpOQu"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def metrics_print(net,expert_fn, n_classes, loader):\n",
    "    '''\n",
    "    Computes metrics for deferal\n",
    "    -----\n",
    "    Arguments:\n",
    "    net: model\n",
    "    expert_fn: expert model\n",
    "    n_classes: number of classes\n",
    "    loader: data loader\n",
    "    '''\n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    alone_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            batch_size = outputs.size()[0]  # batch_size\n",
    "            exp_prediction = expert_fn(images, labels)\n",
    "            for i in range(0, batch_size):\n",
    "                r = (predicted[i].item() == n_classes)\n",
    "                prediction = predicted[i]\n",
    "                if predicted[i] == n_classes:\n",
    "                    max_idx = 0\n",
    "                    # get second max\n",
    "                    for j in range(0, n_classes):\n",
    "                        if outputs.data[i][j] >= outputs.data[i][max_idx]:\n",
    "                            max_idx = j\n",
    "                    prediction = max_idx\n",
    "                else:\n",
    "                    prediction = predicted[i]\n",
    "                alone_correct += (prediction == labels[i]).item()\n",
    "                if r == 0:\n",
    "                    total += 1\n",
    "                    correct += (predicted[i] == labels[i]).item()\n",
    "                    correct_sys += (predicted[i] == labels[i]).item()\n",
    "                if r == 1:\n",
    "                    exp += (exp_prediction[i] == labels[i].item())\n",
    "                    correct_sys += (exp_prediction[i] == labels[i].item())\n",
    "                    exp_total += 1\n",
    "                real_total += 1\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "    to_print = {\"coverage\": cov, \"system accuracy\": 100 * correct_sys / real_total,\n",
    "                \"expert accuracy\": 100 * exp / (exp_total + 0.0002),\n",
    "                \"classifier accuracy\": 100 * correct / (total + 0.0001),\n",
    "                \"alone classifier\": 100 * alone_correct / real_total}\n",
    "    print(to_print)\n",
    "\n",
    "def metrics_print_baseline(net_class, expert_fn, n_classes, loader):\n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs_class = net_class(images)\n",
    "            _, predicted = torch.max(outputs_class.data, 1)\n",
    "            batch_size = outputs_class.size()[0]  # batch_size\n",
    "\n",
    "            exp_prediction = expert_fn(images, labels)\n",
    "            for i in range(0, batch_size):\n",
    "                r = (exp_prediction[i] == labels[i].item())\n",
    "                if r == 0:\n",
    "                    total += 1\n",
    "                    prediction = predicted[i]\n",
    "                    if predicted[i] == n_classes:\n",
    "                        max_idx = 0\n",
    "                        for j in range(0, n_classes):\n",
    "                            if outputs_class.data[i][j] >= outputs_class.data[i][max_idx]:\n",
    "                                max_idx = j\n",
    "                        prediction = max_idx\n",
    "                    else:\n",
    "                        prediction = predicted[i]\n",
    "                    correct += (prediction == labels[i]).item()\n",
    "                    correct_sys += (prediction == labels[i]).item()\n",
    "                if r == 1:\n",
    "                    exp += (exp_prediction[i] == labels[i].item())\n",
    "                    correct_sys += (exp_prediction[i] == labels[i].item())\n",
    "                    exp_total += 1\n",
    "                real_total += 1\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "    to_print = {\"coverage\": cov, \"system accuracy\": 100 * correct_sys / real_total,\n",
    "                \"expert accuracy\": 100 * exp / (exp_total + 0.0002),\n",
    "                \"classifier accuracy\": 100 * correct / (total + 0.0001)}\n",
    "    print(to_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1636488353553,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "iXwqvWappTRh"
   },
   "outputs": [],
   "source": [
    "def my_CrossEntropyLoss(outputs, labels):\n",
    "    batch_size = outputs.size()[0]  # batch_size\n",
    "    outputs = - torch.log2(outputs[range(batch_size), labels])  # pick the values corresponding to the labels\n",
    "    return torch.sum(outputs) / batch_size\n",
    "\n",
    "def train_classifier(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_classes):\n",
    "    \"\"\"Train for one epoch on the training set\"\"\"\n",
    "    # expertfn: a number here k \n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        # compute loss\n",
    "        loss = my_CrossEntropyLoss(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "def validate_classifier(val_loader, model, epoch, expert_fn, n_classes):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output = model(input)\n",
    "        # compute loss\n",
    "        loss = my_CrossEntropyLoss(output, target)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "best_prec1 = 0\n",
    "def run_classifier(model, data_aug, n_dataset, expert_fn, epochs):\n",
    "    global best_prec1\n",
    "    \n",
    "\n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    # for training on multiple GPUs.\n",
    "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
    "    # model = torch.nn.DataParallel(model).cuda()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 0.1,\n",
    "                                momentum=0.9, nesterov=True,\n",
    "                                weight_decay=5e-4)\n",
    "\n",
    "    # cosine learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader) * 200)\n",
    "\n",
    "    for epoch in range(0, epochs):\n",
    "        # train for one epoch\n",
    "        if epoch % 10 ==0:\n",
    "            validate_classifier(train_val_loader, model_classifier, None, None, 10)\n",
    "        train_classifier(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102,
     "referenced_widgets": [
      "ee71b790eef1495289dcd5644f48d1bd",
      "2696cc728ddc40e692d748e5f8a354c3",
      "99fb6bb699d84023ba4bc25bed2b5716",
      "ab495803a8f045d3bcba2017c39e41e1",
      "74859dca02064da095dfec8ccc3fcc35",
      "dcdfc608eedb4ad8994442b9e1439509",
      "08cf2293f3284a56bb2cc8f34171501e",
      "dde3bba679ed400baec4e4cc013d5890",
      "4a6343b9d86e452d9a9f6086471ce563",
      "c510817b77284c04bb513efd2e7902e0",
      "1cbfb1adc9ac4966a4877f233907ef9b"
     ]
    },
    "executionInfo": {
     "elapsed": 11199,
     "status": "ok",
     "timestamp": 1636488364750,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "fT92RLkXvwjJ",
    "outputId": "16d31228-4ec9-4a40-bc97-298754e13d7c"
   },
   "outputs": [],
   "source": [
    "\n",
    "data_aug = False\n",
    "n_dataset = 10\n",
    "normalize = transforms.Normalize(mean=[x / 255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                     std=[x / 255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "if data_aug:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: F.pad(x.unsqueeze(0),\n",
    "                                            (4, 4, 4, 4), mode='reflect').squeeze()),\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "else:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "if n_dataset == 10:\n",
    "    dataset = 'cifar10'\n",
    "elif n_dataset == 100:\n",
    "    dataset = 'cifar100'\n",
    "\n",
    "kwargs = {'num_workers': 0, 'pin_memory': True}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset_all = datasets.__dict__[dataset.upper()]('../data', train=True, download=True,\n",
    "                                                        transform=transform_train)\n",
    "train_size = int(0.90 * len(train_dataset_all))\n",
    "test_size = len(train_dataset_all) - train_size\n",
    "\n",
    "train_dataset, train_test_dataset = torch.utils.data.random_split(train_dataset_all, [train_size, test_size],  generator=torch.Generator().manual_seed(66))\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                            batch_size=128, shuffle=True, **kwargs)\n",
    "train_val_loader = torch.utils.data.DataLoader(train_test_dataset,\n",
    "                                            batch_size=128, shuffle=False, **kwargs)\n",
    "\n",
    "test_dataset_all = datasets.__dict__[dataset.upper()]('../data', train=False, download=True,\n",
    "                                                        transform=transform_train)\n",
    "train_size = int(0.5 * len(train_dataset_all))\n",
    "test_size = len(train_dataset_all) - train_size\n",
    "\n",
    "test_dataset, val_dataset = torch.utils.data.random_split(train_dataset_all, [train_size, test_size], generator=torch.Generator().manual_seed(66))\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                            batch_size=128, shuffle=False, **kwargs)\n",
    "\n",
    "test_all_loader = torch.utils.data.DataLoader(test_dataset_all,\n",
    "                                            batch_size=128, shuffle=False, **kwargs)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                            batch_size=128, shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hGwrCKLtPmWM"
   },
   "source": [
    "# Loading model Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10272,
     "status": "ok",
     "timestamp": 1636490145836,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "FzF5fd0XPrcg"
   },
   "outputs": [],
   "source": [
    "n_dataset = 10  # cifar-10, 100 for cifar-100\n",
    "model_classifier = WideResNet(28, n_dataset, 4, dropRate=0)\n",
    "model_classifier = torch.load(\"cifar_model_seed66_5k.pt\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bYMsE6OnPoRE"
   },
   "source": [
    "# Training model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sjhyGQxpWBM"
   },
   "outputs": [],
   "source": [
    "n_dataset = 10  # cifar-10, 100 for cifar-100\n",
    "model_classifier = WideResNet(28, n_dataset, 4, dropRate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "he9BWV-jpTS-"
   },
   "outputs": [],
   "source": [
    "run_classifier(model_classifier, False, n_dataset, 0, 200)\n",
    "#model_classifier = torch.load(\"model_classifier_cifar_seed66.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdeT2gP5V2V_"
   },
   "outputs": [],
   "source": [
    "model_classifier = torch.load(\"cifar_model_seed66.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nknb8uMQm3ab"
   },
   "outputs": [],
   "source": [
    "torch.save(model_classifier, \"cifar_model_seed66_5k.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zkwGBdRCDVM"
   },
   "outputs": [],
   "source": [
    "model_classifier = torch.load(\"cifar_model_seed66_5k.pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rP9YC0unwaC6"
   },
   "outputs": [],
   "source": [
    "validate_classifier(test_all_loader, model_classifier, None, None, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYMRUZCgvMSn"
   },
   "source": [
    "# Classes for Teaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1636490145837,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "irQ1Y26rrHl5"
   },
   "outputs": [],
   "source": [
    "class HumanLearner:\n",
    "    def __init__(self, kernel):\n",
    "        '''\n",
    "        kernel: function that takes two inputs and returns a similarity\n",
    "        prior rejector: returns rejector\n",
    "        '''\n",
    "        self.teaching_set = []\n",
    "        self.kernel = kernel\n",
    "        self.rejector_tresh = 0.8\n",
    "\n",
    "    def predict(self, xs, prior_rejector_preds, to_print = False):\n",
    "        '''\n",
    "        xs: expected array of inputs\n",
    "        '''\n",
    "        preds = []\n",
    "        idx = 0\n",
    "        used_posterior = 0 \n",
    "        if to_print:\n",
    "            print(\"-- Human making reject predictions --\")\n",
    "            with tqdm(total=len(xs)) as pbar:\n",
    "                for x in xs:\n",
    "                    ball_at_x = []\n",
    "                    similarities = rbf_kernel(x.reshape(1,-1), np.asarray([self.teaching_set[kk][0] for kk in range(len(self.teaching_set))]))[0]\n",
    "                    for i in range(len(self.teaching_set)):\n",
    "                        similarity = similarities[i]\n",
    "                        if similarity >=  self.teaching_set[i][2]:\n",
    "                            ball_at_x.append(self.teaching_set[i])\n",
    "                    if len(ball_at_x) == 0: \n",
    "                        # use prior rejector\n",
    "                        preds.append(prior_rejector_preds[idx])\n",
    "                    else:\n",
    "                        used_posterior += 1\n",
    "                        ball_similarities = rbf_kernel(x.reshape(1,-1), np.asarray([ball_at_x[kk][0] for kk in range(len(ball_at_x))]))[0]\n",
    "                        normalization = np.sum([ball_similarities[i] for i in range(len(ball_at_x))])\n",
    "                        score_one = np.sum([ball_similarities[i]*ball_at_x[i][1] for i in range(len(ball_at_x))])\n",
    "                        pred = score_one / normalization\n",
    "                        if pred >= 0.5:\n",
    "                            preds.append(1)\n",
    "                        else:\n",
    "                            preds.append(0)\n",
    "                    idx += 1\n",
    "                    pbar.update(1)\n",
    "        else:\n",
    "            for x in xs:\n",
    "                ball_at_x = []\n",
    "                similarities = rbf_kernel(x.reshape(1,-1), np.asarray([self.teaching_set[kk][0] for kk in range(len(self.teaching_set))]))[0]\n",
    "                for i in range(len(self.teaching_set)):\n",
    "                    similarity = similarities[i]\n",
    "                    if similarity >=  self.teaching_set[i][2]:\n",
    "                        ball_at_x.append(self.teaching_set[i])\n",
    "                if len(ball_at_x) == 0: \n",
    "                    # use prior rejector\n",
    "                    preds.append(prior_rejector_preds[idx])\n",
    "                else:\n",
    "                    used_posterior += 1\n",
    "                    ball_similarities = rbf_kernel(x.reshape(1,-1), np.asarray([ball_at_x[kk][0] for kk in range(len(ball_at_x))]))[0]\n",
    "                    normalization = np.sum([ball_similarities[i] for i in range(len(ball_at_x))])\n",
    "                    score_one = np.sum([ball_similarities[i]*ball_at_x[i][1] for i in range(len(ball_at_x))])\n",
    "                    pred = score_one / normalization\n",
    "                    if pred >= 0.5:\n",
    "                        preds.append(1)\n",
    "                    else:\n",
    "                        preds.append(0)\n",
    "                idx += 1\n",
    "        if to_print:\n",
    "            print(f'Used posterior {used_posterior/len(xs)*100:.2f}')\n",
    "        return preds\n",
    "\n",
    "    def add_to_teaching(self, teaching_example):\n",
    "        '''\n",
    "        teaching_example: (x, label, gamma)\n",
    "        '''\n",
    "        self.teaching_set.append(teaching_example)\n",
    "\n",
    "    def remove_last_teaching_item(self):\n",
    "        self.teaching_set = self.teaching_set[:-1]\n",
    "\n",
    "    def prior_rejector(self, ai_confidence = 0):\n",
    "        coin = random.random() # random number between [0,1]\n",
    "        if coin >= 0.5:\n",
    "            return 1\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1636490145837,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "tVg8_AVlrI-L"
   },
   "outputs": [],
   "source": [
    "def compute_predictions_humanai(hum_preds, hum_rejector, ai_preds, data_x):\n",
    "    '''\n",
    "    hum_preds: array of human predictions\n",
    "    ai_preds: array of AI predictions\n",
    "    hum_rejector: HumanLearner\n",
    "    data_x: array of inputs\n",
    "\n",
    "    Returns array of final predictions and deferalls\n",
    "    '''\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        reject_decisions = hum_rejector(data_x)\n",
    "        for i in range(len(data_x)):\n",
    "            if reject_decisions[i] == 1:\n",
    "                # defer\n",
    "                predictions.append(ai_preds[i])\n",
    "            else:\n",
    "                predictions.append(hum_preds[i])\n",
    "    return predictions, reject_decisions\n",
    "\n",
    "def get_metrics(preds, truths):\n",
    "    '''\n",
    "    preds: array of predictions\n",
    "    truths:  target array\n",
    "    '''\n",
    "    acc = metrics.accuracy_score(truths, preds)\n",
    "    metrics_computed = { \"accuracy\": acc}\n",
    "    return metrics_computed\n",
    "\n",
    "def compute_metrics(human_preds, ai_preds, reject_decisions, truths, to_print = False):\n",
    "    coverage = 1 - np.sum(reject_decisions)/len(reject_decisions)\n",
    "    humanai_preds = []\n",
    "    human_preds_sys = []\n",
    "    truths_human = []\n",
    "    ai_preds_sys = []\n",
    "    truths_ai = []\n",
    "    for i in range(len(reject_decisions)):\n",
    "        if reject_decisions[i] == 1:\n",
    "            humanai_preds.append(ai_preds[i])\n",
    "            ai_preds_sys.append(ai_preds[i])\n",
    "            truths_ai.append(truths[i])\n",
    "        else:\n",
    "            humanai_preds.append(human_preds[i])\n",
    "            human_preds_sys.append(human_preds[i])\n",
    "            truths_human.append(truths[i])\n",
    "    humanai_metrics = get_metrics(humanai_preds, truths)\n",
    "\n",
    "    human_metrics = get_metrics(human_preds_sys, truths_human)\n",
    "\n",
    "    ai_metrics = get_metrics(ai_preds_sys, truths_ai)\n",
    "\n",
    "    if to_print:\n",
    "        print(f'Coverage is {coverage*100:.2f}')\n",
    "        print(f' metrics of system are: {humanai_metrics}')\n",
    "        print(f' metrics of human are: {human_metrics}')\n",
    "        print(f' metrics of AI are: {ai_metrics}')\n",
    "    return coverage, humanai_metrics, human_metrics, ai_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1636490146066,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "chsOBVozzAe2"
   },
   "outputs": [],
   "source": [
    "class synth_expert:\n",
    "    '''\n",
    "    From https://github.com/clinicalml/learn-to-defer\n",
    "    expert who is perfect if class is in [0,k] and otherwise is uniform at random\n",
    "    k: [0,n_classes] \n",
    "    n_classes: number of classes in data\n",
    "    '''\n",
    "    def __init__(self, k, n_classes):\n",
    "        self.k = k\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def predict(self, input, labels):\n",
    "        batch_size = labels.size()[0]  # batch_size\n",
    "        outs = [0] * batch_size\n",
    "        for i in range(0, batch_size):\n",
    "            if labels[i].item() <= self.k:\n",
    "                outs[i] = labels[i].item()\n",
    "            else:\n",
    "                prediction_rand = random.randint(0, self.n_classes - 1)\n",
    "                outs[i] = prediction_rand\n",
    "        return outs\n",
    "\n",
    "class synth_expert_random:\n",
    "    '''\n",
    "    expert who is accurate at random with probability ~acc_level \n",
    "    acc_level: [0,1] probability of being correct\n",
    "    n_classes: number of classes in data\n",
    "    '''\n",
    "    def __init__(self, acc_level, n_classes):\n",
    "        self.acc_level = acc_level\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def predict(self, input, labels):\n",
    "        batch_size = labels.size()[0]  # batch_size\n",
    "        outs = [0] * batch_size\n",
    "        for i in range(0, batch_size):\n",
    "            coin = random.random()\n",
    "            if coin <= self.acc_level:\n",
    "                outs[i] = labels[i].item()\n",
    "            else:\n",
    "                prediction_rand = random.randint(0, self.n_classes - 1)\n",
    "                outs[i] = prediction_rand\n",
    "        return outs\n",
    "\n",
    "\n",
    "expert = synth_expert_random(0.7, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1636490146067,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "P-cAIckgzjJQ"
   },
   "outputs": [],
   "source": [
    "def metrics_print_baseline(net_class, expert_fn, n_classes, loader, epsilon):\n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs_class = net_class(images)\n",
    "            _, predicted = torch.max(outputs_class.data, 1)\n",
    "            batch_size = outputs_class.size()[0]  # batch_size\n",
    "            exp_prediction = expert_fn(images, labels)\n",
    "            for i in range(0, batch_size):\n",
    "                r = (max(outputs_class[i]) <= epsilon )\n",
    "                if r == 0:\n",
    "                    total += 1\n",
    "                    correct += (predicted[i] == labels[i]).item()\n",
    "                    correct_sys += (predicted[i] == labels[i]).item()\n",
    "                if r == 1:\n",
    "                    exp += (exp_prediction[i] == labels[i].item())\n",
    "                    correct_sys += (exp_prediction[i] == labels[i].item())\n",
    "                    exp_total += 1\n",
    "                real_total += 1\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "    to_print = {\"coverage\": cov, \"system accuracy\": 100 * correct_sys / real_total,\n",
    "                \"expert accuracy\": 100 * exp / (exp_total + 0.0002),\n",
    "                \"classifier accuracy\": 100 * correct / (total + 0.0001)}\n",
    "    #print(to_print)\n",
    "    return to_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1636490146067,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "nsrIGsE6tpAp"
   },
   "outputs": [],
   "source": [
    "# get optimal gammas, ONLY FOR TEACHING\n",
    "def get_optimal_gammas():\n",
    "    optimal_gammas = []\n",
    "    with tqdm(total=len(teaching_embeddings)) as pbar:\n",
    "        similarities_embeds_all = rbf_kernel( np.asarray(teaching_embeddings), np.asarray(teaching_embeddings))\n",
    "        for i in range(len(teaching_embeddings)):\n",
    "            # get all similarities\n",
    "            similarities_embeds = similarities_embeds_all[i]\n",
    "            opt_defer_ex = opt_defer_teaching[i]\n",
    "            opt_gamma = 1\n",
    "            sorted_sim = sorted([(similarities_embeds[k], opt_defer_teaching[k]) for k in range(len(teaching_embeddings))], key=lambda tup: tup[0])\n",
    "            indicess = list(range(1, len(opt_defer_teaching)))\n",
    "            indicess.reverse()\n",
    "            for k in indicess:\n",
    "                if sorted_sim[k][1] == opt_defer_ex and sorted_sim[k- 1][1] != opt_defer_ex:\n",
    "                    opt_gamma = sorted_sim[k][0]\n",
    "                    break\n",
    "            optimal_gammas.append(opt_gamma)\n",
    "            pbar.update(1)\n",
    "    return optimal_gammas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbVqS45kO9dh"
   },
   "source": [
    "The script below helps get embeddings on teaching and testing set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6859,
     "status": "ok",
     "timestamp": 1636490152922,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "DyhU5pSp8Y-B",
    "outputId": "c2cc1148-db61-4051-9a47-e1c04af839c3"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_classifier.eval()\n",
    "# now only for rad_1\n",
    "ai_teaching_preds = []\n",
    "ai_teaching_conf = []\n",
    "teaching_target = []\n",
    "hum_teaching_preds = []\n",
    "teaching_embeddings = []\n",
    "print(\"getting embeddings on teaching set\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for j, (input, target) in enumerate(train_val_loader):\n",
    "        target_np = target.to(device).detach().cpu().numpy()\n",
    "        input = input.to(device)\n",
    "        outputs_class = model_classifier(input)\n",
    "        outputs_conf = outputs_class.data.detach().cpu().numpy()\n",
    "        _, predicted = torch.max(outputs_class, 1)\n",
    "        predicted = predicted.detach().cpu().numpy()\n",
    "        embeddings = model_classifier.get_repr(input)\n",
    "        embeddings = embeddings.data.detach().cpu().numpy()\n",
    "        batch_size = outputs_class.size()[0]  # batch_size\n",
    "        exp_prediction = expert.predict(input, target)\n",
    "        for i in range(0, batch_size):\n",
    "            r = (max(outputs_conf[i]) <=0.8 )\n",
    "            ai_teaching_preds.append(predicted[i])\n",
    "            ai_teaching_conf.append(max(outputs_class[i]))\n",
    "            teaching_target.append(target_np[i])\n",
    "            hum_teaching_preds.append(exp_prediction[i])\n",
    "            teaching_embeddings.append(embeddings[i])\n",
    "\n",
    "teaching_embeddings = np.array(teaching_embeddings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54002,
     "status": "ok",
     "timestamp": 1636490206921,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "2YzkS4a60qgj",
    "outputId": "ce3c4ee9-386a-4c3a-ef9a-6c962fd4b7f6"
   },
   "outputs": [],
   "source": [
    "# get kernel matrix\n",
    "similarities_embeds_all = rbf_kernel(np.asarray(teaching_embeddings), np.asarray(teaching_embeddings))\n",
    "sorted_sims = []\n",
    "print(\"started\")\n",
    "for i in range(len(similarities_embeds_all)):\n",
    "    sorted_sim = sorted([(similarities_embeds_all[i][k], k) for k in range(len(teaching_embeddings))], key=lambda tup: tup[0])\n",
    "    sorted_sims.append(np.asarray(sorted_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jnOQmiJwqko"
   },
   "source": [
    "# Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkcciW9Awukp"
   },
   "source": [
    "## Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1636490206922,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "FpNKC2WGwqKl"
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "indicess = list(range(1, len(teaching_embeddings) -1 ))\n",
    "indicess.reverse()\n",
    "def get_improvement_defer_greedy(current_defer_preds, opt_defer_preds, xs, seen_indices):\n",
    "\n",
    "    error_improvements = []\n",
    "    error_at_i = 0\n",
    "    found_gammas = []\n",
    "    for i in range(len(opt_defer_preds)):\n",
    "        coin = random.random() # random number between [0,1]\n",
    "\n",
    "        similarities_embeds = similarities_embeds_all[i]\n",
    "        sorted_sim = sorted_sims[i] #sorted([(similarities_embeds[k], k) for k in range(len(teaching_embeddings))], key=lambda tup: tup[0])\n",
    "\n",
    "        max_improve = -1000\n",
    "        gamma_value = optimal_gammas[i]\n",
    "        current_improve = 0\n",
    "        so_far = 0\n",
    "        for j in indicess:\n",
    "            if i in seen_indices:\n",
    "                continue\n",
    "\n",
    "            so_far += 1\n",
    "            idx = int(sorted_sim[j][1])\n",
    "            f1_hum = hum_teaching_preds_b[idx]\n",
    "            f1_ai = ai_teaching_preds_b[idx]\n",
    "            if opt_defer_preds[i] == 1:\n",
    "                if current_defer_preds[idx] == 0:\n",
    "                    current_improve += f1_ai - f1_hum\n",
    "            else:\n",
    "                if current_defer_preds[idx] == 1:\n",
    "                    current_improve += f1_hum - f1_ai\n",
    "\n",
    "            if current_improve >= max_improve:\n",
    "                max_improve = current_improve \n",
    "                gamma_value = min(optimal_gammas[i], sorted_sim[j][0] )\n",
    "            \n",
    "        error_improvements.append(max_improve)\n",
    "        found_gammas.append(gamma_value)\n",
    "    return error_improvements, found_gammas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1636490206922,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "xPT8c6sIwzri"
   },
   "outputs": [],
   "source": [
    "def teach_ours_doublegreedy():\n",
    "    human_learner = HumanLearner(None)\n",
    "\n",
    "    errors = []\n",
    "    data_sizes  = []\n",
    "    indices_used = []\n",
    "    points_chosen = []\n",
    "    for itt in range(MAX_SIZE):\n",
    "        print(f'New size {itt}')\n",
    "        best_index = -1\n",
    "        # predict with current human learner\n",
    "        if itt == 0:\n",
    "            preds_teach = priorhum_teaching_preds\n",
    "        else:\n",
    "            preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "        error_improvements, best_gammas = get_improvement_defer_greedy(preds_teach, opt_defer_teaching,  teaching_embeddings, indices_used)\n",
    "        print(f'got improvements with max {max(error_improvements)}')\n",
    "        best_index = np.argmax(error_improvements)\n",
    "        indices_used.append(best_index) # add found element to set used\n",
    "        ex_embed = teaching_embeddings[best_index]\n",
    "        ex_label = opt_defer_teaching[best_index]\n",
    "        gamma = best_gammas[best_index] # + (np.random.rand(1)[0])*(1-optimal_gammas[best_index])-(1-optimal_gammas[best_index])/2 # random choice\n",
    "        human_learner.add_to_teaching([ex_embed, ex_label, gamma])\n",
    "\n",
    "        if False and itt % PLOT_INTERVAL == 0:\n",
    "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "            _, metricsc, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, teaching_target, True)\n",
    "            #errors.append(metricsc)   \n",
    "            print(\"##############################\")\n",
    "\n",
    "        if   itt % PLOT_INTERVAL == 0:\n",
    "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner.predict(testing_embeddings, priorhum_testing_preds)\n",
    "            _, metricsc, __, ___ = compute_metrics(hum_testing_preds, ai_testing_preds, preds_teach, testing_target, True)\n",
    "            errors.append(metricsc['accuracy'])   \n",
    "            print(\"##############################\")\n",
    "    return errors, indices_used\n",
    "#errors_doublegreedy, indices_used_doublegreedy = teach_ours_doublegreedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1636490206923,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "E-twqPc7w75z"
   },
   "outputs": [],
   "source": [
    "def get_improvement_defer(current_defer_preds, opt_defer_preds, gammas, xs, coin_prob = 0.1):\n",
    "    error_improvements = []\n",
    "    #similarities_embeds_all = rbf_kernel(np.asarray(xs), np.asarray(xs))\n",
    "    error_at_i = 0\n",
    "    for i in range(len(gammas)):\n",
    "        coin = random.random() # random number between [0,1]\n",
    "        error_at_i = 0\n",
    "        similarities_embeds = similarities_embeds_all[i]\n",
    "        for j in range(len(similarities_embeds)):\n",
    "            if similarities_embeds[j] >= gammas[i]:\n",
    "                f1_hum = hum_teaching_preds_b[j]\n",
    "                f1_ai = ai_teaching_preds_b[j]\n",
    "                if opt_defer_preds[i] == 1:\n",
    "                    if current_defer_preds[j] == 0:\n",
    "                        error_at_i += f1_ai - f1_hum\n",
    "                else:\n",
    "                    if current_defer_preds[j] == 1:\n",
    "                        error_at_i += f1_hum - f1_ai\n",
    "        error_improvements.append(error_at_i)\n",
    "\n",
    "        # get the ball for x\n",
    "        # in this ball how many does the current defer not match the optimal\n",
    "    return error_improvements\n",
    "\n",
    "\n",
    "\n",
    "def teach_ours(greedy_gamma = False):\n",
    "    human_learner = HumanLearner(None)\n",
    "    errors = []\n",
    "    data_sizes  = []\n",
    "    indices_used = []\n",
    "    points_chosen = []\n",
    "    for itt in range(MAX_SIZE):\n",
    "        print(f'New size {itt}')\n",
    "        best_index = -1\n",
    "        # predict with current human learner\n",
    "        if itt == 0:\n",
    "            preds_teach = priorhum_teaching_preds\n",
    "        else:\n",
    "            preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "        error_improvements = get_improvement_defer(preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
    "        best_index = np.argmax(error_improvements)\n",
    "        indices_used.append(best_index) # add found element to set used\n",
    "        ex_embed = teaching_embeddings[best_index]\n",
    "        ex_label = opt_defer_teaching[best_index]\n",
    "\n",
    "        if greedy_gamma:\n",
    "            _, greedy_gamma = get_greedy_gamma(best_index, preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
    "            gamma = greedy_gamma\n",
    "            print(f'got improvements with max {_}')\n",
    "        else:\n",
    "            gamma = optimal_gammas[best_index]\n",
    "            print(f'got improvements with max {max(error_improvements)}')\n",
    "\n",
    "        #gamma = optimal_gammas[best_index] # + (np.random.rand(1)[0])*(1-optimal_gammas[best_index])-(1-optimal_gammas[best_index])/2 # random choice\n",
    "        human_learner.add_to_teaching([ex_embed, ex_label, gamma])\n",
    "\n",
    "        if False and itt % 3 == 0:\n",
    "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "            _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, teaching_target, True)\n",
    "            #errors.append(metrics)   \n",
    "            print(\"##############################\")\n",
    "\n",
    "        if   itt % PLOT_INTERVAL == 0:\n",
    "\n",
    "            plt.imshow(  train_dataset[best_index][0].permute(1, 2, 0)  )\n",
    "            plt.show()\n",
    "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner.predict(testing_embeddings, priorhum_testing_preds)\n",
    "            _, metrics, __, ___ = compute_metrics(hum_testing_preds, ai_testing_preds, preds_teach, testing_target, True)\n",
    "            errors.append(metrics['accuracy'])   \n",
    "            print(\"##############################\")\n",
    "    return errors, indices_used\n",
    "#errors, indices_used = teach_ours(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQ4YAGApw3qJ"
   },
   "source": [
    "## Medoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1636490207433,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "P4rDtNHew5Sb"
   },
   "outputs": [],
   "source": [
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "def get_greedy_gamma(i, current_defer_preds, opt_defer_preds, gammas, xs):\n",
    "    similarities_embeds = similarities_embeds_all[i]\n",
    "    sorted_sim = sorted_sims[i]#sorted([(similarities_embeds[k], k) for k in range(len(teaching_embeddings))], key=lambda tup: tup[0])\n",
    "    indicess = list(range(1, len(opt_defer_teaching)))\n",
    "    indicess.reverse()\n",
    "    max_improve = -1000\n",
    "    gamma_value = 1\n",
    "    current_improve = 0\n",
    "    so_far = 0\n",
    "\n",
    "    for j in indicess:\n",
    "\n",
    "        so_far += 1\n",
    "        idx = int(sorted_sim[j][1])\n",
    "        #f1_hum = metric_max_over_ground_truths(f1_score, train_answers[idx], [hum_teaching_preds[idx]]) # pass as param plz\n",
    "        #f1_ai = metric_max_over_ground_truths(f1_score, train_answers[idx], [ai_teaching_preds[idx]])# pass as param plz\n",
    "        f1_hum = hum_teaching_preds_b[idx]\n",
    "        f1_ai = ai_teaching_preds_b[idx]\n",
    "        if opt_defer_preds[i] == 1:\n",
    "            if current_defer_preds[idx] == 0:\n",
    "                current_improve += f1_ai - f1_hum#2*(opt_defer_preds[idx]-0.5)#f1_ai - f1_hum\n",
    "        else:\n",
    "            if current_defer_preds[idx] == 1:\n",
    "                current_improve += f1_hum - f1_ai#2*(0.5-opt_defer_preds[idx]) #f1_hum - f1_ai\n",
    "\n",
    "        if current_improve >= max_improve:\n",
    "            max_improve = current_improve \n",
    "            gamma_value = sorted_sim[j][0]\n",
    "\n",
    "    return max_improve, gamma_value\n",
    "\n",
    "\n",
    "def teach_medoids(greedy_gamma = False):\n",
    "    \n",
    "    human_learner_medoid = HumanLearner(None)\n",
    "    errors_medoid = []\n",
    "    data_sizes  = []\n",
    "    indices_used_medoid = []\n",
    "    points_chosen = []\n",
    "    \n",
    "    for itt in range(MAX_SIZE):\n",
    "        indices_used_medoid = []\n",
    "\n",
    "        if False and itt % 3 == 0:\n",
    "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner_medoid.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "            _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, teaching_target, True)\n",
    "            #errors_medoid.append(metrics)   \n",
    "            print(\"##############################\")\n",
    "\n",
    "        if itt % PLOT_INTERVAL == 0:\n",
    "            human_learner_medoid = HumanLearner(None)\n",
    "            kmedoids = KMedoids(n_clusters=itt+1, method='alternate',max_iter=100).fit(teaching_embeddings)\n",
    "            teaching_indices = kmedoids.medoid_indices_\n",
    "\n",
    "            print(f'New size {itt}')\n",
    "            first_time = True\n",
    "            for teach_ex_idx in teaching_indices:\n",
    "                best_index = teach_ex_idx\n",
    "                if greedy_gamma:\n",
    "                    if itt == 0 or first_time:\n",
    "                        preds_teach = priorhum_teaching_preds\n",
    "                        first_time = False\n",
    "                    else:\n",
    "                        preds_teach = human_learner_medoid.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "                    _, greedy_gamma = get_greedy_gamma(best_index, preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
    "                    gamma = greedy_gamma\n",
    "                else:\n",
    "                    gamma = optimal_gammas[best_index]\n",
    "                ex_embed = teaching_embeddings[best_index]\n",
    "                ex_label = opt_defer_teaching[best_index]\n",
    "                indices_used_medoid.append(best_index)\n",
    "                 #+ (np.random.rand(1)[0])*(1-optimal_gammas[best_index])-(1-optimal_gammas[best_index])/2 # random choice\n",
    "                human_learner_medoid.add_to_teaching([ex_embed, ex_label, gamma])\n",
    "\n",
    "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner_medoid.predict(testing_embeddings, priorhum_testing_preds)\n",
    "            _, metrics, __, ___ = compute_metrics(hum_testing_preds, ai_testing_preds, preds_teach, testing_target, True)\n",
    "            errors_medoid.append(metrics['accuracy'])   \n",
    "            print(\"##############################\")\n",
    "    return errors_medoid, indices_used_medoid\n",
    "#errors_medoid, indices_used_medoid = teach_medoids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXOHkXbXw-ms"
   },
   "source": [
    "## Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1636490207434,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "x-D5TV8ww3NY"
   },
   "outputs": [],
   "source": [
    "def teach_random(greedy_gamma = False):\n",
    "    human_learner_random = HumanLearner(None)\n",
    "    errors_random = []\n",
    "    data_sizes  = []\n",
    "    indices_used_random = random.sample(list(range(len(teaching_embeddings))), MAX_SIZE) # used to take gradient steps\n",
    "    points_chosen = []\n",
    "    for itt in range(MAX_SIZE):\n",
    "        print(f'New size {itt}')\n",
    "        best_index = indices_used_random[itt]\n",
    "        ex_embed = teaching_embeddings[best_index]\n",
    "        ex_label = opt_defer_teaching[best_index]\n",
    "        if greedy_gamma:\n",
    "            if itt == 0 or first_time:\n",
    "                preds_teach = priorhum_teaching_preds\n",
    "                first_time = False\n",
    "            else:\n",
    "                preds_teach = human_learner_random.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "            _, greedy_gamma = get_greedy_gamma(best_index, preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
    "            gamma = greedy_gamma\n",
    "        else:\n",
    "            gamma = optimal_gammas[best_index]\n",
    "        human_learner_random.add_to_teaching([ex_embed, ex_label, gamma])\n",
    "\n",
    "\n",
    "        if False and itt % 3 == 0:\n",
    "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner_random.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "            _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, teaching_target, True)\n",
    "            #errors.append(metrics)   \n",
    "            print(\"##############################\")\n",
    "\n",
    "        if   itt % PLOT_INTERVAL == 0:\n",
    "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner_random.predict(testing_embeddings, priorhum_testing_preds)\n",
    "            _, metrics_c, __, ___ = compute_metrics(hum_testing_preds, ai_testing_preds, preds_teach, testing_target, True)\n",
    "            errors_random.append(metrics_c['accuracy'])   \n",
    "            print(\"##############################\")\n",
    "    return errors_random, indices_used_random\n",
    "#errors_random, indices_used_random = teach_random()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3NOLUKmxG-a"
   },
   "source": [
    "## LEARN AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1636490207435,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "1UvH6VsSxJcl"
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import RadiusNeighborsClassifier, KNeighborsClassifier\n",
    "def teach_learnai(greedy_gamma = False, knn_value = 5, sub_sampling = 1000):\n",
    "    human_learner_learnai = HumanLearner(None)\n",
    "    errors_learnai = []\n",
    "    data_sizes  = []\n",
    "    indices_used_learnai = []\n",
    "    points_chosen = []\n",
    "    set_xs = [teaching_embeddings[0]]\n",
    "    set_ys = [ai_teaching_preds_b[0]]\n",
    "    for itt in range(MAX_SIZE):\n",
    "        print(f'New size {itt}')\n",
    "        best_index = 0\n",
    "        best_value = 0\n",
    "        neigh = KNeighborsClassifier(n_neighbors = 1, weights='distance')\n",
    "        random_teach_subset = random.sample(list(range(len(teaching_embeddings))), sub_sampling) \n",
    "        random_test_subset = random.sample(list(range(len(teaching_embeddings))), sub_sampling) \n",
    "\n",
    "        for j in random_teach_subset:\n",
    "            if j in indices_used_learnai:\n",
    "                continue\n",
    "            x_try = teaching_embeddings[j]\n",
    "            y_try = ai_teaching_preds_b[j]\n",
    "            set_xs.append(x_try)\n",
    "            set_ys.append(y_try)\n",
    "            np_set_xs = np.asarray(set_xs)\n",
    "            np_set_ys = np.asarray(set_ys)\n",
    "            if len(np_set_xs) > knn_value:\n",
    "                neigh = KNeighborsClassifier(n_neighbors = knn_value, weights='distance')\n",
    "            else:\n",
    "                neigh = KNeighborsClassifier(n_neighbors = 1, weights='distance')\n",
    "            neigh.fit(np_set_xs, np_set_ys)\n",
    "            acc = neigh.score(np.asarray([teaching_embeddings[kk] for kk in random_test_subset]), np.asarray([ai_teaching_preds_b[kk] for kk in random_test_subset]))\n",
    "            if acc >= best_value:\n",
    "                best_value = acc\n",
    "                best_index = j\n",
    "            set_xs = set_xs[:-1]\n",
    "            set_ys = set_ys[:-1]\n",
    "        print(best_value)\n",
    "        indices_used_learnai.append(best_index)\n",
    "        ex_embed = teaching_embeddings[best_index]\n",
    "        ex_label = opt_defer_teaching[best_index]\n",
    "        \n",
    "        if greedy_gamma:\n",
    "            if itt == 0 :\n",
    "                preds_teach = priorhum_teaching_preds\n",
    "                first_time = False\n",
    "            else:\n",
    "                preds_teach = human_learner_learnai.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "                \n",
    "            _, greedy_gamma = get_greedy_gamma(best_index, preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
    "            gamma = greedy_gamma\n",
    "        else:\n",
    "            gamma = optimal_gammas[best_index]\n",
    "        human_learner_learnai.add_to_teaching([ex_embed, ex_label, gamma])\n",
    "\n",
    "\n",
    "\n",
    "        if False and itt % 3 == 0:\n",
    "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner_learnai.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "            _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, teaching_target, True)\n",
    "            #errors.append(metrics)   \n",
    "            print(\"##############################\")\n",
    "\n",
    "        if   itt % PLOT_INTERVAL == 0:\n",
    "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner_learnai.predict(testing_embeddings, priorhum_testing_preds)\n",
    "            _, metrics_c, __, ___ = compute_metrics(hum_testing_preds, ai_testing_preds, preds_teach, testing_target, True)\n",
    "            errors_learnai.append(metrics_c['accuracy'])   \n",
    "            print(\"##############################\")\n",
    "\n",
    "    return errors_learnai, indices_used_learnai\n",
    "#errors_learnai, indices_used_learnai = teach_learnai()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUFsgsy2xMhh"
   },
   "source": [
    "## LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1636490207435,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "t4aoZk5hxNtC"
   },
   "outputs": [],
   "source": [
    "def get_improvement_lime(features_covered, xs, indices_used_lime):\n",
    "    error_improvements = []\n",
    "    for i in range(len(xs)):\n",
    "        error_at_i = 0\n",
    "        if i in indices_used_lime:\n",
    "            error_at_i = -10000\n",
    "        for feat_id, feat_val in individual_feature_importance[i].items():\n",
    "            if features_covered[feat_id] == 0:\n",
    "                error_at_i +=  global_feature_importance[feat_id]\n",
    "                    \n",
    "        error_improvements.append(error_at_i)\n",
    "        # get the ball for x\n",
    "        # in this ball how many does the current defer not match the optimal\n",
    "    \n",
    "    best_index = np.argmax(error_improvements)\n",
    "    for feat_id, feat_val in individual_feature_importance[best_index].items():\n",
    "        if features_covered[feat_id] == 0:\n",
    "            features_covered[feat_id] = 1\n",
    "    return error_improvements, features_covered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1636490207809,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "A7k9a2vNxP6L"
   },
   "outputs": [],
   "source": [
    "def teach_lime(greedy_gamma = False):\n",
    "    human_learner = HumanLearner(None)\n",
    "    errors_lime = []\n",
    "    data_sizes  = []\n",
    "    indices_used_lime = {}\n",
    "    points_chosen = []\n",
    "    features_covered = {}\n",
    "    for i in range(DATA_DIM):\n",
    "        features_covered[i] = 0\n",
    "\n",
    "    for itt in range(MAX_SIZE):\n",
    "        print(f'New size {itt}')\n",
    "        best_index = -1\n",
    "        # predict with current human learner\n",
    "\n",
    "        error_improvements, features_covered = get_improvement_lime(features_covered, teaching_embeddings[:cutoff_size], indices_used_lime)\n",
    "        print(f'got improvements with max {max(error_improvements)}')\n",
    "        best_index = np.argmax(error_improvements)\n",
    "        indices_used_lime[best_index] =1 # add found element to set used\n",
    "        ex_embed = teaching_embeddings[best_index]\n",
    "        ex_label = opt_defer_teaching[best_index]\n",
    "\n",
    "        if greedy_gamma:\n",
    "            if itt == 0 :\n",
    "                preds_teach = priorhum_teaching_preds\n",
    "            else:\n",
    "                preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "                \n",
    "            _, greedy_gamma = get_greedy_gamma(best_index, preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n",
    "            gamma = greedy_gamma\n",
    "        else:\n",
    "            gamma = optimal_gammas[best_index]\n",
    "\n",
    "        #gamma = optimal_gammas[best_index]  #+ (np.random.rand(1)[0])*2*(1-optimal_gammas[best_index])-(1-optimal_gammas[best_index]) # random choice\n",
    "        human_learner.add_to_teaching([ex_embed, ex_label, gamma])\n",
    "\n",
    "\n",
    "\n",
    "        if False and itt % 3 == 0:\n",
    "            print(\"####### train eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n",
    "            _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, teaching_target, True)\n",
    "            #errors.append(metrics)   \n",
    "            print(\"##############################\")\n",
    "\n",
    "        if   itt % PLOT_INTERVAL == 0:\n",
    "            print(\"####### val eval \" +str(itt)+ \" ###########\")\n",
    "            preds_teach = human_learner.predict(testing_embeddings, priorhum_testing_preds)\n",
    "            _, metrics_c, __, ___ = compute_metrics(hum_testing_preds, ai_testing_preds, preds_teach, testing_target, True)\n",
    "            errors_lime.append(metrics_c['accuracy'])   \n",
    "            print(\"##############################\")\n",
    "\n",
    "    return errors_lime, indices_used_lime\n",
    "#errors_lime, indices_used_lime = teach_lime()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pK-C_oPfyLrx"
   },
   "source": [
    "# Experimental setup for expert deferral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1636490208561,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "22OZucprzhR3"
   },
   "outputs": [],
   "source": [
    "expert_k = 6 # expert classes able to classify\n",
    "expert = synth_expert(expert_k, 10)\n",
    "MAX_SIZE = 11\n",
    "PLOT_INTERVAL = 4\n",
    "greedy_gamma = True\n",
    "MAX_TRIALS = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286399,
     "status": "ok",
     "timestamp": 1636490494955,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "NgaCk1GW6O_P",
    "outputId": "6dc180da-86ee-4f77-8abb-2c7892a0f6e4"
   },
   "outputs": [],
   "source": [
    "# replace with optimization over epsilon\n",
    "print(\"Optimizing over prior rejector epsilon paramtetr\")\n",
    "max_eps = 0\n",
    "max_value = 0\n",
    "for eps_iter in range(50,100,2):\n",
    "    value_eps = metrics_print_baseline(model_classifier, expert.predict, 10, test_all_loader, eps_iter/100)['system accuracy'] # test_all_loader train_val_loader\n",
    "    if value_eps >= max_value:\n",
    "        print(max_value)\n",
    "        max_value = value_eps\n",
    "        max_eps = eps_iter/100\n",
    "print(f' on test with epsilon {max_eps}')\n",
    "print( metrics_print_baseline(model_classifier, expert.predict, 10, test_all_loader, max_eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1224984,
     "status": "ok",
     "timestamp": 1636491719935,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "HODOy7EzzqxC",
    "outputId": "76d56659-97a6-45be-cad6-9102fa27f896"
   },
   "outputs": [],
   "source": [
    "scores_ours = []\n",
    "scores_medoid = []\n",
    "scores_random = []\n",
    "scores_aibaseline = []\n",
    "scores_oracle = []\n",
    "scores_lime = []\n",
    "scores_human = []\n",
    "for trial in range(MAX_TRIALS):\n",
    "    # Get Human Predictions\n",
    "    print(f' \\n \\n trial {trial}  \\n \\n')\n",
    "\n",
    "    model_classifier.eval()\n",
    "    # now only for rad_1\n",
    "    ai_teaching_preds = []\n",
    "    ai_teaching_conf = []\n",
    "    teaching_target = []\n",
    "    hum_teaching_preds = []\n",
    "    teaching_embeddings = []\n",
    "    print(\"getting predictions on teaching set\")\n",
    "\n",
    "    priorhum_teaching_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, (input, target) in enumerate(train_val_loader):\n",
    "            target_np = target.to(device).detach().cpu().numpy()\n",
    "            input = input.to(device)\n",
    "            outputs_class = model_classifier(input)\n",
    "            outputs_conf = outputs_class.data.detach().cpu().numpy()\n",
    "            _, predicted = torch.max(outputs_class, 1)\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            embeddings = model_classifier.get_repr(input)\n",
    "            embeddings = embeddings.data.detach().cpu().numpy()\n",
    "            batch_size = outputs_class.size()[0]  # batch_size\n",
    "            exp_prediction = expert.predict(input, target)\n",
    "            for i in range(0, batch_size):\n",
    "                r = (max(outputs_conf[i]) >=0.5 )\n",
    "                priorhum_teaching_preds.append(r)\n",
    "                ai_teaching_preds.append(predicted[i])\n",
    "                ai_teaching_conf.append(max(outputs_class[i]))\n",
    "                teaching_target.append(target_np[i])\n",
    "                hum_teaching_preds.append(exp_prediction[i])\n",
    "                teaching_embeddings.append(embeddings[i])\n",
    "\n",
    "    teaching_embeddings = np.array(teaching_embeddings)\n",
    "\n",
    "\n",
    "    model_classifier.eval()\n",
    "    # now only for rad_1\n",
    "    ai_testing_preds = []\n",
    "    ai_testing_conf = []\n",
    "    testing_target = []\n",
    "    hum_testing_preds = []\n",
    "    testing_embeddings = []\n",
    "    print(\"getting predictions on testing set\")\n",
    "    priorhum_testing_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for j, (input, target) in enumerate(test_all_loader):\n",
    "            target_np = target.to(device).detach().cpu().numpy()\n",
    "            input = input.to(device)\n",
    "            outputs_class = model_classifier(input)\n",
    "            outputs_conf = outputs_class.data.detach().cpu().numpy()\n",
    "            _, predicted = torch.max(outputs_class, 1)\n",
    "            predicted = predicted.detach().cpu().numpy()\n",
    "            embeddings = model_classifier.get_repr(input)\n",
    "            embeddings = embeddings.data.detach().cpu().numpy()\n",
    "            batch_size = outputs_class.size()[0]  # batch_size\n",
    "            exp_prediction = expert.predict(input, target)\n",
    "            for i in range(0, batch_size):\n",
    "                r = (max(outputs_conf[i]) >=0.5 )\n",
    "                priorhum_testing_preds.append(r)\n",
    "                ai_testing_preds.append(predicted[i])\n",
    "                ai_testing_conf.append(max(outputs_class[i]))\n",
    "                testing_target.append(target_np[i])\n",
    "                hum_testing_preds.append(exp_prediction[i])\n",
    "                testing_embeddings.append(embeddings[i])\n",
    "\n",
    "\n",
    "    testing_embeddings = np.array(testing_embeddings)\n",
    "\n",
    "\n",
    "    opt_defer_teaching = []\n",
    "    for i in range(len(hum_teaching_preds)):\n",
    "        # optimal decision is to defer to AI only if not in the first k classes\n",
    "        if teaching_target[i] <= expert_k :\n",
    "            opt_defer_teaching.append(0)\n",
    "        else:\n",
    "            opt_defer_teaching.append(1)\n",
    "\n",
    "\n",
    "        '''\n",
    "        score_hum = hum_teaching_preds[i]== teaching_target[i]\n",
    "        score_ai = ai_teaching_preds[i] == teaching_target[i]\n",
    "\n",
    "        if score_ai > score_hum:\n",
    "            opt_defer_teaching.append(1)\n",
    "        elif score_hum >= score_ai:\n",
    "            opt_defer_teaching.append(0)\n",
    "        else:\n",
    "            opt_defer_teaching.append(random.randint(0,1))\n",
    "        '''\n",
    "\n",
    "    opt_defer_testing = []\n",
    "    for i in range(len(hum_testing_preds)):\n",
    "        if testing_target[i] <= expert_k :\n",
    "            opt_defer_testing.append(0)\n",
    "        else:\n",
    "            opt_defer_testing.append(1)\n",
    "        '''\n",
    "        score_hum = hum_testing_preds[i] == testing_target[i]\n",
    "        score_ai = round(ai_testing_preds[i])== testing_target[i]\n",
    "        if score_ai > score_hum:\n",
    "            opt_defer_testing.append(1)\n",
    "        elif score_hum >= score_ai:\n",
    "            opt_defer_testing.append(0)\n",
    "        else:\n",
    "            opt_defer_testing.append(random.randint(0,1))\n",
    "        '''\n",
    "\n",
    "\n",
    "\n",
    "    hum_teaching_preds_b = [ (hum_teaching_preds[i] == teaching_target[i]) * 1.0 for i in range(len(teaching_target))] \n",
    "    ai_teaching_preds_b = [(ai_teaching_preds[i] == teaching_target[i]) * 1.0  for i in range(len(teaching_target))] \n",
    "\n",
    "\n",
    "\n",
    "    ai_linear = LogisticRegression(random_state=0).fit(teaching_embeddings, ai_teaching_preds)\n",
    "    ai_linear.score(teaching_embeddings, ai_teaching_preds)\n",
    "\n",
    "    \n",
    "    \n",
    "    prior_score = compute_metrics(hum_testing_preds, ai_testing_preds, priorhum_testing_preds, testing_target, True)\n",
    "    optimal_score = compute_metrics(hum_testing_preds, ai_testing_preds, opt_defer_testing, testing_target, True)\n",
    "    prior_score = prior_score[1]['accuracy']\n",
    "\n",
    "    human_score =  compute_metrics(hum_testing_preds, ai_testing_preds, [0]*len(ai_testing_preds), testing_target, False)\n",
    "    ai_score =  compute_metrics(hum_testing_preds, ai_testing_preds, [1]*len(ai_testing_preds), testing_target, False)\n",
    "\n",
    "    scores_human.append(human_score[1]['accuracy'])\n",
    "    scores_oracle.append(optimal_score[1]['accuracy'])\n",
    "    indicess = list(range(1, len(teaching_embeddings)))\n",
    "    indicess.reverse()\n",
    "    if not greedy_gamma:\n",
    "        optimal_gammas = get_optimal_gammas()\n",
    "    else:\n",
    "        optimal_gammas = [1]*len(teaching_embeddings)\n",
    "        \n",
    "    print(\"running medoid\")\n",
    "    errors_medoid, indices_used_medoid = teach_medoids(greedy_gamma)        \n",
    "        \n",
    "    print(\"running our method\")\n",
    "    if greedy_gamma:\n",
    "        errors, indices_used = teach_ours_doublegreedy()\n",
    "\n",
    "    else:\n",
    "        errors, indices_used = teach_ours(False)\n",
    "\n",
    "    print(\"running learnai\")\n",
    "    errors_learnai, indices_used_learnai = teach_learnai(greedy_gamma)\n",
    "\n",
    "    print(\"running lime\")\n",
    "    DATA_DIM = teaching_embeddings.shape[1]\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(teaching_embeddings, discretize_continuous= False)\n",
    "    global_feature_importance = {}\n",
    "    cutoff_size = min(20000,len(teaching_embeddings))\n",
    "    for i in range(DATA_DIM):\n",
    "        global_feature_importance[i] = 0\n",
    "    individual_feature_importance = []\n",
    "    with tqdm(total=len(teaching_embeddings)) as pbar:\n",
    "        for ex in range(cutoff_size):\n",
    "            exp = explainer.explain_instance(teaching_embeddings[ex], ai_linear.predict_proba,  num_samples = 100)\n",
    "            feat_weights = exp.local_exp[1]\n",
    "            dic_feat_weights = {}\n",
    "            for j in range(len(feat_weights)):\n",
    "                dic_feat_weights[feat_weights[j][0]] = abs(feat_weights[j][1])\n",
    "                global_feature_importance[feat_weights[j][0]] += abs(feat_weights[j][1])\n",
    "            individual_feature_importance.append(dic_feat_weights)\n",
    "            pbar.update(1)\n",
    "\n",
    "    for i in range(DATA_DIM):\n",
    "        global_feature_importance[i] = math.sqrt(global_feature_importance[i])\n",
    "    errors_lime, indices_used_lime= teach_lime(greedy_gamma)\n",
    "\n",
    "    \n",
    "    print(\"running random\")\n",
    "    errors_random, indices_used_random = teach_random(greedy_gamma)\n",
    "        \n",
    "               \n",
    "    errors.insert(0, prior_score)\n",
    "    errors_learnai.insert(0, prior_score)\n",
    "    errors_medoid.insert(0, prior_score)\n",
    "    errors_random.insert(0, prior_score)\n",
    "    errors_lime.insert(0, prior_score)\n",
    "    \n",
    "    scores_ours.append(errors)\n",
    "    scores_aibaseline.append(errors_learnai)\n",
    "    scores_medoid.append(errors_medoid)\n",
    "    scores_random.append(errors_random)\n",
    "    scores_lime.append(errors_lime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1636491719936,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "1VjtB_penEM_"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "plt.rc('text', usetex=False)\n",
    "plt.rc('font', family='serif')\n",
    "def get_conf_interval(arr):\n",
    "    alpha_level = 0.4\n",
    "    err  = st.t.interval(alpha_level, len(arr)-1, loc=np.mean(arr), scale=st.sem(arr))[1]/2  - st.t.interval(alpha_level, len(arr)-1, loc=np.mean(arr), scale=st.sem(arr))[0]/2 \n",
    "    return err\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 750,
     "status": "ok",
     "timestamp": 1636491720681,
     "user": {
      "displayName": "Hussein Mozannar",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03221912190329599144"
     },
     "user_tz": 300
    },
    "id": "nZLLLaEtnEM_",
    "outputId": "2855d443-76a0-4650-e332-20fe740d9901"
   },
   "outputs": [],
   "source": [
    "\n",
    "teaching_sizes = [PLOT_INTERVAL*i for i in range(0,math.floor(MAX_SIZE/PLOT_INTERVAL))]\n",
    "actual_max_trials = len(scores_ours) \n",
    "\n",
    "avgs_rand = [np.average([ scores_oracle[triall] - scores_ours[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "stds_rand = [np.std([scores_oracle[triall] - scores_ours[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "plt.errorbar(teaching_sizes,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'DOUBLE-GREEDY (Ours)')\n",
    "\n",
    "print(avgs_rand)\n",
    "print(stds_rand)\n",
    "avgs_rand = [np.average([scores_oracle[triall] - scores_medoid[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "stds_rand = [np.std([ scores_oracle[triall] -scores_medoid[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "plt.errorbar(teaching_sizes,  avgs_rand, yerr=stds_rand, marker = \"s\",   label=f'K-Medoids')\n",
    "\n",
    "print(avgs_rand)\n",
    "print(stds_rand)\n",
    "avgs_rand = [np.average([scores_oracle[triall] - scores_random[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "stds_rand = [np.std([scores_oracle[triall] -scores_random[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "#plt.errorbar(list(range(1,len(teaching_sizes)+1)),  avgs_rand, yerr=stds_rand, label=f'random')\n",
    "plt.errorbar(teaching_sizes,  avgs_rand,yerr=stds_rand, marker = \"v\",  label=f'Random')\n",
    "print(avgs_rand)\n",
    "print(stds_rand)\n",
    "\n",
    "avgs_rand = [np.average([scores_oracle[triall]- scores_aibaseline[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "stds_rand = [np.std([scores_oracle[triall] -scores_aibaseline[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "plt.errorbar(teaching_sizes,  avgs_rand,yerr=stds_rand, marker = \"*\",   label=f'AI-Behavior')\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([scores_oracle[triall] - scores_lime[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "stds_rand = [np.std([scores_oracle[triall] -scores_lime[triall][i] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "#plt.errorbar(list(range(1,len(teaching_sizes)+1)),  avgs_rand, yerr=stds_rand, label=f'random')\n",
    "plt.errorbar(teaching_sizes,  avgs_rand,yerr=stds_rand, marker = \"+\",  label=f'LIME')\n",
    "avgs_rand = [np.average([scores_oracle[triall] - scores_human[triall] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "stds_rand = [np.std([scores_oracle[triall] -scores_human[triall] for triall in range(actual_max_trials)]) for i in range(len(teaching_sizes))]\n",
    "#plt.errorbar(list(range(1,len(teaching_sizes)+1)),  avgs_rand, yerr=stds_rand, label=f'random')\n",
    "#plt.errorbar(teaching_sizes,  avgs_rand,yerr=stds_rand, marker = \"x\",  label=f'Human alone')\n",
    "#plt.errorbar(teaching_sizes[0],  avgs_rand[0],yerr=stds_rand[0], marker = \"x\",  label=f'Human alone {avgs_rand[0]:.2f} $\\pm$ {stds_rand[0]:.2f}')\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()   \n",
    "plt.grid()\n",
    "plt.legend(fontsize='large')\n",
    "plt.legend()\n",
    "plt.ylabel('Difference to Oracle Accuracy',  fontsize='x-large')\n",
    "plt.xlabel('Teaching set size', fontsize='x-large')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 6\n",
    "fig_size[1] = 4\n",
    "plt.savefig(\"teaching_complexity_cifar10.pdf\", dpi = 1000)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "qHFgm6GQAlQ0",
    "MhX-OFF-LiSS",
    "hGwrCKLtPmWM",
    "bYMsE6OnPoRE",
    "FYMRUZCgvMSn",
    "_jnOQmiJwqko",
    "QkcciW9Awukp",
    "tQ4YAGApw3qJ",
    "fXOHkXbXw-ms",
    "SUFsgsy2xMhh"
   ],
   "name": "cifar_figures_github.ipynb",
   "provenance": [
    {
     "file_id": "1xazNo65bA7Cr-LaByU4_oA4qTZEhZqA6",
     "timestamp": 1636395211723
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08cf2293f3284a56bb2cc8f34171501e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1cbfb1adc9ac4966a4877f233907ef9b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2696cc728ddc40e692d748e5f8a354c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4a6343b9d86e452d9a9f6086471ce563": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74859dca02064da095dfec8ccc3fcc35": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1cbfb1adc9ac4966a4877f233907ef9b",
      "placeholder": "",
      "style": "IPY_MODEL_c510817b77284c04bb513efd2e7902e0",
      "value": " 170499072/? [00:06&lt;00:00, 31606767.35it/s]"
     }
    },
    "99fb6bb699d84023ba4bc25bed2b5716": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08cf2293f3284a56bb2cc8f34171501e",
      "placeholder": "",
      "style": "IPY_MODEL_dcdfc608eedb4ad8994442b9e1439509",
      "value": ""
     }
    },
    "ab495803a8f045d3bcba2017c39e41e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4a6343b9d86e452d9a9f6086471ce563",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dde3bba679ed400baec4e4cc013d5890",
      "value": 170498071
     }
    },
    "c510817b77284c04bb513efd2e7902e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dcdfc608eedb4ad8994442b9e1439509": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dde3bba679ed400baec4e4cc013d5890": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ee71b790eef1495289dcd5644f48d1bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99fb6bb699d84023ba4bc25bed2b5716",
       "IPY_MODEL_ab495803a8f045d3bcba2017c39e41e1",
       "IPY_MODEL_74859dca02064da095dfec8ccc3fcc35"
      ],
      "layout": "IPY_MODEL_2696cc728ddc40e692d748e5f8a354c3"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
