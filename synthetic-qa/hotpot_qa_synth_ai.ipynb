{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"hotpot_qa_synth_ai.ipynb","provenance":[],"collapsed_sections":["UP1CACu3irWL","UpcZrrMcwNWP","QKfkqiNTwNWI","tCCcnqUbLBnp","VhD70Q-dwNWS","k-L_wImhCL-o","rBHrWz39CjhT","iN5k79djCpJC","VMCUulSHCy78"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UP1CACu3irWL"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"nxZKWKFX1vde"},"source":["!pip install transformers\n","!pip install sentence_transformers\n","!pip install datasets\n","!pip install sentencepiece\n","!pip install scikit-learn-extra\n","!pip install wikipedia\n","!pip install eli5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"meSB2v0g0EH2"},"source":["from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)\n","from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JRNFoaJXvO4x"},"source":["import math\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import argparse\n","from __future__ import print_function\n","from collections import Counter\n","import string\n","import re\n","import argparse\n","import json\n","import os\n","import random\n","import shutil\n","import wikipedia\n","import time\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","from tqdm import tqdm\n","import torch.optim\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.autograd import Variable\n","from PIL import Image\n","import torch.utils.data as data\n","from torchvision.datasets.utils import download_url, check_integrity\n","import sys\n","from sklearn.decomposition import PCA\n","from matplotlib import pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","from sklearn.gaussian_process.kernels import RBF\n","import torch\n","from sklearn.metrics.pairwise import rbf_kernel\n","\n","from scipy.stats import multivariate_normal\n","import  scipy.stats as st\n","from matplotlib import cm\n","import torch.optim as optim\n","from __future__ import print_function\n","from transformers import AutoTokenizer, AutoModelForQuestionAnswering, AdamW, AutoModel\n","import nltk\n","from torch.utils.data import DataLoader\n","import pickle\n","import spacy\n","from gensim import corpora, models, similarities\n","import gensim\n","from spacy.lang.en import English\n","from nltk.corpus import wordnet as wn\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from sklearn.cluster import KMeans\n","import pickle\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","import matplotlib\n","from sklearn.metrics.pairwise import rbf_kernel\n","import copy\n","import eli5\n","from eli5.lime import TextExplainer\n","matplotlib.rcParams['pdf.fonttype'] = 42\n","matplotlib.rcParams['ps.fonttype'] = 42\n","plt.rc('text', usetex=False)\n","plt.rc('font', family='serif')\n","import nltk\n","nltk.download('punkt')\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","stopwords = stopwords.words('english')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5WSnSiY1dF2E"},"source":["# HotpotQA\n"]},{"cell_type":"code","metadata":{"id":"e-1r7KAwnJZJ"},"source":["DISTRACTORS_PARS_LEN = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3E0eakH8dHfv"},"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"hotpot_qa\", 'distractor')\n","train_dataset = dataset['train']\n","validation_dataset = dataset['validation']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bqzhee23lkrz"},"source":["def get_token_lenght(context, question):\n","    inputs = tokenizer.encode_plus(question, context, add_special_tokens=True, return_tensors=\"pt\")\n","    return len(inputs['input_ids'][0])\n","\n","def remove_distractors(example, n_distractors = 0):\n","    all_pars = example['context']['title']\n","    gold_pars = list(set(example['supporting_facts']['title']))\n","    distractor_pars = list(set(all_pars) - set(gold_pars))\n","    # get indices to keep from disractors\n","    if len(distractor_pars) == 0 or n_distractors == 0:\n","        distract_indices = []\n","    else:\n","        distract_indices = random.sample(range(len(distractor_pars)), n_distractors)\n","    distractor_pars = [distractor_pars[idx] for idx in distract_indices]\n","    keep_pars = gold_pars + distractor_pars\n","    keep_pars_indices = [all_pars.index(keep_par) for keep_par in keep_pars]\n","    example['context']['title'] = [example['context']['title'][idx] for idx in keep_pars_indices]\n","    example['context']['sentences'] = [example['context']['sentences'][idx] for idx in keep_pars_indices]\n","    \n","    sentences_par = [\"\".join(example['context']['sentences'][idx]) for idx in range(len(example['context']['sentences']))]\n","    example['intros'] = \" \".join([example['context']['sentences'][idx][0] for idx in range(len(example['context']['sentences']))])\n","    example['passage'] = \" #$ \".join(sentences_par)\n","    return example\n","\n","def remove_distractors_remove_key_sent(example, n_distractors = 0):\n","    all_pars = example['context']['title']\n","    gold_pars = list(set(example['supporting_facts']['title']))\n","    distractor_pars = list(set(all_pars) - set(gold_pars))\n","    # get indices to keep from disractors\n","    if len(distractor_pars) == 0 or n_distractors == 0:\n","        distract_indices = []\n","    else:\n","        distract_indices = random.sample(range(len(distractor_pars)), n_distractors)\n","    distractor_pars = [distractor_pars[idx] for idx in distract_indices]\n","    keep_pars = gold_pars + distractor_pars\n","    keep_pars_indices = [all_pars.index(keep_par) for keep_par in keep_pars]\n","    example['context']['title'] = [example['context']['title'][idx] for idx in keep_pars_indices]\n","    example['context']['sentences'] = [example['context']['sentences'][idx] for idx in keep_pars_indices]\n","\n","    # removed all distractors, now we remove a key sentence\n","    rand_sup_idx =random.sample(list(range(len(example['supporting_facts']['sent_id']))), 1)[0] \n","    title_to_index = {}\n","    for i in range(len(example['context']['title'])):\n","        title_to_index[example['context']['title'][i]] = i\n","    par_to_remove = title_to_index[example['supporting_facts']['title'][rand_sup_idx]]\n","    sent_to_remove = example['supporting_facts']['sent_id'][rand_sup_idx]\n","    sentencess = copy.deepcopy(example['context']['sentences'])\n","    able_to_remove = False\n","    times_tried = 0\n","    while not able_to_remove:\n","        if times_tried >4:\n","            print(\"giving up\")\n","            break\n","        try:\n","            sent = sentencess[par_to_remove][sent_to_remove]\n","            if example['answer'] in sent:\n","                raise Exception('error')\n","            #sentencess[par_to_remove].pop(sent_to_remove)\n","            sentencess[par_to_remove][sent_to_remove] = \" [...] . \"\n","            able_to_remove = True\n","        except:\n","            times_tried += 1\n","            print(\"failed trying again\")\n","            rand_sup_idx =random.sample(list(range(len(example['supporting_facts']['sent_id']))), 1)[0] \n","            title_to_index = {}\n","            for i in range(len(example['context']['title'])):\n","                title_to_index[example['context']['title'][i]] = i\n","            par_to_remove = title_to_index[example['supporting_facts']['title'][rand_sup_idx]]\n","            sent_to_remove = example['supporting_facts']['sent_id'][rand_sup_idx]\n","            sentencess = copy.deepcopy(example['context']['sentences'])\n","\n","    example['intros'] = \" \".join([sentencess[idx][0] for idx in range(len(sentencess)) if len(sentencess[idx])>0])\n","\n","    sentences_par = [\"\".join(sentencess[idx]) for idx in range(len(sentencess))]\n","\n","    example['passage'] = \" #$ \".join(sentences_par)\n","    return example\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"unz9KJ9qofO3"},"source":["train_dataset_hum = train_dataset.map(remove_distractors_remove_key_sent)\n","validation_dataset_hum = validation_dataset.map(remove_distractors_remove_key_sent)\n","train_dataset = train_dataset_hum#train_dataset.map(remove_distractors_remove_key_sent)\n","validation_dataset = validation_dataset_hum#validation_dataset.map(remove_distractors_remove_key_sent)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k1AI46NXwNV-"},"source":["# Free form vs yes/no\n","all_indices = set(list(range(len(validation_dataset))))\n","yes_no_indices = set([i  for i in range(len(validation_dataset)) if validation_dataset[i]['answer'] in [\"yes\",\"no\"] ])\n","free_indices_val = all_indices - yes_no_indices\n","all_indices = set(list(range(len(train_dataset))))\n","yes_no_indices_train = set([i  for i in range(len(train_dataset)) if (train_dataset[i]['answer'] in [\"yes\",\"no\"]) or (train_dataset[i]['level'] in ['easy','medium']) ])\n","free_indices_train = all_indices - yes_no_indices_train\n","print(len(free_indices_val))\n","print(len(free_indices_train))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G6fP4_P2wNWA"},"source":["hard_dataset = [train_dataset[i] for i in free_indices_train ]\n","hard_dataset += [validation_dataset[i] for i in free_indices_val]\n","dataset_indices =  set(list(range(len(hard_dataset))))\n","VAL_START = len([train_dataset[i] for i in free_indices_train ])\n","len(hard_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_KztU5GwNWB"},"source":["VAL_START"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C9AO1lwQuRcl"},"source":["hard_dataset[0]['passage']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AK9JLhs-97wO"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"40Mqih9Z8rTx"},"source":["\n","def normalize_answer(s):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    def remove_articles(text):\n","        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n","\n","    def white_space_fix(text):\n","        return ' '.join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return ''.join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_articles(remove_punc(lower(s))))\n","\n","\n","def f1_score(prediction, ground_truth):\n","    prediction_tokens = normalize_answer(prediction).split()\n","    ground_truth_tokens = normalize_answer(ground_truth).split()\n","    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n","    num_same = sum(common.values())\n","    if num_same == 0:\n","        return 0\n","    precision = 1.0 * num_same / len(prediction_tokens)\n","    recall = 1.0 * num_same / len(ground_truth_tokens)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","\n","\n","def exact_match_score(prediction, ground_truth):\n","    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n","\n","\n","def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n","    scores_for_ground_truths = []\n","    for ground_truth in ground_truths:\n","        score = metric_fn(prediction, ground_truth)\n","        scores_for_ground_truths.append(score)\n","    return max(scores_for_ground_truths)\n","\n","\n","\n","def evaluate_truth_pred(truths, preds):\n","    '''\n","    truths, preds: matched arrays of ground truth answers and predictions\n","    '''\n","    f1 = exact_match = total = 0\n","    for i in range(len(truths)):\n","        total += 1\n","        if truths[i] in ['yes', \"no\"]:\n","            continue\n","        ground_truths = [truths[i]]\n","        prediction = preds[i]\n","        exact_match += metric_max_over_ground_truths(\n","            exact_match_score, prediction, ground_truths)\n","        f1 += metric_max_over_ground_truths(\n","            f1_score, prediction, ground_truths)\n","\n","    exact_match = 100.0 * exact_match / (total+ 0.00000000001)\n","    f1 = 100.0 * f1 / (total+ 0.00000000001)\n","\n","    return {'exact_match': exact_match, 'f1': f1}\n","\n","\n","def get_answer( model, tokenizer, context, question):\n","    # 1. TOKENIZE THE INPUT\n","    # note: if you don't include return_tensors='pt' you'll get a list of lists which is easier for\n","    # exploration but you cannot feed that into a model.\n","    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n","    inputs = inputs.to(device)\n","    # 2. OBTAIN MODEL SCORES\n","    # the AutoModelForQuestionAnswering class includes a span predictor on top of the model.\n","    # the model returns answer start and end scores for each word in the text\n","    answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)\n","    answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n","    answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n","    # 3. GET THE ANSWER SPAN\n","    # once we have the most likely start and end tokens, we grab all the tokens between them\n","    # and convert tokens back to words!\n","    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n","    return answer\n","# https://huggingface.co/transformers/migration.html\n","def model_evaluation(model, tokenizer, questions, contexts, answers, to_print = False):\n","    preds = []\n","    my_list = list(range(len(questions)))\n","    with tqdm(total=len(my_list)) as pbar:\n","        for ex in range(len(questions)):\n","            answer = get_answer(model, tokenizer, contexts[ex],questions[ex])\n","            preds.append(answer)\n","            if ex % 100 == 0 and to_print:\n","                print(\"context \" +contexts[ex] )\n","                print(\"quest \" +questions[ex] )\n","                print(\"truth \" +answers[ex]['text'] )\n","                print(\"pred \" + answer)\n","            pbar.update(1)\n","    truths = [answers[i]['text'] for i in range(len(answers))]\n","    scores = evaluate_truth_pred(truths, preds)\n","    print(scores)\n","    return scores\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LjLzhebZqo8w"},"source":["from sklearn.metrics.pairwise import rbf_kernel\n","def kernel_similarity(x, y):\n","        kernel_dist = rbf_kernel(x.reshape(1, -1),y.reshape(1, -1))\n","        return kernel_dist[0][0]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UpcZrrMcwNWP"},"source":["# FakeAI model"]},{"cell_type":"code","metadata":{"id":"tbyNu1-WwNWQ"},"source":["class FakeAI:\n","    r\"\"\"\n","    FakeAI that is incorrect in random clusters in the input space\n","    \n","    Args:\n","        train_passages: list of embedded passages\n","        train_questions: list of embedded questions\n","        n_clus_p: number of centroids in kmeans for passages\n","        n_clus_q: number of centroids in kmeans for questions\n","        clust_err_p: clusters where AI makes error on passages\n","        clust_err_q: clusters where AI makes error on questions\n","    \"\"\"\n","    def __init__(self, train_passages, train_questions, n_clus_p, n_clus_q, n_clust_err_p, n_clust_err_q):\n"," \n","        self.train_passages = train_passages\n","        self.train_questions = train_questions\n","        self.n_clus_p = n_clus_p\n","        self.n_clus_q = n_clus_q\n","        self.n_clust_err_p = n_clust_err_p\n","        self.n_clust_err_q = n_clust_err_q\n","        self.build_kmeans()\n","        self.clust_err_p = random.sample(list(range(self.n_clus_p)), self.n_clust_err_p)\n","        self.clust_err_q = random.sample(list(range(self.n_clus_q)), self.n_clust_err_q)\n","        \n","        \n","    def build_kmeans(self):\n","        # Builds kmeans for passages and questions\n","        self.kmeans_quest = KMeans(n_clusters=self.n_clus_q, max_iter = 10000).fit(self.train_questions)\n","        self.kmeans_pass = KMeans(n_clusters=self.n_clus_p, max_iter = 100000).fit(self.train_passages)\n","\n","    def predict_right_wrong(self, passages, questions):\n","        '''\n","        Args:\n","            passages: list of embedded passages\n","            questions: list of embedded questions (same size as passages)\n","        Returns:\n","            preds: binary array indicating if AI is right (1) or wrong (0)\n","        '''\n","        clusts_p = self.kmeans_pass.predict(passages)\n","        clusts_q = self.kmeans_quest.predict(questions)\n","        preds = []\n","        for i in range(len(clusts_p)):\n","            if (clusts_p[i] in self.clust_err_p) or (clusts_q[i] in self.clust_err_q):\n","                preds.append(0)\n","            else:\n","                preds.append(1)\n","        return preds\n","\n","    def predict_proba(self, raw_passages):\n","        embed_ps = model.encode(raw_passages)\n","        embed_ps = [embed_ps[i] for i in range(len(embed_ps))]\n","        clusts_p = self.kmeans_pass.predict(embed_ps)\n","        preds = []\n","        for i in range(len(clusts_p)):\n","            if (clusts_p[i] in self.clust_err_p):\n","                preds.append([0.0,1.0])\n","            else:\n","                preds.append([1.0,0.0])\n","        return np.asarray(preds)\n","\n","\n","def is_a_stopword(feature, weight):\n","    split_words = feature.split(' ')\n","    for word in split_words:\n","        if word in stopwords:\n","            return False\n","    return True\n","\n","def get_highlighted_p(ai_model, paragraph):\n","    '''\n","    from https://eli5.readthedocs.io/en/latest/tutorials/black-box-text-classifiers.html#lime-tutorial\n","    '''\n","    \n","    te = TextExplainer(random_state=42)\n","    te.fit(paragraph, ai_model.predict_proba)\n","    show_pred = te.show_prediction(target_names=['correct','false'], top=(8,0), feature_filter = is_a_stopword)\n","    correct = False\n","    if show_pred.data.find(\"y=correct\") != -1:\n","        correct = True\n","    b = show_pred.data.split('<p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">')\n","    to_show = '<p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">' +b[-1]\n","    if not correct:\n","        to_show = to_show.replace(\"hsl(120,\",\"hsl(0,\")\n","    return to_show"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jN6a89iFAk25"},"source":["from sentence_transformers import SentenceTransformer\n","model = SentenceTransformer('paraphrase-distilroberta-base-v1') #distilbert-base-nli-stsb-mean-tokens\n","# https://www.sbert.net/docs/pretrained_models.html"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QKfkqiNTwNWI"},"source":["# Check clusters "]},{"cell_type":"code","metadata":{"id":"udqWX44uwNWJ"},"source":["all_passages = [hard_dataset[i]['passage'] for i in dataset_indices]\n","embeddings_passage = model.encode(all_passages)\n","\n","all_questions = [hard_dataset[i]['question'] for i in dataset_indices]\n","embeddings_question = model.encode(all_questions)\n","\n","all_answers = [hard_dataset[i]['answer'] for i in dataset_indices]\n","embeddings_ans= model.encode(all_answers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E6yT8NMLlp9x"},"source":["all_passages = [hard_dataset[i]['passage'] for i in range(VAL_START,len(dataset_indices))]\n","embeddings_passage = model.encode(all_passages)\n","\n","all_intros = [hard_dataset[i]['intros'] for i in range(VAL_START,len(dataset_indices))]\n","embeddings_intro = model.encode(all_intros)\n","\n","all_questions = [hard_dataset[i]['question'] for i in range(VAL_START,len(dataset_indices))]\n","embeddings_question = model.encode(all_questions)\n","\n","all_answers = [hard_dataset[i]['answer'] for i in range(VAL_START,len(dataset_indices))]\n","embeddings_ans= model.encode(all_answers)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-LzuJSyRwNWL"},"source":["N_CLUSTERS = 20\n","for k in range(5,30):\n","    kmeans_pass = KMeans(n_clusters=k,max_iter = 1000, verbose = False, tol = 0.0001, n_init = 1,random_state = 66).fit(embeddings_passage)\n","    print(f'For k {k} inertia is {kmeans_pass.inertia_:.3f}')    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9cMLMFQ47NH"},"source":["N_CLUSTERS = 13\n","kmeans_pass = KMeans(n_clusters=N_CLUSTERS,max_iter = 100000, verbose = False, tol = 0.0001, n_init = 30).fit(embeddings_passage)\n","print(f'For k {N_CLUSTERS} inertia is {kmeans_pass.inertia_:.3f}')    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s9i0ujivg7R3"},"source":["while True:\n","    a= 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uqpCebZgwNWM"},"source":["plt.hist(kmeans_pass.labels_, bins=N_CLUSTERS)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3uHfx1NDBJq1"},"source":["Understand the topic of clusters from wikipedia categories of articles"]},{"cell_type":"code","metadata":{"id":"hT3ENuSlwNWN"},"source":["from sklearn_extra.cluster import KMedoids\n","\n","passages = [[] for _ in range(N_CLUSTERS)]\n","embeddings = [[] for _ in range(N_CLUSTERS)]\n","categories = [[] for _ in range(N_CLUSTERS)]\n","for i in range(len(all_passages)):\n","    cluster_index = kmeans_pass.labels_[i]\n","    passages[cluster_index].append(all_passages[i])\n","    embeddings[cluster_index].append(embeddings_passage[i])\n","\n","for i in range(N_CLUSTERS):\n","    print(\"########################################\")\n","    print(\"############## CLUSTER \"+str(i)+\"##############\")\n","    print(\"########################################\")\n","    #if i not in [5]:\n","    #    continue\n","    \n","    passage_sub = random.sample(list(range(len(passages[i]))),5)\n","    for j in passage_sub:\n","        print(\"%%%%%%%%%%%%%%%\")\n","        print(passages[i][j])    \n","    \n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EOn4RzPOVuZU"},"source":["kmeans_pass.cluster_centers_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6dEIOOl9VwtM"},"source":["similarity = rbf_kernel(kmeans_pass.cluster_centers_,kmeans_pass.cluster_centers_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tafHosScWVe6"},"source":["\n","plt.imshow(similarity, cmap='hot', interpolation='nearest')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j5J82z-ZWtGz"},"source":["kmeans_pass2 = KMeans(n_clusters=10,max_iter = 1000, verbose = False, tol = 0.0001, n_init = 1,random_state = 66).fit(kmeans_pass.cluster_centers_)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wr6EMkPYW2tl"},"source":["kmeans_pass2.labels_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kSsy2YKRYdd7"},"source":["label_to_newlabel = {}\n","labels_seen = {}\n","for i in range(len(kmeans_pass.cluster_centers_)):\n","    newlabel = kmeans_pass2.labels_[i]\n","    if newlabel in labels_seen:\n","        label_to_newlabel[i] = -1\n","    else:\n","        labels_seen[newlabel] = 1\n","        label_to_newlabel[i] = newlabel"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUWjmt8YY9uh"},"source":["from sklearn_extra.cluster import KMedoids\n","\n","passages = [[] for _ in range(N_CLUSTERS)]\n","embeddings = [[] for _ in range(N_CLUSTERS)]\n","categories = [[] for _ in range(N_CLUSTERS)]\n","\n","for i in range(len(all_passages)):\n","    cluster_index = kmeans_pass.labels_[i]\n","    if label_to_newlabel[cluster_index] != -1:\n","        passages[label_to_newlabel[cluster_index]].append(all_passages[i])\n","        embeddings[label_to_newlabel[cluster_index]].append(embeddings_passage[i])\n","for i in range(10):\n","    print(\"########################################\")\n","    print(\"############## CLUSTER \"+str(i)+\"##############\")\n","    print(\"########################################\")\n","    #if i not in [5]:\n","    #    continue\n","    passage_sub = random.sample(list(range(len(passages[i]))),5)\n","    for j in passage_sub:\n","        print(\"%%%%%%%%%%%%%%%\")\n","        print(passages[i][j])        \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUmLnNBlg0Rl"},"source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=2, whiten=True)\n","pca.fit(embeddings_passage)\n","X_pca = pca.transform(embeddings_passage)\n","\n","\n","from matplotlib import pyplot as plt\n","plt.figure(figsize=(6, 5))\n","plt.scatter(X_pca[:, 0], X_pca[:, 1],\n","            c=kmeans_pass.labels_)\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dcvHBd00i2wX"},"source":["#pickle.dump(all_categories,open(\"categs_wiki.p\",\"wb\"))\n","all_categories = range(len(all_passages))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xi1OjoRX0LFE"},"source":["all_categories = []\n","def is_valid(title):\n","    not_allowed = [\"Wikipedia\",\"Wikidata\", \"articles\",\"Articles\",\"Year of birth\",\"Living people\"]\n","    for term in not_allowed:\n","        if term in title:\n","            return False\n","    return True\n","        \n","for i in range(0, len(all_passages)):\n","    if i % 50 == 0:\n","        print(i)\n","    #result_search = search(all_passages[i][0:100], tld='com', lang='en', num=2, start=0, stop=1, pause=1.0)\n","    #search_result = next(result_search)\n","    #if \"wiki\" not in search_result:\n","    article_passage = wikipedia.search(all_passages[i][0:30])\n","    if len(article_passage) == 0:\n","        all_categories.append([])\n","        continue\n","    else:\n","        article_passage = article_passage[0]\n","    '''\n","    else:\n","       article_passage = wikipedia.search(search_result.split('/wiki/')[1])\n","        if len(article_passage) == 0:\n","            all_categories.append([])\n","            continue\n","        else:\n","            article_passage = article_passage[0]\n","    '''\n","    try:\n","        wiki_page = wikipedia.WikipediaPage(article_passage)\n","        categs = wiki_page.categories\n","        categs = [categs[i] for i in range(len(categs)) if is_valid(categs[i])]\n","        all_categories.append(categs)\n","    except:\n","        print(\"something bad happened\")\n","        all_categories.append([])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2x44rKkMwP8W"},"source":["from sklearn_extra.cluster import KMedoids\n","\n","passages = [[] for _ in range(N_CLUSTERS)]\n","embeddings = [[] for _ in range(N_CLUSTERS)]\n","categories = [[] for _ in range(N_CLUSTERS)]\n","for i in range(len(all_categories)):\n","    cluster_index = kmeans_pass.labels_[i]\n","    passages[cluster_index].append(all_passages[i])\n","    embeddings[cluster_index].append(embeddings_passage[i])\n","    for categ in all_categories[i]:\n","        categories[cluster_index].append(categ)\n","    #hard_dataset\n","    #if (cluster_index == 0): #and (i%2 == 0):\n","        #print(\"###############\")\n","        #print(validation_dataset[i]['answer'])\n","        #print(hard_dataset[i]['passage'])\n","        #print(validation_dataset[i]['question'])\n","\n","              #      print(validation_dataset[i]['answer'])\n","from collections import Counter\n","\n","\n","for i in range(N_CLUSTERS):\n","    print(\"########################################\")\n","    print(\"############## CLUSTER \"+str(i)+\"##############\")\n","    print(\"########################################\")\n","    #if i not in [5]:\n","    #    continue\n","    \n","    c = Counter(categories[i])\n","    mc = c.most_common(30)\n","    for tt in mc:\n","        if \"CS1\" not in tt[0] and \"Pages\" not in tt[0] and \"Webarchive\" not in tt[0] :\n","            print(tt[0])\n","    #print(c.most_common(20))\n","    '''\n","    passage_sub = random.sample(list(range(len(passages[i]))),10)\n","    for j in passage_sub:\n","        print(\"%%%%%%%%%%%%%%%\")\n","        print(passages[i][j])    \n","    \n","    #'''\n","    kmedoids = KMedoids(n_clusters=7, init='k-medoids++', max_iter=10000).fit(embeddings[i])\n","    #example_indices = kmedoids.medoid_indices_\n","    passage_sub = random.sample(list(range(len(passages[i]))),6)\n","    for j in passage_sub:\n","        print(\"%%%%%%%%%%%%%%%\")\n","        print(passages[i][j])\n","    '''\n","    print(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sKYslzgrwNWP"},"source":["# The same for questions\n","'''\n","N_CLUSTERS = 20\n","kmeans_quest = KMeans(n_clusters=N_CLUSTERS,max_iter = 100000).fit(embeddings_question)\n","print(f'inertia is {kmeans_quest.inertia_:.3f}')\n","\n","plt.hist(kmeans_quest.labels_, bins=N_CLUSTERS)\n","\n","passages = [[] for _ in range(N_CLUSTERS)]\n","embeddings = [[] for _ in range(N_CLUSTERS)]\n","for i in range(len(all_passages)):\n","    cluster_index = kmeans_pass.labels_[i]\n","    passages[cluster_index].append(all_questions[i])\n","    embeddings[cluster_index].append(embeddings_question[i])\n","    #hard_dataset\n","    #if (cluster_index == 0): #and (i%2 == 0):\n","        #print(\"###############\")\n","        #print(validation_dataset[i]['answer'])\n","        #print(hard_dataset[i]['passage'])\n","        #print(validation_dataset[i]['question'])\n","\n","              #      print(validation_dataset[i]['answer'])\n","            \n","for i in range(N_CLUSTERS):\n","    print(\"########################################\")\n","    print(\"############## CLUSTER \"+str(i)+\"##############\")\n","    print(\"########################################\")\n","    kmedoids = KMedoids(n_clusters=7, init='k-medoids++', max_iter=10000).fit(embeddings[i])\n","    #passage_sub = kmedoids.medoid_indices_\n","    passage_sub = random.sample(list(range(len(passages[i]))),6)\n","    for j in passage_sub:\n","        print(\"%%%%%%%%%%%%%%%\")\n","        print(passages[i][j])\n","    print(\"\\n\")\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tCCcnqUbLBnp"},"source":["# Different clustering approach\n","https://www.kdnuggets.com/2020/11/topic-modeling-bert.html"]},{"cell_type":"code","metadata":{"id":"dSSOKAi8LEsv"},"source":["!pip install umap-learn\n","!pip install hdbscan\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"daPNsTmOLDnE"},"source":["\n","import hdbscan\n","cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n","                          metric='euclidean',                      \n","                          cluster_selection_method='eom').fit(embeddings_passage)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4VoOnRHMKKD"},"source":["import umap\n","umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings_passage)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K6DieX-zNmgn"},"source":["cluster.labels_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dk2_QTSYUWtQ"},"source":["kmeans_pass.labels_[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W7dnQcBSNTfS"},"source":["\n","import hdbscan\n","cluster = hdbscan.HDBSCAN(min_cluster_size=15,\n","                          metric='euclidean',                      \n","                          cluster_selection_method='eom').fit(umap_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dSCNSfz7NPdg"},"source":["import pandas as pd\n","result = pd.DataFrame(umap_data, columns=['x', 'y'])\n","result['labels'] = kmeans_pass.labels_\n","\n","# Visualize clusters\n","fig, ax = plt.subplots(figsize=(20, 10))\n","outliers = result.loc[result.labels == -1, :]\n","clustered = result.loc[result.labels != -1, :]\n","plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n","plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n","plt.colorbar()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xV85UN0SL9Uw"},"source":["\n","import matplotlib.pyplot as plt\n","import umap\n","# Prepare data\n","umap_data = umap.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings_passage)\n","result = pd.DataFrame(umap_data, columns=['x', 'y'])\n","result['labels'] = cluster.labels_\n","\n","# Visualize clusters\n","fig, ax = plt.subplots(figsize=(20, 10))\n","outliers = result.loc[result.labels == -1, :]\n","clustered = result.loc[result.labels != -1, :]\n","plt.scatter(outliers.x, outliers.y, color='#BDBDBD', s=0.05)\n","plt.scatter(clustered.x, clustered.y, c=clustered.labels, s=0.05, cmap='hsv_r')\n","plt.colorbar()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eNzlngbKM3IC"},"source":["umap_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VhD70Q-dwNWS"},"source":["# Classifier to predict mistakes\n","Build classifier based on concatenated embeddings of passage, question and AI prediction, predicts If EM=1 or not."]},{"cell_type":"code","metadata":{"id":"LLx5t-YWwNWT"},"source":["import numpy as np\n","from sklearn.decomposition import PCA\n","#Our sentences we like to encode\n","#embeddings = np.asarray([np.concatenate([train_passages[i],train_questions[i]]) for i in range(len(train_questions))])\n","embeddings = np.asarray([np.concatenate([train_passages[i]]) for i in range(len(train_questions))])\n","pca = PCA(n_components=200)\n","X_pca = pca.fit_transform(embeddings)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ONr6n4gZwNWT"},"source":["print(pca.explained_variance_ratio_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIEe99SHwNWT"},"source":["from matplotlib.colors import ListedColormap\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.datasets import make_moons, make_circles, make_classification\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import SVC\n","from sklearn.gaussian_process import GaussianProcessClassifier\n","from sklearn.gaussian_process.kernels import RBF\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","\n","X = embeddings\n","y = train_preds\n","#X = StandardScaler().fit_transform(X)\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=.3, random_state=42)\n","clf = MLPClassifier(hidden_layer_sizes=(512,256,128,64,32,16),alpha=0.8, max_iter=1000, early_stopping = True, verbose= True)\n","clf.fit(X_train, y_train)\n","score = clf.score(X_test, y_test)\n","print(f'score is {score:.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x2GO9SicWMs2"},"source":["clf = KNeighborsClassifier(n_neighbors=6, weights='distance')\n","clf.fit(X_train, y_train)\n","score = clf.score(X_test, y_test)\n","print(f'score is {score:.3f}')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSsgkXFANVPy"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UViR9iDtJKkX"},"source":["import numpy as np\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n","\n","\n","class TextEncoder(BaseEstimator, TransformerMixin):\n","    def fit(self, X, y=None):\n","        return self\n","    def transform(self, X):\n","        encoded = model.encode(X)\n","        return encoded\n","\n","X = train_raw_passages\n","y = train_preds\n","X_train, X_test, y_train, y_test = \\\n","    train_test_split(X, y, test_size=.3, random_state=42)\n","text_encoder = TextEncoder()\n","clf = MLPClassifier(hidden_layer_sizes=(512,256,128,64,32,16),alpha=0.5, max_iter=1000, early_stopping = True, verbose= True)\n","text_classifier = make_pipeline(text_encoder, clf)\n","text_classifier.fit(X_train, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VyMgnBSDOaQQ"},"source":["!pip install eli5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"18x_W99mNHkX"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJZLTLLVOPJO"},"source":["te = TextExplainer(random_state=42)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qTvJDUD3O555"},"source":["te.fit(test_raw_passages[12], text_classifier.predict_proba)\n","a = te.show_prediction(target_names=['false','correct'], top=6)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Br31hjwpVZGs"},"source":["b = a.data.split('<p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">')\n","c = '<p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">' +b[-1]\n","a.data = a\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XsnoORBFV7My"},"source":["te.fit(test_raw_passages[20], fakeai.predict_proba)\n","a = te.show_prediction(target_names=['false','correct'], top=8)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OkgByUsUjF8"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lvui9VGSbpec"},"source":["get_highlighted_p(fakeai,train_raw_passages[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ehxfw95AwNWS"},"source":["\n","train_preds = fakeai.predict_right_wrong(train_passages, train_questions)\n","test_preds = fakeai.predict_right_wrong(test_passages, test_questions)\n","print(\"percentage right test \" + str(np.sum(test_preds)/len(test_indices)*100))\n","print(\"percentage right train \" + str(np.sum(train_preds)/len(train_indices)*100))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DrR1GSWTwNWR"},"source":["train_passages = [embeddings_passage[i] for i in train_indices]\n","train_questions = [embeddings_question[i] for i in train_indices]\n","fakeai = FakeAI(train_passages, train_questions, 15, 15 , 7, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CKyR4HztwNWR"},"source":["# Define teaching and validation\n","all_passages = [validation_dataset[i]['passage'] for i in free_indices_val]\n","embeddings_passage = model.encode(all_passages)\n","all_questions = [validation_dataset[i]['question'] for i in free_indices_val]\n","embeddings_question = model.encode(all_questions)\n","all_answers = [validation_dataset[i]['answer'] for i in free_indices_val]\n","embeddings_ans= model.encode(all_answers)\n","\n","train_size = math.floor(0.8 * len(free_indices_val))\n","train_indices = random.sample(list(range(len(free_indices_val))), train_size)\n","dataset_indices =  set(list(range(len(all_answers))))\n","test_indices = list(dataset_indices - set(train_indices))\n","\n","train_passages = [embeddings_passage[i] for i in train_indices]\n","train_questions = [embeddings_question[i] for i in train_indices]\n","test_passages = [embeddings_passage[i] for i in test_indices]\n","test_questions = [embeddings_question[i] for i in test_indices]\n","\n","train_answers = [all_answers[i] for i in train_indices]\n","test_answers = [all_answers[i] for i in test_indices]\n","train_raw_passages = [all_passages[i] for i in train_indices]\n","test_raw_passages = [all_passages[i] for i in test_indices]\n","train_raw_questions= [all_questions[i] for i in train_indices]\n","test_raw_questions = [all_questions[i] for i in test_indices]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VYBacIDTwNWV"},"source":["# Teaching User Interface\n","\n"]},{"cell_type":"markdown","metadata":{"id":"k-L_wImhCL-o"},"source":["## Human classes and utilities"]},{"cell_type":"code","metadata":{"id":"Qk2qRGqqG4tt"},"source":["class HumanPredictor_simple:\n","    def __init__(self, accuracy, reject_percent):\n","        '''\n","        accuracy and reject_percent: floats in [0,1]\n","        '''\n","        self.accuracy = accuracy\n","        self.reject_percent = reject_percent\n","\n","    def predict(self, contexts, questions, answers):\n","        '''\n","        expects array of strings for inputs\n","        returns list of string answer\n","        '''\n","        preds = []\n","        for i in range(len(answers)):\n","            coin = random.random() # random number between [0,1]\n","            if coin <= self.accuracy:\n","                preds.append(answers[i])\n","            else:\n","                # generate random answer\n","                sents = nltk.sent_tokenize(contexts[i])\n","                rand_sent = random.randint(0,len(sents)-1)\n","                preds.append(sents[rand_sent])\n","        return preds\n","    def prior_rejector(self, contexts, questions, answers):\n","        '''\n","        this rejector is agnostic to input, random deferall 1-self.accuracy times\n","        '''\n","        preds = []\n","        for i in range(len(answers)):\n","            coin = random.random() # random number between [0,1]\n","            if coin <= self.reject_percent:\n","                preds.append(1)\n","            else:\n","                preds.append(0)\n","        return preds"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GYUTZTA3wNWW"},"source":["class HumanLearner:\n","    def __init__(self, kernel):\n","        '''\n","        kernel: function that takes two inputs and returns a similarity\n","        prior rejector: returns rejector\n","        '''\n","        self.teaching_set = []\n","        self.kernel = kernel\n","    def predict(self, xs, prior_rejector_preds, to_print = False):\n","        '''\n","        xs: expected array of inputs\n","        '''\n","        preds = []\n","        idx = 0\n","        used_posterior = 0 \n","        if to_print:\n","            print(\"-- Human making reject predictions --\")\n","            with tqdm(total=len(xs)) as pbar:\n","                for x in xs:\n","                    ball_at_x = []\n","                    similarities = rbf_kernel(x.reshape(1,-1), np.asarray([self.teaching_set[kk][0] for kk in range(len(self.teaching_set))]))[0]\n","                    for i in range(len(self.teaching_set)):\n","                        similarity = similarities[i]\n","                        if similarity >=  self.teaching_set[i][2]:\n","                            ball_at_x.append(self.teaching_set[i])\n","                    if len(ball_at_x) == 0: \n","                        # use prior rejector\n","                        preds.append(prior_rejector_preds[idx])\n","                    else:\n","                        used_posterior += 1\n","                        ball_similarities = rbf_kernel(x.reshape(1,-1), np.asarray([ball_at_x[kk][0] for kk in range(len(ball_at_x))]))[0]\n","                        normalization = np.sum([ball_similarities[i] for i in range(len(ball_at_x))])\n","                        score_one = np.sum([ball_similarities[i]*ball_at_x[i][1] for i in range(len(ball_at_x))])\n","                        pred = score_one / normalization\n","                        if pred >= 0.5:\n","                            preds.append(1)\n","                        else:\n","                            preds.append(0)\n","                    idx += 1\n","                    pbar.update(1)\n","        else:\n","            for x in xs:\n","                ball_at_x = []\n","                similarities = rbf_kernel(x.reshape(1,-1), np.asarray([self.teaching_set[kk][0] for kk in range(len(self.teaching_set))]))[0]\n","                for i in range(len(self.teaching_set)):\n","                    similarity = similarities[i]\n","                    if similarity >=  self.teaching_set[i][2]:\n","                        ball_at_x.append(self.teaching_set[i])\n","                if len(ball_at_x) == 0: \n","                    # use prior rejector\n","                    preds.append(prior_rejector_preds[idx])\n","                else:\n","                    used_posterior += 1\n","                    ball_similarities = rbf_kernel(x.reshape(1,-1), np.asarray([ball_at_x[kk][0] for kk in range(len(ball_at_x))]))[0]\n","                    normalization = np.sum([ball_similarities[i] for i in range(len(ball_at_x))])\n","                    score_one = np.sum([ball_similarities[i]*ball_at_x[i][1] for i in range(len(ball_at_x))])\n","                    pred = score_one / normalization\n","                    if pred >= 0.5:\n","                        preds.append(1)\n","                    else:\n","                        preds.append(0)\n","                idx += 1\n","        if to_print:\n","            print(f'Used posterior {used_posterior/len(xs)*100:.2f}')\n","        return preds\n","\n","    def add_to_teaching(self, teaching_example):\n","        '''\n","        teaching_example: (x, label, gamma)\n","        '''\n","        self.teaching_set.append(teaching_example)\n","\n","    def remove_last_teaching_item(self):\n","        self.teaching_set = self.teaching_set[:-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z4329rtlwNWX"},"source":["def compute_predictions_humanai(hum_preds, hum_rejector, ai_preds, data_x):\n","    '''\n","    hum_preds: array of human predictions\n","    ai_preds: array of AI predictions\n","    hum_rejector: HumanLearner\n","    data_x: array of inputs\n","\n","    Returns array of final predictions and deferalls\n","    '''\n","    predictions = []\n","    with torch.no_grad():\n","        reject_decisions = hum_rejector(data_x)\n","        for i in range(len(data_x)):\n","            if reject_decisions[i] == 1:\n","                # defer\n","                predictions.append(ai_preds[i])\n","            else:\n","                predictions.append(hum_preds[i])\n","    return predictions, reject_decisions\n","\n","def get_metrics(preds, truths):\n","    # custom for each use case\n","    return evaluate_truth_pred(truths, preds)\n","\n","def compute_metrics(human_preds, ai_preds, reject_decisions, truths, to_print = False):\n","    coverage = 1 - np.sum(reject_decisions)/len(reject_decisions)\n","    humanai_preds = []\n","    human_preds_sys = []\n","    truths_human = []\n","    ai_preds_sys = []\n","    truths_ai = []\n","    for i in range(len(reject_decisions)):\n","        if reject_decisions[i] == 1:\n","            humanai_preds.append(ai_preds[i])\n","            ai_preds_sys.append(ai_preds[i])\n","            truths_ai.append(truths[i])\n","        else:\n","            humanai_preds.append(human_preds[i])\n","            human_preds_sys.append(human_preds[i])\n","            truths_human.append(truths[i])\n","    humanai_metrics = get_metrics(humanai_preds, truths)\n","    human_metrics = get_metrics(human_preds_sys, truths_human)\n","    ai_metrics = get_metrics(ai_preds_sys, truths_ai)\n","    if to_print:\n","        print(f'Coverage is {coverage*100:.2f}')\n","        print(f' metrics of system are: {humanai_metrics}')\n","        print(f' metrics of human are: {human_metrics}')\n","        print(f' metrics of AI are: {ai_metrics}')\n","    return coverage, humanai_metrics, human_metrics, ai_metrics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rBHrWz39CjhT"},"source":["## Get predictions"]},{"cell_type":"code","metadata":{"id":"0GXsrYT4wNWY"},"source":["# Preliminary steps:\n","# Step 1.1: get predictions of human and predictions of AI on teaching set\n","# Step 1.2: get prior rejector decisions of Human on teaching set\n","# Step 2: Compute optimal deferal decision for each point in teaching set  \n","# Step 3: Compute optimal gamma for each point, can start with a constant guess\n","\n","# Algorithms:\n","# Initialize HumanLearner\n","# Baseline 1:\n","# step 1: get k-mediods points\n","# step 2: add to HumanLearner\n","# step 3: evaluate on validation set"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2dtK3TIrwNWY","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1620361612045,"user_tz":240,"elapsed":25941,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"3589d8c8-e2f4-4bac-93c2-27a5a29f3258"},"source":["# Define teaching and validation\n","random.seed(66)\n","all_passages = [validation_dataset[i]['passage'] for i in free_indices_val]\n","embeddings_passage = model.encode(all_passages)\n","embeddings_passage = [embeddings_passage[i] for i in range(len(embeddings_passage))]\n","\n","all_questions = [validation_dataset[i]['question'] for i in free_indices_val]\n","embeddings_question = model.encode(all_questions)\n","all_answers = [validation_dataset[i]['answer'] for i in free_indices_val]\n","embeddings_ans= model.encode(all_answers)\n","all_sentences =  [validation_dataset[i]['context']['sentences'] for i in free_indices_val]\n","train_size = math.floor(0.8 * len(free_indices_val))\n","train_indices = random.sample(list(range(len(free_indices_val))), train_size)\n","dataset_indices =  set(list(range(len(all_answers))))\n","test_indices = list(dataset_indices - set(train_indices))\n","\n","train_passages = [embeddings_passage[i] for i in train_indices]\n","train_questions = [embeddings_question[i] for i in train_indices]\n","test_passages = [embeddings_passage[i] for i in test_indices]\n","test_questions = [embeddings_question[i] for i in test_indices]\n","train_sentences = [all_sentences[i] for i in train_indices]\n","test_sentences = [all_sentences[i] for i in test_indices]\n","\n","train_answers = [all_answers[i] for i in train_indices]\n","test_answers = [all_answers[i] for i in test_indices]\n","train_raw_passages = [all_passages[i] for i in train_indices]\n","test_raw_passages = [all_passages[i] for i in test_indices]\n","train_raw_questions= [all_questions[i] for i in train_indices]\n","test_raw_questions = [all_questions[i] for i in test_indices]\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"5YmH1lT4PZGw"},"source":["test_answers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SlmbszFLuTW7","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1620361640380,"user_tz":240,"elapsed":1529,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"a74de1ac-08ad-4e93-e1f6-fdb23ad97828"},"source":["all_passages = [validation_dataset_hum[i]['passage'] for i in free_indices_val]\n","train_raw_passages_hum = [all_passages[i] for i in train_indices]\n","test_raw_passages_hum = [all_passages[i] for i in test_indices]\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"O0oeCwpOwhll","colab":{"base_uri":"https://localhost:8080/","height":191},"executionInfo":{"status":"ok","timestamp":1620361701887,"user_tz":240,"elapsed":674,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"bb552cfa-659f-430e-c5ad-dabe01cc366c"},"source":["# Get Human Predictions\n","HUMAN_ACCURACY = 0.8\n","REJECT_PERCENT = 0.5\n","hum_teaching_preds = []\n","hum_validation_preds = []\n","priorhum_teaching_preds = []\n","priorhum_validation_preds = []\n","\n","human_predictor = HumanPredictor_simple(HUMAN_ACCURACY, REJECT_PERCENT)\n","hum_teaching_preds = []\n","hum_validation_preds = []\n","priorhum_teaching_preds = []\n","priorhum_validation_preds = []\n","print(\"--Getting Human predictions on teach and validation sets --\\n\")\n","hum_teaching_preds = human_predictor.predict(train_raw_passages, train_answers, train_answers)\n","hum_validation_preds = human_predictor.predict(test_raw_passages, test_answers, test_answers)\n","print(\"--Getting prior rejector predictions on teach and validation sets\") \n","priorhum_teaching_preds = human_predictor.prior_rejector(train_raw_passages, train_answers, train_answers)\n","priorhum_validation_preds = human_predictor.prior_rejector(test_raw_passages, test_answers, test_answers)\n","\n","'''\n","ai_val_preds = []\n","with open('C:/Users/Hussein/Documents/Research/Human learn to defer/hotpotqa/outputs_val_1dis/predictions_ans.json', 'r') as handle:\n","    preds_val = json.load(handle)\n","    \n","for key, value in preds_val.items():\n","    ai_val_preds.append(value)\n","\n","ai_val_preds = [ai_val_preds[i] for i in free_indices_val]\n","hum_teaching_preds =  [ai_val_preds[i] for i in train_indices]\n","hum_validation_preds =  [ai_val_preds[i] for i in test_indices]\n","\n","log_odds_ai = []\n","with open('C:/Users/Hussein/Documents/Research/Human learn to defer/hotpotqa/outputs_val_1dis/null_odds_ans.json', 'r') as handle:\n","    preds_val = json.load(handle)\n","    \n","for key, value in preds_val.items():\n","    log_odds_ai.append(value)\n","log_odds_ai = [log_odds_ai[i] for i in free_indices_val]\n","log_odds_ai_train =  [log_odds_ai[i] for i in train_indices]\n","log_odds_ai_test =  [log_odds_ai[i] for i in test_indices]\n","epsilon = 0.2\n","threshold_odds = np.quantile(log_odds_ai_train, epsilon)\n","for i in range(len(log_odds_ai_train)):\n","    if log_odds_ai_train[i]>= threshold_odds:\n","        priorhum_teaching_preds.append(0)\n","    else:\n","        priorhum_teaching_preds.append(1)\n","        \n","for i in range(len(log_odds_ai_test)):\n","    if log_odds_ai_test[i]>= threshold_odds:\n","        priorhum_validation_preds.append(0)\n","    else:\n","        priorhum_validation_preds.append(1)\n","'''"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["--Getting Human predictions on teach and validation sets --\n","\n","--Getting prior rejector predictions on teach and validation sets\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nai_val_preds = []\\nwith open('C:/Users/Hussein/Documents/Research/Human learn to defer/hotpotqa/outputs_val_1dis/predictions_ans.json', 'r') as handle:\\n    preds_val = json.load(handle)\\n    \\nfor key, value in preds_val.items():\\n    ai_val_preds.append(value)\\n\\nai_val_preds = [ai_val_preds[i] for i in free_indices_val]\\nhum_teaching_preds =  [ai_val_preds[i] for i in train_indices]\\nhum_validation_preds =  [ai_val_preds[i] for i in test_indices]\\n\\nlog_odds_ai = []\\nwith open('C:/Users/Hussein/Documents/Research/Human learn to defer/hotpotqa/outputs_val_1dis/null_odds_ans.json', 'r') as handle:\\n    preds_val = json.load(handle)\\n    \\nfor key, value in preds_val.items():\\n    log_odds_ai.append(value)\\nlog_odds_ai = [log_odds_ai[i] for i in free_indices_val]\\nlog_odds_ai_train =  [log_odds_ai[i] for i in train_indices]\\nlog_odds_ai_test =  [log_odds_ai[i] for i in test_indices]\\nepsilon = 0.2\\nthreshold_odds = np.quantile(log_odds_ai_train, epsilon)\\nfor i in range(len(log_odds_ai_train)):\\n    if log_odds_ai_train[i]>= threshold_odds:\\n        priorhum_teaching_preds.append(0)\\n    else:\\n        priorhum_teaching_preds.append(1)\\n        \\nfor i in range(len(log_odds_ai_test)):\\n    if log_odds_ai_test[i]>= threshold_odds:\\n        priorhum_validation_preds.append(0)\\n    else:\\n        priorhum_validation_preds.append(1)\\n\""]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"j_bYimiLwNWZ","executionInfo":{"status":"ok","timestamp":1620361729183,"user_tz":240,"elapsed":19607,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"0adcd40e-c0d7-4b2e-e2e6-189d28f4d99f"},"source":["# Get AI predictions\n","\n","ai_teaching_preds = []\n","ai_validation_preds = []\n","fakeai = FakeAI(embeddings_passage, embeddings_passage, 15, 15, 7, 1)\n","ai_teaching_preds_b = fakeai.predict_right_wrong(train_passages, train_questions)\n","ai_validation_preds_b = fakeai.predict_right_wrong(test_passages, test_questions)\n","print(\"percentage right test \" + str(np.sum(ai_validation_preds_b)/len(test_indices)*100))\n","print(\"percentage right train \" + str(np.sum(ai_teaching_preds_b)/len(train_indices)*100))\n","\n","ai_teaching_preds = []\n","for i in range(len(ai_teaching_preds_b)):\n","    if ai_teaching_preds_b[i] == 1:\n","        ai_teaching_preds.append(train_answers[i])\n","    else:\n","        # generate random answer\n","        sents = nltk.sent_tokenize(train_raw_passages[i].replace(\"#$\",\"</br> </br>\"))\n","        rand_sent = random.randint(0,len(sents)-1)\n","        ai_teaching_preds.append(sents[rand_sent])\n","\n","ai_validation_preds = []\n","for i in range(len(ai_validation_preds_b)):\n","    if ai_validation_preds_b[i] == 1:\n","        ai_validation_preds.append(test_answers[i])\n","    else:\n","        # generate random answer\n","        sents = nltk.sent_tokenize(test_raw_passages[i].replace(\"#$\",\"</br> </br>\"))\n","        rand_sent = random.randint(0,len(sents)-1)\n","        ai_validation_preds.append(sents[rand_sent])\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["percentage right test 46.618705035971225\n","percentage right train 46.05002699298183\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gF5GamxWCtvI","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1620361776029,"user_tz":240,"elapsed":255,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"71e60268-c751-4767-bc77-f78fe46ef1e7"},"source":["# get embeddings for teaching set \n","#teaching_embeddings = np.asarray([np.concatenate([train_passages[i],train_questions[i]]) for i in range(len(train_questions))])\n","teaching_embeddings = np.asarray([np.concatenate([train_passages[i]]) for i in range(len(train_questions))])\n","\n","#validation_embeddings = np.asarray([np.concatenate([test_passages[i],test_questions[i]]) for i in range(len(test_passages))])\n","validation_embeddings = np.asarray([np.concatenate([test_passages[i]]) for i in range(len(test_passages))])\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"iN5k79djCpJC"},"source":["## Get gammas and optimal defer decisions"]},{"cell_type":"code","metadata":{"id":"EVyiRqcEwNWa","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1620361911223,"user_tz":240,"elapsed":689,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"e0ea8214-1d3c-4dbf-cbba-9a3cbb9d3c8d"},"source":["# Optimal deferall decisions\n","opt_defer_teaching = []\n","opt_defer_validation = []\n","for ex in range(len(hum_teaching_preds)):\n","    f1_hum = metric_max_over_ground_truths(f1_score, train_answers[ex], [hum_teaching_preds[ex]])\n","    f1_ai = metric_max_over_ground_truths(f1_score, train_answers[ex], [ai_teaching_preds[ex]])\n","\n","    if f1_ai >= f1_hum and f1_ai >= 0.9:\n","        opt_defer_teaching.append(1)\n","    else:\n","        opt_defer_teaching.append(0)\n","\n","for ex in range(len(hum_validation_preds)):\n","    f1_hum = metric_max_over_ground_truths(f1_score, test_answers[ex], [hum_validation_preds[ex]])\n","    f1_ai = metric_max_over_ground_truths(f1_score, test_answers[ex], [ai_validation_preds[ex]])\n","    if f1_ai >= f1_hum and f1_ai >= 0.9:\n","        opt_defer_validation.append(1)\n","    else:\n","        opt_defer_validation.append(0)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"-31PiKYkwNWa","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1620361913591,"user_tz":240,"elapsed":427,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"372bf4a3-341c-42a9-e3bf-a3337717fff7"},"source":["compute_metrics(hum_validation_preds, ai_validation_preds, priorhum_validation_preds, test_answers)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["(0.4805755395683453,\n"," {'exact_match': 63.09352517985566, 'f1': 65.21907959707562},\n"," {'exact_match': 81.43712574850177, 'f1': 82.5317489243154},\n"," {'exact_match': 46.121883656509056, 'f1': 49.201263654420735})"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"id":"odoFGiHzwNWb","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1620361915903,"user_tz":240,"elapsed":401,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"5cc76f49-0b83-4cc8-9a64-66b2b444dd7c"},"source":["compute_metrics(hum_validation_preds, ai_validation_preds, opt_defer_validation, test_answers)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["(0.5338129496402877,\n"," {'exact_match': 89.2805755395677, 'f1': 90.00458427351498},\n"," {'exact_match': 79.9191374663062, 'f1': 81.27543415119396},\n"," {'exact_match': 99.99999999999845, 'f1': 99.99999999999845})"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"r-TXbhPgahpW","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1620361922311,"user_tz":240,"elapsed":741,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"534d9af7-46e9-451e-98ed-d33cb9e26254"},"source":["compute_metrics(hum_teaching_preds, ai_teaching_preds, priorhum_teaching_preds, train_answers)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["(0.49001259672485153,\n"," {'exact_match': 62.33579269389947, 'f1': 64.6649603282008},\n"," {'exact_match': 79.94858611825164, 'f1': 81.1562826213086},\n"," {'exact_match': 45.41284403669709, 'f1': 48.81955785673564})"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"code","metadata":{"id":"z0MwZqRewNWb","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1620361924959,"user_tz":240,"elapsed":806,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"657c6660-8015-460c-d29e-7a04c77b8e06"},"source":["compute_metrics(hum_teaching_preds, ai_teaching_preds, opt_defer_teaching, train_answers)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["(0.5394997300701818,\n"," {'exact_match': 89.13082598524367, 'f1': 89.76696698303002},\n"," {'exact_match': 79.85323549032661, 'f1': 81.0323667527347},\n"," {'exact_match': 99.9999999999996, 'f1': 99.9999999999996})"]},"metadata":{"tags":[]},"execution_count":56}]},{"cell_type":"code","metadata":{"id":"ySVY8IgVwNWc","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1620364800911,"user_tz":240,"elapsed":32937,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"2492b6b2-bf43-43ee-ab8f-2d4c89441a44"},"source":["# get optimal gammas\n","optimal_gammas = []\n","with tqdm(total=len(teaching_embeddings)) as pbar:\n","    similarities_embeds_all = rbf_kernel( np.asarray(teaching_embeddings), np.asarray(teaching_embeddings))\n","    for i in range(len(teaching_embeddings)):\n","        # get all similarities\n","        similarities_embeds = similarities_embeds_all[i]\n","        opt_defer_ex = opt_defer_teaching[i]\n","        opt_gamma = 1\n","        sorted_sim = sorted([(similarities_embeds[k], opt_defer_teaching[k]) for k in range(len(teaching_embeddings))], key=lambda tup: tup[0])\n","        indicess = list(range(1, len(opt_defer_teaching)))\n","        indicess.reverse()\n","        for k in indicess:\n","            if sorted_sim[k][1] == opt_defer_ex and sorted_sim[k- 1][1] != opt_defer_ex:\n","                opt_gamma = sorted_sim[k][0]\n","                break\n","        threshold_gammas = [np.quantile(similarities_embeds, k) for k in [0.2,0.5,0.6,0.7,0.8,0.9,0.95]] + [opt_gamma]\n","        optimal_gammas.append(opt_gamma)\n","        pbar.update(1)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["100%|██████████| 5557/5557 [00:32<00:00, 169.87it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"fxdttN2lN7h4","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1620365318020,"user_tz":240,"elapsed":649,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"1f52dffb-b048-448a-aa7f-8e94bb9989d4"},"source":["sorted(optimal_gammas[:100])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["[0.9237341,\n"," 0.92524964,\n"," 0.92621064,\n"," 0.9267377,\n"," 0.9287057,\n"," 0.9288139,\n"," 0.9288515,\n"," 0.9296406,\n"," 0.92965966,\n"," 0.9299063,\n"," 0.93020254,\n"," 0.93056804,\n"," 0.9308235,\n"," 0.9311746,\n"," 0.93145657,\n"," 0.9317969,\n"," 0.9325921,\n"," 0.9326522,\n"," 0.93281245,\n"," 0.93304765,\n"," 0.9332495,\n"," 0.9337906,\n"," 0.93388903,\n"," 0.9338901,\n"," 0.9339693,\n"," 0.9340136,\n"," 0.93407184,\n"," 0.934233,\n"," 0.9347966,\n"," 0.93487656,\n"," 0.9349432,\n"," 0.935098,\n"," 0.93530154,\n"," 0.93564445,\n"," 0.9358086,\n"," 0.93593717,\n"," 0.93622595,\n"," 0.9364329,\n"," 0.9365693,\n"," 0.93687075,\n"," 0.93700355,\n"," 0.9371141,\n"," 0.93748057,\n"," 0.93795913,\n"," 0.938156,\n"," 0.9382958,\n"," 0.93837404,\n"," 0.939398,\n"," 0.9394063,\n"," 0.9395581,\n"," 0.9398171,\n"," 0.93986374,\n"," 0.940056,\n"," 0.94026506,\n"," 0.9404764,\n"," 0.9407175,\n"," 0.9409041,\n"," 0.9412024,\n"," 0.9420698,\n"," 0.9425327,\n"," 0.9425697,\n"," 0.94308907,\n"," 0.94324714,\n"," 0.94326335,\n"," 0.9435766,\n"," 0.9436128,\n"," 0.94457597,\n"," 0.9462349,\n"," 0.9464659,\n"," 0.9466125,\n"," 0.94663584,\n"," 0.9471686,\n"," 0.94817126,\n"," 0.9492122,\n"," 0.9498397,\n"," 0.9501495,\n"," 0.9508404,\n"," 0.95338386,\n"," 0.95570254,\n"," 0.9575361,\n"," 0.9597813,\n"," 0.9754372,\n"," 0.9959745,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0,\n"," 1.0]"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"code","metadata":{"id":"Czd2J20eN8VB","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1620365323131,"user_tz":240,"elapsed":261,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"457b86ba-8d47-4181-ea6d-108bae8bd3b5"},"source":["def get_improvement_defer(current_defer_preds, opt_defer_preds, gammas, xs):\n","    error_improvements = []\n","    similarities_embeds_all = rbf_kernel(np.asarray(xs), np.asarray(xs))\n","    error_at_i = 0\n","    for i in range(len(gammas)):\n","        error_at_i = 0\n","        similarities_embeds = similarities_embeds_all[i]\n","        for j in range(len(similarities_embeds)):\n","            if similarities_embeds[j] >= gammas[i]:\n","                f1_hum = metric_max_over_ground_truths(f1_score, train_answers[j], [hum_teaching_preds[j]]) # pass as param plz\n","                f1_ai = metric_max_over_ground_truths(f1_score, train_answers[j], [ai_teaching_preds[j]])# pass as param plz\n","                if current_defer_preds[j] != opt_defer_preds[j] and opt_defer_preds[j]==opt_defer_preds[i]:\n","                    #if (current_defer_preds[j] == 1 and f1_ai <=0.5) or (current_defer_preds[j] == 0 and f1_hum <=0.5):\n","                    error_at_i += 1\n","                elif current_defer_preds[j] == opt_defer_preds[j] and opt_defer_preds[j] !=opt_defer_preds[i]:\n","                    error_at_i -= 1\n","                #error_at_i += (current_defer_preds[j] != opt_defer_preds[j] and )\n","\n","        error_improvements.append(error_at_i)\n","        # get the ball for x\n","        # in this ball how many does the current defer not match the optimal\n","    return error_improvements"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"0A-BXXszQ_SM","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1620365338019,"user_tz":240,"elapsed":12880,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"b24c9529-5e71-453b-a233-4bfffa9c9c6d"},"source":["b = get_improvement_defer(priorhum_teaching_preds, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n","sorted(b, reverse = True)[:100]"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["[105,\n"," 103,\n"," 91,\n"," 90,\n"," 87,\n"," 86,\n"," 82,\n"," 81,\n"," 74,\n"," 74,\n"," 74,\n"," 73,\n"," 73,\n"," 72,\n"," 72,\n"," 71,\n"," 71,\n"," 70,\n"," 69,\n"," 69,\n"," 67,\n"," 65,\n"," 65,\n"," 64,\n"," 64,\n"," 64,\n"," 64,\n"," 63,\n"," 63,\n"," 62,\n"," 62,\n"," 61,\n"," 61,\n"," 60,\n"," 60,\n"," 60,\n"," 60,\n"," 58,\n"," 58,\n"," 57,\n"," 57,\n"," 57,\n"," 57,\n"," 56,\n"," 55,\n"," 55,\n"," 55,\n"," 55,\n"," 55,\n"," 54,\n"," 54,\n"," 54,\n"," 54,\n"," 54,\n"," 54,\n"," 53,\n"," 53,\n"," 53,\n"," 53,\n"," 52,\n"," 52,\n"," 52,\n"," 52,\n"," 52,\n"," 52,\n"," 52,\n"," 51,\n"," 51,\n"," 51,\n"," 51,\n"," 51,\n"," 51,\n"," 50,\n"," 50,\n"," 50,\n"," 50,\n"," 50,\n"," 49,\n"," 49,\n"," 49,\n"," 49,\n"," 48,\n"," 48,\n"," 48,\n"," 47,\n"," 47,\n"," 47,\n"," 46,\n"," 46,\n"," 46,\n"," 46,\n"," 46,\n"," 46,\n"," 46,\n"," 45,\n"," 45,\n"," 45,\n"," 45,\n"," 45,\n"," 45]"]},"metadata":{"tags":[]},"execution_count":86}]},{"cell_type":"markdown","metadata":{"id":"VMCUulSHCy78"},"source":["## Selection Algorithm"]},{"cell_type":"code","metadata":{"id":"y4llaAF5NsL2","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1620364746641,"user_tz":240,"elapsed":762171,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"3709bf98-5c4f-4856-f6e8-ea05afaf7f10"},"source":["MAX_SIZE = 20\n","human_learner = HumanLearner(None)\n","errors = []\n","data_sizes  = []\n","indices_used = []\n","points_chosen = []\n","for itt in range(MAX_SIZE):\n","    print(f'New size {itt}')\n","    best_index = -1\n","    # predict with current human learner\n","    if itt == 0:\n","        preds_teach = priorhum_teaching_preds\n","    else:\n","        preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n","    error_improvements = get_improvement_defer(preds_teach, opt_defer_teaching, optimal_gammas, teaching_embeddings)\n","    print(f'got improvements with max {max(error_improvements)}')\n","    best_index = np.argmax(error_improvements)\n","    indices_used.append(best_index) # add found element to set used\n","    ex_embed = teaching_embeddings[best_index]\n","    ex_label = opt_defer_teaching[best_index]\n","    gamma = optimal_gammas[best_index] # random choice\n","    human_learner.add_to_teaching([ex_embed, ex_label, gamma])\n","\n","    if itt % 3 == 0:\n","        print(\"####### train eval \" +str(itt)+ \" ###########\")\n","        preds_teach = human_learner.predict(teaching_embeddings, priorhum_teaching_preds)\n","        _, metrics, __, ___ = compute_metrics(hum_teaching_preds, ai_teaching_preds, preds_teach, train_answers, True)\n","        errors.append(metrics)   \n","        print(\"##############################\")\n","\n","    if itt % 5 == 0:\n","        print(\"####### val eval \" +str(itt)+ \" ###########\")\n","        preds_teach = human_learner.predict(validation_embeddings, priorhum_validation_preds)\n","        _, metrics, __, ___ = compute_metrics(hum_validation_preds, ai_validation_preds, preds_teach, test_answers, True)\n","        errors.append(metrics)   \n","        print(\"##############################\")"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["New size 0\n","got improvements with max 31\n","####### train eval 0 ###########\n","Coverage is 48.44\n"," metrics of system are: {'exact_match': 62.443764621198376, 'f1': 64.77293225549971}\n"," metrics of human are: {'exact_match': 79.9405646359581, 'f1': 81.16216849101906}\n"," metrics of AI are: {'exact_match': 46.003490401396, 'f1': 49.373342745545834}\n","##############################\n","####### val eval 0 ###########\n","Coverage is 47.55\n"," metrics of system are: {'exact_match': 63.16546762589883, 'f1': 65.28679013452803}\n"," metrics of human are: {'exact_match': 81.39183055975671, 'f1': 82.48914663918535}\n"," metrics of AI are: {'exact_match': 46.63923182441637, 'f1': 49.689043015763744}\n","##############################\n","New size 1\n","got improvements with max 28\n","New size 2\n","got improvements with max 27\n","New size 3\n","got improvements with max 20\n","####### train eval 3 ###########\n","Coverage is 49.79\n"," metrics of system are: {'exact_match': 63.45150260932146, 'f1': 65.73712139769522}\n"," metrics of human are: {'exact_match': 79.79761474521113, 'f1': 81.03843757142123}\n"," metrics of AI are: {'exact_match': 47.240143369175456, 'f1': 50.56194510640508}\n","##############################\n","New size 4\n","got improvements with max 19\n","New size 5\n","got improvements with max 17\n","####### val eval 5 ###########\n","Coverage is 50.22\n"," metrics of system are: {'exact_match': 65.32374100719377, 'f1': 67.21490794409644}\n"," metrics of human are: {'exact_match': 81.37535816618795, 'f1': 82.47120907394931}\n"," metrics of AI are: {'exact_match': 49.1329479768779, 'f1': 51.826326746642756}\n","##############################\n","New size 6\n","got improvements with max 14\n","####### train eval 6 ###########\n","Coverage is 50.69\n"," metrics of system are: {'exact_match': 64.22530142163026, 'f1': 66.42374865704373}\n"," metrics of human are: {'exact_match': 79.90770323038666, 'f1': 81.12650222226571}\n"," metrics of AI are: {'exact_match': 48.10218978102172, 'f1': 51.30781552082844}\n","##############################\n","New size 7\n","got improvements with max 13\n","New size 8\n","got improvements with max 13\n","New size 9\n","got improvements with max 13\n","####### train eval 9 ###########\n","Coverage is 51.39\n"," metrics of system are: {'exact_match': 64.74716573690829, 'f1': 66.896624051207}\n"," metrics of human are: {'exact_match': 79.83193277310896, 'f1': 81.05485969236751}\n"," metrics of AI are: {'exact_match': 48.796741947426696, 'f1': 51.92590172941734}\n","##############################\n","New size 10\n","got improvements with max 11\n","####### val eval 10 ###########\n","Coverage is 50.94\n"," metrics of system are: {'exact_match': 65.75539568345276, 'f1': 67.61073911293678}\n"," metrics of human are: {'exact_match': 81.07344632768248, 'f1': 82.20658228171708}\n"," metrics of AI are: {'exact_match': 49.85337243401686, 'f1': 52.45845617525775}\n","##############################\n","New size 11\n","got improvements with max 10\n","New size 12\n","got improvements with max 10\n","####### train eval 12 ###########\n","Coverage is 51.59\n"," metrics of system are: {'exact_match': 65.03509087637202, 'f1': 67.16828113778003}\n"," metrics of human are: {'exact_match': 79.73491454482009, 'f1': 80.96040087275605}\n"," metrics of AI are: {'exact_match': 49.368029739776766, 'f1': 52.46865017860685}\n","##############################\n","New size 13\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-81-d254674b4b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpreds_teach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpriorhum_teaching_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mpreds_teach\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhuman_learner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mteaching_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriorhum_teaching_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0merror_improvements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_improvement_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_teach\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_defer_teaching\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_gammas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteaching_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'got improvements with max {max(error_improvements)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-70b295f1db18>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, xs, prior_rejector_preds, to_print)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mball_at_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrbf_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteaching_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteaching_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteaching_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0msimilarity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mrbf_kernel\u001b[0;34m(X, Y, gamma)\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m     \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meuclidean_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquared\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1106\u001b[0m     \u001b[0mK\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# exponentiate K in-place\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;31m# To minimize precision issues with float32, we compute the distance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;31m# matrix on chunks of X and Y upcast to float64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mdistances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_euclidean_distances_upcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;31m# if dtype is already float64, no need to chunk and upcast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36m_euclidean_distances_upcast\u001b[0;34m(X, XX, Y, YY, batch_size)\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0my_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_slice\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m                 \u001b[0;31m# when X is Y the distance matrix is symmetric so we only need\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36mgen_batches\u001b[0;34m(n, batch_size, min_batch_size)\u001b[0m\n\u001b[1;32m    710\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \"\"\"\n\u001b[0;32m--> 712\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m         raise TypeError(\"gen_batches got batch_size=%s, must be an\"\n\u001b[1;32m    714\u001b[0m                         \" integer\" % batch_size)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"ClQLSVIyTyk2","colab":{"base_uri":"https://localhost:8080/","height":243},"executionInfo":{"status":"ok","timestamp":1620364750296,"user_tz":240,"elapsed":264,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"756db21e-cf79-47ca-8b56-26e4cee4ee14"},"source":["b = fakeai.kmeans_pass.predict(train_passages)\n","for i in indices_used:\n","    print(b[i])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["6\n","5\n","14\n","14\n","14\n","5\n","14\n","5\n","0\n","14\n","4\n","6\n","7\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"DvYo68aPMNtE","executionInfo":{"status":"ok","timestamp":1620363528026,"user_tz":240,"elapsed":264,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"4c375c58-060f-47e8-b658-e9087c9117f9"},"source":[""],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"BGGrkU84MUuk","executionInfo":{"status":"ok","timestamp":1620363532017,"user_tz":240,"elapsed":261,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"891fe363-27b7-4405-ab94-a4e5551ead9e"},"source":[""],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["array([ 3,  8,  8, ..., 14,  7,  6], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":364},"id":"hjIvvZEKMCag","executionInfo":{"status":"ok","timestamp":1620363458649,"user_tz":240,"elapsed":275,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"a53ca9be-2f04-4cba-a32d-91bd7f0980a8"},"source":["indices_used"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"text/html":["\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["[2737,\n"," 4934,\n"," 2690,\n"," 3554,\n"," 3572,\n"," 1368,\n"," 218,\n"," 1159,\n"," 946,\n"," 390,\n"," 835,\n"," 1514,\n"," 4683,\n"," 4887,\n"," 3320,\n"," 4192,\n"," 3791,\n"," 5480,\n"," 5057,\n"," 65]"]},"metadata":{"tags":[]},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"znDSyPuFwNWd"},"source":["''''\n","MAX_SIZE = 2000\n","human_learner = HumanLearner(None)\n","errors = []\n","for itt in range(MAX_SIZE):\n","    print(f'New size {itt}')\n","\n","    valid_indices = list(set(list(range(len(train_answers)))) - set(indices_used))\n","    subset_size = min(100, len(valid_indices))\n","    random_teach_subset = random.sample(valid_indices, 200) # used to take gradient steps\n","    random_validation_subset = random.sample(valid_indices, subset_size)\n","    print(\"\")\n","    # for each point, add and see effect then remove\n","    counter_ = 0\n","    best_index = -1\n","    best_value = 0\n","    best_gamma = 1\n","    for j in random_teach_subset:\n","        for gamma_try in optimal_gammas[j]:\n","            counter_ += 1\n","            ex_embed = teaching_embeddings[j]\n","            ex_label = opt_defer_teaching[j]\n","            gamma = gamma_try # random choice\n","            human_learner.add_to_teaching([ex_embed, ex_label, gamma])\n","            preds_teach = human_learner.predict([teaching_embeddings[i] for i in random_validation_subset], [priorhum_teaching_preds[i] for i in random_validation_subset])\n","            _, metrics, __, ___ = compute_metrics([hum_teaching_preds[kk] for kk in random_validation_subset], [ai_teaching_preds[kk] for kk in random_validation_subset], preds_teach, [train_answers[kk] for kk in random_validation_subset])\n","            if counter_ % 300 == 0:\n","                print(metrics)\n","            acc = metrics[\"f1\"]\n","            if acc >= best_value:\n","                best_value = acc\n","                best_index = j\n","                best_gamma = gamma\n","            human_learner.remove_last_teaching_item()\n","\n","    indices_used.append(best_index) # add found element to set used\n","    ex_embed = teaching_embeddings[best_index]\n","    ex_label = opt_defer_teaching[best_index]\n","    gamma = optimal_gammas[best_index] # random choice\n","    human_learner.add_to_teaching([ex_embed, ex_label, best_gamma])\n","    if itt % 1 == 0:\n","        print(\"####### Actual eval \" +str(itt)+ \" ###########\")\n","        preds_teach = human_learner.predict(validation_embeddings, priorhum_validation_preds)\n","        _, metrics, __, ___ = compute_metrics(hum_validation_preds, ai_validation_preds, preds_teach, test_answers, True)\n","        errors.append(metrics)   \n","        print(\"##############################\")\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BkRZHAIRC5fw"},"source":["## Task creation and upload"]},{"cell_type":"code","metadata":{"id":"IWoSRNGM-JYi"},"source":["MAX_TEST_SIZE = 10\n","MAX_TEACH_SIZE = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0jrYuOZsntZa"},"source":["# get help indices\n","help_indices = []\n","for i in indices_used[:MAX_TEACH_SIZE]:\n","    # get all similarities\n","    similarities_embeds = rbf_kernel(teaching_embeddings[i].reshape(1,-1), np.asarray(teaching_embeddings))[0]\n","    sorted_sim = sorted([(similarities_embeds[k], opt_defer_teaching[k], k) for k in range(len(teaching_embeddings))], key=lambda tup: tup[0])\n","    indicess = list(range(1, len(opt_defer_teaching)))\n","    indicess.reverse()\n","    opt_defer_ex = opt_defer_teaching[i]\n","    for k in indicess:\n","        if sorted_sim[k][1] == opt_defer_ex and sorted_sim[k- 1][1] != opt_defer_ex:\n","            help_indices.append(sorted_sim[k][2])\n","            help_indices.append(sorted_sim[k-1][2])\n","            break\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"loIOr6513Qcr"},"source":["def print_qa(p,q,a,ai_a,i):\n","    print(\"#######################\")\n","    print(f'######## Example {i} ###########')\n","    print(\"Passage\")\n","    print(p)\n","    print(\"Question\")\n","    print(q)\n","    print(\"Answer:\")\n","    print(a)\n","    print(\"AI answer:\")\n","    print(ai_a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zqblmRG8wNWe"},"source":["# Baseline get medoids\n","from sklearn_extra.cluster import KMedoids\n","kmedoids = KMedoids(n_clusters=204, init='k-medoids++', max_iter=10000, random_state=41).fit(validation_embeddings)\n","print(kmedoids.inertia_)\n","indices_medoids = kmedoids.medoid_indices_\n","# filter small paragraphs\n","indices_test = []\n","for i in indices_medoids:\n","    if len(test_raw_passages[i]) >= 500:\n","        indices_test.append(i)\n","print(len(indices_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_XsaWWSx4cv"},"source":["7305.4697"]},{"cell_type":"code","metadata":{"id":"ZCWMOhzm3Rds"},"source":["j = 0\n","quest_impossible = []\n","fixed_ai_answers = []\n","for i in indices_test:\n","    ai_ans = ai_validation_preds[i]\n","    a = test_answers[i]\n","    p = test_raw_passages[i]\n","    q = test_raw_questions[i]\n","    f1_ai = metric_max_over_ground_truths(f1_score, ai_ans, [a])\n","    print_qa(p.replace(\"#$\",\"\\n \\n\"), q, a, ai_ans,j)\n","    is_imposs = input(\"Is the question impossible (-1 no answer, 0 no, 1 sorta, 2 yes) \")\n","    quest_impossible.append(is_imposs)\n","    j += 1\n","    fake_ans = \"\"\n","    fake_ans = input(\"Give a plausible wrong answer:\")\n","    fixed_ai_answers.append(fake_ans)  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DkSZ3YgsIulN"},"source":["j = 0\n","#sents_to_remove = []\n","for i in indices_test:\n","    a = test_answers[i]\n","    p = test_raw_passages[i]\n","    q = test_raw_questions[i]\n","    s = test_sentences[i]\n","    print(\"#######################\")\n","    print(f'######## Example {j} ###########')\n","    print(\"#########Paragraph 0 ########\")\n","    for k in range(len(s[0])):\n","        print(f'##sentence {k} ##:')\n","        print(s[0][k])\n","    print(\"#########Paragraph 1 #########\")\n","    for k in range(len(s[1])):\n","        print(f'##sentence {k} ##:')\n","        print(s[1][k])   \n","    print(\"###################################33\")\n","    print(\"Question\")\n","    print(q)\n","    print(\"Answer:\")\n","    print(a)\n","    par_to_remove = input(f'Par to remove (0 or 1)')\n","    par_to_remove = int(par_to_remove)\n","    sent_to_remove = input(f'sent to remove in range {list(range(len(s[par_to_remove])))}')\n","    sent_to_remove = int(sent_to_remove)\n","    sents_to_remove.append([par_to_remove,sent_to_remove])\n","    print(a)\n","    j += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHmGJZlIRInt"},"source":["            sentencess[par_to_remove][sent_to_remove] = \" [...] . \"\n","            able_to_remove = True\n","        except:\n","            times_tried += 1\n","            print(\"failed trying again\")\n","            rand_sup_idx =random.sample(list(range(len(example['supporting_facts']['sent_id']))), 1)[0] \n","            title_to_index = {}\n","            for i in range(len(example['context']['title'])):\n","                title_to_index[example['context']['title'][i]] = i\n","            par_to_remove = title_to_index[example['supporting_facts']['title'][rand_sup_idx]]\n","            sent_to_remove = example['supporting_facts']['sent_id'][rand_sup_idx]\n","            sentencess = copy.deepcopy(example['context']['sentences'])\n","\n","    example['intros'] = \" \".join([sentencess[idx][0] for idx in range(len(sentencess)) if len(sentencess[idx])>0])\n","\n","    sentences_par = [\"\".join(sentencess[idx]) for idx in range(len(sentencess))]\n","\n","    example['passage'] = \" #$ \".join(sentences_par)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i5mmFZGORJhL"},"source":["j = 0\n","#fixed_ai_answers = []\n","for i in indices_test:\n","    if j >= len(sents_to_remove):\n","        break\n","    a = test_answers[i]\n","    p = test_raw_passages[i]\n","    q = test_raw_questions[i]\n","    s = test_sentences[i]\n","    s[sents_to_remove[j][0]][sents_to_remove[j][1]] = \" [...] . \"\n","    p2 = [\"\".join(s[idx]) for idx in range(len(s))]\n","    p2 = \" #$ \".join(p2)\n","    p2 = p2.replace(\"#$\",\"\\n \\n\")\n","    print(\"#######################\")\n","    print(f'######## Example {j} ###########')\n","    print(p2)\n","    print(\"###################################33\")\n","    print(\"Question\")\n","    print(q)\n","    print(\"Answer:\")\n","    print(a)\n","    fake_ans = \"\"\n","    fake_ans = input(\"Give a plausible wrong answer:\")\n","    fixed_ai_answers.append(fake_ans)  \n","    j += 1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4QlFS4wbVABv"},"source":["j = 0\n","#fixed_ai_answers = []\n","saved_test_set = []\n","for i in indices_test:\n","    if j >= len(sents_to_remove):\n","        break\n","    a = test_answers[i]\n","    p = test_raw_passages[i]\n","    q = test_raw_questions[i]\n","    s = test_sentences[i]\n","    s[sents_to_remove[j][0]][sents_to_remove[j][1]] = \" [...] . \"\n","    p2 = [\"\".join(s[idx]) for idx in range(len(s))]\n","    p2 = \" #$ \".join(p2)\n","    p2 = p2.replace(\"#$\",\"\\n \\n\")\n","\n","    fake_ans = fixed_ai_answers[j]\n","    saved_test_set.append([p2,q,a,fake_ans,i])\n","    j += 1\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JNwLl-UVcPK"},"source":["pickle.dump(saved_test_set,open(\"saved_test_set51.p\",\"wb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":183},"id":"KFVOrU3-SDfd","executionInfo":{"status":"error","timestamp":1620361240138,"user_tz":240,"elapsed":835,"user":{"displayName":"Hussein Mozannar","photoUrl":"","userId":"03221912190329599144"}},"outputId":"c5df036b-6687-46f3-b4e5-6b24d1427ddd"},"source":["while True:\n","    a= 1"],"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-1ca9f9cef9ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"PxIkFotgP_b2"},"source":["sents_to_remove"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EK2pEGQyIlb4"},"source":["entries_to_save = []\n","j = 0\n","for i in indices_test:\n","    if j >= len(sents_to_remove):\n","        break\n","    a = test_answers[i]\n","    p = test_raw_passages[i]\n","    q = test_raw_questions[i]\n","    entries_to_save.append([p,q,a,quest_impossible[j],fixed_ai_answers[j]])\n","    j += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ntdS6KPLIDw"},"source":["pickle.dump(entries_to_save,open(\"test_prep.p\",\"wb\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jwaVwBu6fnBO"},"source":["wrong_ai_indices = []\n","wrong_ai_j_indices = []\n","right_ai_indices = []\n","right_ai_j_indices = []\n","j = 0\n","for i in indices_test:\n","    if j >= len(quest_impossible):\n","        break\n","    a = test_answers[i]\n","    ai_ans = ai_validation_preds[i]\n","    f1_ai = metric_max_over_ground_truths(f1_score, ai_ans, [a])\n","\n","    if f1_ai <= 0.9:\n","        wrong_ai_indices.append(i)\n","        wrong_ai_j_indices.append(j)\n","    else:\n","        right_ai_indices.append(i)\n","        right_ai_j_indices.append(j)\n","    j += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1TrHhzBsiKCK"},"source":["right_ai_indices"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cu9E-Rv3kj-S"},"source":["testing_tasks_ps = [[] for _ in range(10)]\n","testing_tasks_qs = [[] for _ in range(10)]\n","testing_tasks_as = [[] for _ in range(10)]\n","testing_tasks_ai_as = [[] for _ in range(10)]\n","for i in range(4):\n","    rand_order = [0,1,1,0,1,0,0,0,1,1]\n","    for j in range(10):\n","        if rand_order[j] == 0:\n","            testing_tasks_ps[i].append(test_raw_passages[wrong_ai_indices[j]].replace(\"#$\",\"</br> </br>\"))\n","            testing_tasks_qs[i].append(test_raw_questions[wrong_ai_indices[j]])\n","            testing_tasks_as[i].append(test_answers[wrong_ai_indices[j]])\n","            testing_tasks_ai_as[i].append(fixed_ai_answers[wrong_ai_j_indices[j]])\n","        else:\n","            testing_tasks_ps[i].append(test_raw_passages[right_ai_indices[j]].replace(\"#$\",\"</br> </br>\"))\n","            testing_tasks_qs[i].append(test_raw_questions[right_ai_indices[j]])\n","            testing_tasks_as[i].append(test_answers[right_ai_indices[j]])\n","            testing_tasks_ai_as[i].append(fixed_ai_answers[right_ai_j_indices[j]])\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jW4B_Z2g7lq"},"source":["kk = 9\n","print_qa(testing_tasks_ps[0][kk], testing_tasks_qs[0][kk], testing_tasks_as[0][kk], testing_tasks_ai_as[0][kk], 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"boRvQQO0i7Yb"},"source":["''''\n","from sklearn_extra.cluster import KMedoids\n","kmedoids = KMedoids(n_clusters=5, init='k-medoids++', max_iter=10000).fit(teaching_embeddings)\n","print(kmedoids.inertia_)\n","indices_medoids_teach = kmedoids.medoid_indices_\n","\n","teaching_tasks_ps = []\n","teaching_tasks_qs = []\n","teaching_ai_as = []\n","teaching_as = []\n","for i in range(5):\n","    teaching_tasks_ps.append(train_raw_passages[indices_medoids_teach[i]].replace(\"#$\",\"</br> </br>\"))\n","    teaching_tasks_qs.append(train_raw_questions[indices_medoids_teach[i]])\n","    teaching_ai_as.append(ai_teaching_preds[indices_medoids_teach[i]])\n","    teaching_as.append(train_answers[indices_medoids_teach[i]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"81Relm1ql8CZ"},"source":["''''\n","fixed_teaching_ai_answers = []\n","j = 0\n","for i in indices_used:\n","    ai_ans = ai_teaching_preds[i]\n","    a = train_answers[i]\n","    p = train_raw_passages[i]\n","    q = train_raw_questions[i]\n","    f1_ai = metric_max_over_ground_truths(f1_score, ai_ans, [a])\n","    print_qa(p.replace(\"#$\",\"\\n \\n\"), q, a, ai_ans,j)\n","    j += 1\n","    fake_ans = \"\"\n","    if f1_ai <= 0.9:\n","        print(f1_ai)\n","        fake_ans = input(\"Give a plausible wrong answer:\")\n","    else:\n","        fake_ans = a\n","    fixed_teaching_ai_answers.append(fake_ans)\n","'''' "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F2yBAWRKoGVs"},"source":["fixed_teaching_ai_answers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5wPMJTmFp6L4"},"source":["teaching_tasks_ps = []\n","teaching_tasks_qs = []\n","teaching_ai_as = []\n","teaching_as = []\n","teaching_help_ps = []\n","teaching_help_qs = []\n","teaching_help_as = []\n","for i in range(MAX_TEACH_SIZE):\n","    teaching_tasks_ps.append(train_raw_passages[indices_used[i]].replace(\"#$\",\"</br> </br>\"))\n","    teaching_tasks_qs.append(train_raw_questions[indices_used[i]])\n","    teaching_ai_as.append(fixed_teaching_ai_answers[i])\n","    teaching_as.append(train_answers[indices_used[i]])\n","    teaching_help_ps.append(get_highlighted_p(fakeai,train_raw_passages[indices_used[i]].replace(\"#$\",\" \")))\n","    teaching_help_qs.append(train_raw_questions[indices_used[i]])\n","    if ai_teaching_preds[indices_used[i]] == train_answers[indices_used[i]]:\n","        teaching_help_as.append(\"<span class='highlight_green'> AI is correct </span>\" )\n","    else:\n","        teaching_help_as.append(\"<span class='highlight_red'> AI is incorrect </span>\" )\n","\n","    for j in range(2):\n","        teaching_help_ps.append(get_highlighted_p(fakeai, train_raw_passages[help_indices[i*2+j]].replace(\"#$\",\"  \")))\n","        teaching_help_qs.append(train_raw_questions[help_indices[i*2+j]])\n","        if ai_teaching_preds[help_indices[i*2+j]] == train_answers[help_indices[i*2+j]]:\n","            teaching_help_as.append(\"<span class='highlight_green'> AI is correct </span>\" )\n","        else:\n","            teaching_help_as.append(\"<span class='highlight_red'> AI is incorrect </span>\" )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Ns2hrMs0oFL"},"source":["kk = 1\n","print_qa(teaching_tasks_ps[kk], teaching_tasks_qs[kk], teaching_as[kk], teaching_ai_as[kk], kk)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3B75LjuHofC"},"source":["\n","teaching_help_as = []\n","for i in range(MAX_TEACH_SIZE):\n","    if ai_teaching_preds[indices_used[i]] == train_answers[indices_used[i]]:\n","        teaching_help_as.append(\" AI is correct </span>\" )\n","    else:\n","        teaching_help_as.append(\"<span class='highlight_red'> AI is incorrect </span>\" )\n","\n","    for j in range(2):\n","\n","        if ai_teaching_preds[help_indices[i*2+j]] == train_answers[help_indices[i*2+j]]:\n","            teaching_help_as.append(\"<span class='highlight_green'> AI is correct </span>\" )\n","        else:\n","            teaching_help_as.append(\"<span class='highlight_red'> AI is incorrect </span>\" )\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZToC0aF1luLa"},"source":["welcome_q = train_dataset[62012]['question']\n","welcome_p = train_dataset[62012]['passage'].replace(\"#$\",\"</br> </br>\")\n","welcome_a = train_dataset[62012]['answer']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M1NEIjEhmYj6"},"source":["import firebase_admin\n","from firebase_admin import credentials\n","from firebase_admin import firestore\n","from random import randint\n","\n","import csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qhrDYk4FlzN0"},"source":["\n","\n","def add_data_firestore():\n","    cred = credentials.Certificate(\"/content/teaching-to-defer-firebase-adminsdk-ffb3y-118d103364.json\")\n","    firebase_admin.initialize_app(cred)\n","    db = firestore.client()\n","    for tasks_indx in range(4):\n","\n","        task_id = \"task\" + str(tasks_indx + 1)\n","        doc_ref = db.collection(u'tasks').document(task_id)\n","        if tasks_indx<=1:\n","            treatment = 0\n","        else:\n","            treatment = 1\n","\n","        doc_ref.set({\n","            \"test_ps\": testing_tasks_ps[tasks_indx],\n","            \"test_qs\": testing_tasks_qs[tasks_indx],\n","            \"test_as\": testing_tasks_as[tasks_indx],\n","            \"test_ai_as\": testing_tasks_ai_as[tasks_indx],\n","            \"treatment\": treatment,\n","            \"welcome_a\": welcome_a,\n","            \"welcome_p\": welcome_p,\n","            \"welcome_q\": welcome_q,\n","            \"teach_ai_as\": teaching_ai_as,\n","            \"teach_as\": teaching_as,\n","            \"teach_ps\": teaching_tasks_ps,\n","            \"teach_qs\": teaching_tasks_qs,\n","            \"teaching_help_ps\": teaching_help_ps,\n","            \"teach_help_qs\": teaching_help_qs,\n","            \"teach_help_as\": teaching_help_as\n","        })\n","\n","\n","add_data_firestore()\n","# update_database()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QcSnZm1Wns_y"},"source":["\"da\".replace(\"a\",\"b\")"],"execution_count":null,"outputs":[]}]}