{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"guassian_teaching.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UP1CACu3irWL"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"JRNFoaJXvO4x"},"source":["import math\n","import torch\n","import numpy as np\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import argparse\n","import os\n","import random\n","import shutil\n","import time\n","from sklearn.metrics.pairwise import rbf_kernel\n","import torch.nn.parallel\n","import torch.backends.cudnn as cudnn\n","import torch.optim\n","import torch.utils.data\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.autograd import Variable\n","from PIL import Image\n","import torch.utils.data as data\n","from torchvision.datasets.utils import download_url, check_integrity\n","import sys\n","from sklearn.decomposition import PCA\n","from matplotlib import pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","import pickle\n","from sklearn.gaussian_process.kernels import RBF\n","import torch\n","from scipy.stats import multivariate_normal\n","import  scipy.stats as st\n","from matplotlib import cm\n","import torch.optim as optim\n","from __future__ import print_function\n","from tqdm import tqdm\n","from sklearn.metrics.pairwise import rbf_kernel\n","from scipy.special import expit\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hzkPMsn5jTsj"},"source":["# Training and Algorithm"]},{"cell_type":"markdown","metadata":{"id":"ysptfmjNGab-"},"source":["Data generating code"]},{"cell_type":"code","metadata":{"id":"C3swu_JWGZHy"},"source":["\n","def sample(mu, var, nb_samples=500):\n","    \"\"\"\n","    :param mu: torch.Tensor (features)\n","    :param var: torch.Tensor (features) (note: zero covariance)\n","    :return: torch.Tensor (nb_samples, features)\n","    \"\"\"\n","    out = []\n","    for i in range(nb_samples):\n","        out += [\n","            torch.normal(mu, var.sqrt())\n","        ]\n","    return torch.stack(out, dim=0)\n","\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1GYixcHgGb6q"},"source":["Linear classifier definition and training"]},{"cell_type":"code","metadata":{"id":"84cwNiGcjVun"},"source":["\n","class Linear_net_sig(nn.Module):\n","    def __init__(self, input_dim):\n","        super(Linear_net_sig, self).__init__()\n","        # an affine operation: y = Wx + b\n","        self.fc1 = nn.Linear(input_dim, 1)\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.sigmoid(x)\n","        return x\n","\n","def run_classifier_sig(net, data_x, data_y, n_epochs = 10000):\n","    '''\n","    training code using GD\n","    '''\n","    BCE = torch.nn.BCELoss(size_average=True)\n","    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0)\n","    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 1000*10000)\n","    for epoch in range(n_epochs):  # loop over the dataset multiple times\n","\n","        running_loss = 0.0\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs = data_x\n","        labels = data_y\n","        order = np.array(range(len(data_x)))\n","        np.random.shuffle(order)\n","        # in-place changing of values\n","        inputs[np.array(range(len(data_x)))] = inputs[order]\n","        labels[np.array(range(len(data_x)))] = labels[order]\n","        # zero the parameter gradients\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)[:,0]\n","\n","        loss = BCE(outputs, labels*1.0) \n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        #scheduler.step()\n","        running_loss += loss.item()\n","        if epoch % 10000 == 0:\n","            print(\"loss \" + str(loss.item()))\n","\n","    #print('Finished Training')\n","\n","\n","def test_classifier_sig(net, data_x, data_y):\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        inputs =  data_x\n","        labels = data_y\n","        outputs = net(inputs)\n","        predicted = torch.round(outputs.data)\n","        total = labels.size(0)\n","        for i in range(total):\n","            correct += predicted[i].item() == labels[i].item()\n","        #correct = (predicted == labels).sum()\n","    print('Accuracy of the network on the 10000 test images: %d %%' % (\n","        100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mM2ezbR4Gnsk"},"source":["Simplified human learner model"]},{"cell_type":"code","metadata":{"id":"qylH5y9r_z4W"},"source":["class HumanLearner:\n","    def __init__(self):\n","        self.teaching_set = []\n","        self.kernel_raw = RBF()\n","\n","    def kernel(self, x,y):\n","        kernel_computation = self.kernel_raw(x.reshape(1,-1),y.reshape(1,-1))[0][0]\n","        return kernel_computation\n","\n","    def predict(self, xs):\n","        '''\n","        x: expected array of inputs\n","        '''\n","        preds = []\n","        for x in xs:\n","            ball_at_x = []\n","            if len(self.teaching_set) == 0:\n","                preds.append(-1)\n","                continue\n","            similarities = rbf_kernel(x.reshape(1,-1), np.asarray([self.teaching_set[kk][0] for kk in range(len(self.teaching_set))]))[0]\n","            for i in range(len(self.teaching_set)):\n","\n","                similarity = similarities[i]\n","                if similarity >= self.teaching_set[i][2]:\n","                    ball_at_x.append(self.teaching_set[i])\n","            if len(ball_at_x) == 0:\n","                preds.append(-1)\n","                continue\n","            ball_similarities = rbf_kernel(x.reshape(1,-1), np.asarray([ball_at_x[kk][0] for kk in range(len(ball_at_x))]))[0]\n","            normalization = np.sum([ball_similarities[i] for i in range(len(ball_at_x))])\n","            score_one = np.sum([ball_similarities[i]*ball_at_x[i][1] for i in range(len(ball_at_x))])\n","            pred = score_one / normalization\n","            if pred >= 0.5:\n","                preds.append(1)\n","            else:\n","                preds.append(0)\n","        return preds\n","    def predict_prior(self, xs, prior_rejector_preds, to_print = False):\n","        preds = []\n","        j = 0\n","        for x in xs:\n","            ball_at_x = []\n","            if len(self.teaching_set) == 0:\n","                preds.append(prior_rejector_preds[j])\n","                j += 1\n","                continue\n","            similarities = rbf_kernel(x.reshape(1,-1), np.asarray([self.teaching_set[kk][0] for kk in range(len(self.teaching_set))]))[0]\n","            for i in range(len(self.teaching_set)):\n","\n","                similarity = similarities[i]\n","                if similarity >= self.teaching_set[i][2]:\n","                    ball_at_x.append(self.teaching_set[i])\n","            if len(ball_at_x) == 0:\n","                preds.append(prior_rejector_preds[j])\n","                j += 1\n","                continue\n","            j += 1\n","            ball_similarities = rbf_kernel(x.reshape(1,-1), np.asarray([ball_at_x[kk][0] for kk in range(len(ball_at_x))]))[0]\n","            normalization = np.sum([ball_similarities[i] for i in range(len(ball_at_x))])\n","            score_one = np.sum([ball_similarities[i]*ball_at_x[i][1] for i in range(len(ball_at_x))])\n","            pred = score_one / normalization\n","            if pred >= 0.5:\n","                preds.append(1)\n","            else:\n","                preds.append(0)\n","        return preds\n","    def add_to_teaching(self, exam):\n","        self.teaching_set.append(exam)\n","\n","    def remove_teaching(self):\n","        self.teaching_set = self.teaching_set[:-1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ls9WCvv4JQ2_"},"source":["Evaluation code of prior and posterior rejector of human learner"]},{"cell_type":"code","metadata":{"id":"1859SmRvkKyb"},"source":["\n","def test_prior(net_hum, net_mach, epsilon, data_x, data_y):\n","    correct = 0\n","    correct_sys = 0\n","    exp = 0\n","    exp_total = 0\n","    total = 0\n","    real_total = 0\n","    alone_correct = 0\n","    prior_rejectors = []\n","    with torch.no_grad():\n","        inputs =  data_x\n","        labels = data_y\n","        m = net_mach(inputs)\n","        predicted_exp = torch.round(m.data)\n","        outputs = net_hum(inputs)\n","        predicted = torch.round(outputs.data)\n","        for i in range(len(inputs)):\n","            r_score = max(1 - outputs.data[i].item(), outputs.data[i].item())\n","            r = 0\n","            if r_score <  epsilon:\n","                r = 1\n","            else:\n","                r =  0\n","            prior_rejectors.append(r)\n","            if r == 1:\n","                exp += (predicted_exp[i] == labels[i]).item()\n","                correct_sys += (predicted_exp[i] == labels[i]).item()\n","                exp_total += 1\n","            elif r == 0:\n","                correct += (predicted[i] == labels[i]).item() \n","                correct_sys += (predicted[i] == labels[i]).item()\n","                total += 1\n","        real_total += labels.size(0)\n","    cov = str(total) + str(\" out of\") + str(real_total)\n","    to_print={\"coverage\":cov, \"system accuracy\": 100*correct_sys/real_total, \"expert accuracy\":100* exp/(exp_total+0.0002),\"classifier accuracy\":100*correct/(total+0.0001), \"alone classifier\": 100*alone_correct/real_total }\n","    #print(to_print)\n","    return to_print, prior_rejectors\n","\n","\n","def test_posterior(net_hum, net_mach, epsilon, knn_learner, data_x, data_y):\n","    correct = 0\n","    correct_sys = 0\n","    exp = 0\n","    exp_total = 0\n","    total = 0\n","    real_total = 0\n","    alone_correct = 0\n","    mistakes = []\n","    rejector_preds = []\n","    with torch.no_grad():\n","        inputs =  data_x\n","        labels = data_y\n","        m = net_mach(inputs)\n","        predicted_exp = torch.round(m.data)\n","        outputs = net_hum(inputs)\n","        predicted = torch.round(outputs.data)\n","        post_preds = knn_learner.predict(inputs.numpy())\n","        for i in range(len(inputs)):\n","            r = post_preds[i]\n","            if r == -1: # if no point in  ball\n","                r_score = max(1 - outputs.data[i].item(), outputs.data[i].item())\n","                if r_score <  epsilon:\n","                    r = 1\n","                else:\n","                    r =  0\n","            rejector_preds.append(r)\n","            if r == 1:\n","                exp += (predicted_exp[i] == labels[i]).item()\n","                correct_sys += (predicted_exp[i] == labels[i]).item()\n","                mistakes.append((predicted_exp[i] == labels[i]).item()*1.0)\n","                exp_total += 1\n","            elif r == 0:\n","                correct += (predicted[i] == labels[i]).item() \n","                correct_sys += (predicted[i] == labels[i]).item()\n","                mistakes.append((predicted[i] == labels[i]).item()*1.0)\n","                total += 1\n","        real_total += labels.size(0)\n","    cov = str(total) + str(\" out of\") + str(real_total)\n","    to_print={\"coverage\":cov, \"system accuracy\": 100*correct_sys/real_total, \"expert accuracy\":100* exp/(exp_total+0.0002),\"classifier accuracy\":100*correct/(total+0.0001), \"alone classifier\": 100*alone_correct/real_total }\n","    #print(to_print)\n","    return mistakes, to_print, rejector_preds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KwAHH2OkLudK"},"source":["Algorithm helpers"]},{"cell_type":"code","metadata":{"id":"bj6T9TVXLv9p"},"source":["def get_improvement_defer(current_defer_preds, opt_defer_preds, gammas, xs, ai_preds, hum_preds, truth):\n","    '''\n","    evaluates improvement for each data point if it were to be added\n","    '''\n","    error_improvements = []\n","    similarities_embeds_all = rbf_kernel(np.asarray(xs), np.asarray(xs))\n","    error_at_i = 0\n","    for i in range(len(gammas)):\n","        coin = random.random() # random number between [0,1]\n","        error_at_i = 0\n","\n","        similarities_embeds = similarities_embeds_all[i]\n","        for j in range(len(similarities_embeds)):\n","            if similarities_embeds[j] >= gammas[i]:\n","                error_hum = (hum_preds[j] == truth[j]) * 1.0 \n","                error_ai = (ai_preds[j] == truth[j]) * 1.0\n","                \n","                if opt_defer_preds[i] == 1:\n","                    if current_defer_preds[j] == 0:\n","                        error_at_i += error_ai - error_hum  \n","                else:\n","                    if current_defer_preds[j] == 1:\n","                        error_at_i +=   error_hum - error_ai\n","        error_improvements.append(error_at_i)\n","\n","        # get the ball for x\n","        # in this ball how many does the current defer not match the optimal\n","    return error_improvements"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xKAWG9NnJbyH"},"source":["Plotting code"]},{"cell_type":"code","metadata":{"id":"8LpCkx56Ojbl"},"source":["def mscatter(x,y,ax=None, m=None, **kw):\n","    import matplotlib.markers as mmarkers\n","    if not ax: ax=plt.gca()\n","    sc = ax.scatter(x,y,**kw)\n","    if (m is not None) and (len(m)==len(x)):\n","        paths = []\n","        for marker in m:\n","            if isinstance(marker, mmarkers.MarkerStyle):\n","                marker_obj = marker\n","            else:\n","                marker_obj = mmarkers.MarkerStyle(marker)\n","            path = marker_obj.get_path().transformed(\n","                        marker_obj.get_transform())\n","            paths.append(path)\n","        sc.set_paths(paths)\n","    return sc\n","def conv_to_color(arr):\n","    cols = []\n","    for a in arr:\n","        if a == 1:\n","            cols.append(\"blue\")\n","        else:\n","            cols.append(\"red\")\n","    return cols"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kPoL2tzeNAy4"},"source":["# Generate Data"]},{"cell_type":"code","metadata":{"id":"Cvv0cnxZkRxd"},"source":["\n","d = 2\n","total_samples = 500\n","mean_scale = 12\n","variance = 1.0\n","group_proportion = 0.5#np.random.uniform()\n","'''\n","if group_proportion <= 0.02:\n","    group_proportion = 0.02\n","if group_proportion >= 0.98:\n","    group_proportion = 0.98\n","#group_proportion = 0.4\n","'''\n","cluster1_mean = torch.rand(d)*mean_scale\n","cluster1_var = torch.tensor(variance)#torch.rand(d)*d\n","cluster1 = sample(\n","    cluster1_mean,\n","    cluster1_var,\n","    nb_samples= math.floor(total_samples * group_proportion * 0.5 )\n",")\n","cluster1_labels = torch.ones([math.floor(total_samples * group_proportion * 0.5 )], dtype=torch.long)\n","cluster2_mean = torch.rand(d)*mean_scale\n","cluster2_var = torch.tensor(variance)#torch.rand(d)*d\n","cluster2 = sample(\n","    cluster2_mean,\n","    cluster2_var,\n","    nb_samples= math.floor(total_samples * group_proportion * 0.5 )\n",")\n","cluster2_labels = torch.zeros([math.floor(total_samples * group_proportion * 0.5 )], dtype=torch.long)\n","cluster3_mean = torch.rand(d)*mean_scale\n","cluster3_var = torch.tensor(variance)#torch.rand(d)*d\n","cluster3 = sample(\n","    cluster3_mean,\n","    cluster3_var,\n","    nb_samples= math.floor(total_samples * (1-group_proportion) * 0.5 )\n",")\n","cluster3_labels = torch.ones([math.floor(total_samples * (1-group_proportion) * 0.5 )], dtype=torch.long)\n","\n","cluster4_mean = torch.rand(d)*mean_scale\n","cluster4_var = torch.tensor(variance)#torch.rand(d)*d\n","cluster4 = sample(\n","    cluster4_mean,\n","    cluster4_var,\n","    nb_samples= math.floor(total_samples * (1-group_proportion) * 0.5 )\n",")\n","cluster4_labels = torch.zeros([math.floor(total_samples * (1-group_proportion) * 0.5 )], dtype=torch.long)\n","\n","# test data\n","cluster1_test = sample(\n","    cluster1_mean,\n","    cluster1_var,\n","    nb_samples= math.floor(total_samples * group_proportion * 0.5 )\n",")\n","cluster1_labels_test = torch.ones([math.floor(total_samples * group_proportion * 0.5 )], dtype=torch.long)\n","\n","cluster2_test = sample(\n","    cluster2_mean,\n","    cluster2_var,\n","    nb_samples= math.floor(total_samples * group_proportion * 0.5 )\n",")\n","cluster2_labels_test = torch.zeros([math.floor(total_samples * group_proportion * 0.5 )], dtype=torch.long)\n","\n","cluster3_test = sample(\n","    cluster3_mean,\n","    cluster3_var,\n","    nb_samples= math.floor(total_samples * (1-group_proportion) * 0.5 )\n",")\n","cluster3_labels_test = torch.ones([math.floor(total_samples * (1-group_proportion) * 0.5 )], dtype=torch.long)\n","\n","cluster4_test = sample(\n","    cluster4_mean,\n","    cluster4_var,\n","    nb_samples= math.floor(total_samples * (1-group_proportion) * 0.5 )\n",")\n","cluster4_labels_test = torch.zeros([math.floor(total_samples * (1-group_proportion) * 0.5 )], dtype=torch.long)\n","\n","fig, ax = plt.subplots(1)\n","x1 = cluster1.numpy()\n","x2 = cluster2.numpy()\n","x3 = cluster3.numpy()\n","x4 = cluster4.numpy()\n","epsilon = 0.8\n","ax.set_facecolor('white')\n","#ax.set(xlim=(-4, 10), ylim=(-4, 10))\n","#ax.vlines([-12,-6,0,6,12.3],-12,12.3)\n","#ax.hlines([-12,-6,0,6,12.3],-12,12.3)\n","#ax.plot([x1h, x2h], [y1h, y2h], color='red', marker='x',label = \"human\")\n","#ax.plot([x1m, x2m], [y1m, y2m], color='blue', marker='x',label = \"machine\")\n","\n","scatter = mscatter(x1[:, 0], x1[:, 1],  cmap='RdBu',  ax=ax,s=100, label=\"human 0\")\n","scatter = mscatter(x2[:, 0], x2[:, 1], cmap='RdBu',  ax=ax,s=100, label=\"human 1\")\n","scatter = mscatter(x3[:, 0], x3[:, 1],  cmap='RdBu',  ax=ax,s=100, label=\"machine 0\")\n","scatter = mscatter(x4[:, 0], x4[:, 1],  cmap='RdBu',  ax=ax,s=100, label=\"machine 1\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O2oQ3oyhNEew"},"source":["# Obtain Human and AI predictors"]},{"cell_type":"markdown","metadata":{"id":"ANooNFWEPXEG"},"source":["get AI to be linear model on 2 clusters and human on the other 2 clusters"]},{"cell_type":"code","metadata":{"id":"9rTsLD6Zy9KX"},"source":["print(\"Obtaining AI\")\n","net_machine = Linear_net_sig(d)\n","data_x = torch.cat([cluster3, cluster4])\n","data_y = torch.cat([cluster3_labels, cluster4_labels])\n","run_classifier_sig(net_machine, data_x, data_y, 50000)\n","\n","print(\"Obtaining human\")\n","net_human = Linear_net_sig(d)\n","data_x = torch.cat([cluster1, cluster2])\n","data_y = torch.cat([cluster1_labels, cluster2_labels])\n","run_classifier_sig(net_human, data_x, data_y, 50000)\n","knn_learner = HumanLearner()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oxJD97U7Om9n"},"source":["# get line human\n","weights = net_human.fc1.weight.detach().numpy()[0]\n","bias = net_human.fc1.bias.detach().numpy()[0]\n","x1h =  -100\n","x2h = 100\n","y1h = -(1/weights[1])*(weights[0]*x1h + bias)\n","y2h = -(1/weights[1])*(weights[0]*x2h + bias)\n","\n","# get line machine\n","weights = net_machine.fc1.weight.detach().numpy()[0]\n","bias = net_machine.fc1.bias.detach().numpy()[0]\n","x1m =  -100\n","x2m = 100\n","y1m = -(1/weights[1])*(weights[0]*x1m + bias)\n","y2m = -(1/weights[1])*(weights[0]*x2m + bias)\n","\n","data_x = torch.cat([cluster1])\n","data_y = torch.cat([cluster1_labels])\n","mists1, _, _  = test_posterior(net_human, net_machine, epsilon, knn_learner, data_x, data_y)\n","\n","data_x = torch.cat([cluster2])\n","data_y = torch.cat([cluster2_labels])\n","mists2, _, _  = test_posterior(net_human, net_machine, epsilon, knn_learner, data_x, data_y)\n","\n","data_x = torch.cat([cluster3])\n","data_y = torch.cat([cluster3_labels])\n","mists3, _, _  = test_posterior(net_human, net_machine, epsilon, knn_learner, data_x, data_y)\n","\n","data_x = torch.cat([cluster4])\n","data_y = torch.cat([cluster4_labels])\n","mists4, _, _ = test_posterior(net_human, net_machine, epsilon, knn_learner, data_x, data_y)\n","\n","\n","fig, ax = plt.subplots(1)\n","\n","ax.set_facecolor('white')\n","ax.set(xlim=(-2, 13), ylim=(-2, 13))\n","#ax.vlines([-12,-6,0,6,12.3],-12,12.3)\n","#ax.hlines([-12,-6,0,6,12.3],-12,12.3)\n","ax.plot([x1h, x2h], [y1h, y2h], color='black', marker='x',label = \"human\")\n","ax.plot([x1m, x2m], [y1m, y2m], color='green', marker='x',label = \"machine\")\n","\n","\n","ax.scatter(x1[:, 0], x1[:, 1], c = conv_to_color(mists1), cmap='RdBu',marker ='o' )\n","ax.scatter(x2[:, 0], x2[:, 1], c = conv_to_color(mists2), cmap='RdBu',marker ='x' )\n","ax.scatter(x3[:, 0], x3[:, 1], c = conv_to_color(mists3), cmap='RdBu', marker ='o' )\n","ax.scatter(x4[:, 0], x4[:, 1], c = conv_to_color(mists4), cmap='RdBu', marker ='x' )\n","\n","#scatter = mscatter(x1[:, 0], x1[:, 1], c = mists1 , cmap='RdBu',  ax=ax,s=100, label=\"human 0\")\n","#scatter = mscatter(x2[:, 0], x2[:, 1], c = mists2 , cmap='RdBu',  ax=ax,s=100, label=\"human 1\")\n","#scatter = mscatter(x3[:, 0], x3[:, 1], c = mists3 , cmap='RdBu',  ax=ax,s=100, label=\"machine 0\")\n","#scatter = mscatter(x4[:, 0], x4[:, 1], c = mists4 , cmap='RdBu',  ax=ax,s=100, label=\"machine 1\")\n","\n","\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MfRE6iXpQhoQ"},"source":["test prior rejector with a given epsilon_prior rejector"]},{"cell_type":"code","metadata":{"collapsed":true,"id":"AsDpsQ2ITDnu","jupyter":{"outputs_hidden":true}},"source":["knn_learner = HumanLearner()\n","epsilon_prior = 0.9\n","data_x = torch.cat([cluster1, cluster2, cluster3, cluster4])\n","data_y = torch.cat([cluster1_labels, cluster2_labels, cluster3_labels, cluster4_labels])\n","to_print, _ = test_prior(net_human, net_machine, epsilon_prior, data_x, data_y)\n","print(to_print)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UJbgYvntNK5f"},"source":["# Run Teaching"]},{"cell_type":"code","metadata":{"id":"apFtFxgIzg3i"},"source":["data_x = torch.cat([cluster1, cluster2, cluster3, cluster4])\n","data_y = torch.cat([cluster1_labels, cluster2_labels, cluster3_labels, cluster4_labels])\n","data_x_np = data_x.numpy()\n","data_y_np = data_y.numpy()\n","outputs = net_human(data_x)\n","predicted_hum = torch.round(outputs.data).numpy()[:,0]\n","outputs = net_machine(data_x)\n","predicted_mach = torch.round(outputs.data).numpy()[:,0]\n","points_chosen = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_yXJjm1v-_tH"},"source":["get optimal deferall decision"]},{"cell_type":"code","metadata":{"id":"5v37yFF4t--B"},"source":["# get optimal deferall decision\n","opt_defer_teaching = []\n","emperical_deferall = True # compute optimal deferall based on distribution or emperical error\n","for ex in range(len(predicted_hum)):\n","    if not emperical_deferall:\n","        if ex < len(cluster1) + len(cluster2):\n","            opt_defer_teaching.append(0)\n","        else:\n","            opt_defer_teaching.append(1)\n","    else:\n","        error_hum = (predicted_hum[ex] == data_y[ex])\n","        error_ai = (predicted_mach[ex] == data_y[ex])\n","        if error_hum > error_ai:\n","            opt_defer_teaching.append(0)\n","        else:\n","            opt_defer_teaching.append(1)\n","# get optimal gammas\n","from tqdm import tqdm\n","optimal_gammas = []\n","with tqdm(total=len(opt_defer_teaching)) as pbar:\n","    for i in range(len(opt_defer_teaching)):\n","        # get all similarities\n","        opt_defer_ex = opt_defer_teaching[i]\n","        similarities_embeds = rbf_kernel(data_x_np[i].reshape(1,-1), data_x_np)[0]\n","        sorted_sim = sorted([(similarities_embeds[k], opt_defer_teaching[k]) for k in range(len(opt_defer_teaching))], key=lambda tup: tup[0])\n","        indicess = list(range(1, len(opt_defer_teaching)))\n","        indicess.reverse()\n","\n","        for k in indicess:\n","            if sorted_sim[k][1] == opt_defer_ex and sorted_sim[k- 1][1] != opt_defer_ex:\n","                optimal_gammas.append(sorted_sim[k][0])\n","                break\n","        pbar.update(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tdeb8HP8prUs"},"source":["def plot(indexx, human_learner, points_chosen):\n","    x1 = cluster1.numpy()\n","    x2 = cluster2.numpy()\n","    x3 = cluster3.numpy()\n","    x4 = cluster4.numpy()\n","    epsilon = 0.8\n","    def Extract(lst, indx): \n","        return [item[indx] for item in lst] \n","\n","    # get line human\n","    weights = net_human.fc1.weight.detach().numpy()[0]\n","    bias = net_human.fc1.bias.detach().numpy()[0]\n","    x1h =  -100\n","    x2h = 100\n","    y1h = -(1/weights[1])*(weights[0]*x1h + bias)\n","    y2h = -(1/weights[1])*(weights[0]*x2h + bias)\n","\n","    # get line machine\n","    weights = net_machine.fc1.weight.detach().numpy()[0]\n","    bias = net_machine.fc1.bias.detach().numpy()[0]\n","    x1m =  -100\n","    x2m = 100\n","    y1m = -(1/weights[1])*(weights[0]*x1m + bias)\n","    y2m = -(1/weights[1])*(weights[0]*x2m + bias)\n","\n","    data_x = torch.cat([cluster1])\n","    data_y = torch.cat([cluster1_labels])\n","    mists1, _, _ = test_posterior(net_human, net_machine, epsilon, human_learner, data_x, data_y)\n","\n","    data_x = torch.cat([cluster2])\n","    data_y = torch.cat([cluster2_labels])\n","    mists2, _, _ = test_posterior(net_human, net_machine, epsilon, human_learner, data_x, data_y)\n","\n","    data_x = torch.cat([cluster3])\n","    data_y = torch.cat([cluster3_labels])\n","    mists3, _, _ = test_posterior(net_human, net_machine, epsilon, human_learner, data_x, data_y)\n","\n","    data_x = torch.cat([cluster4])\n","    data_y = torch.cat([cluster4_labels])\n","    mists4, _, _ = test_posterior(net_human, net_machine, epsilon, human_learner, data_x, data_y)\n","\n","\n","    fig, ax = plt.subplots(1)\n","\n","    ax.set_facecolor('white')\n","    ax.set(xlim=(0, 15), ylim=(-2, 15))\n","    #ax.vlines([-12,-6,0,6,12.3],-12,12.3)\n","    #ax.hlines([-12,-6,0,6,12.3],-12,12.3)\n","    ax.plot([x1h, x2h], [y1h, y2h], color='black', marker='x',label = \"human\")\n","    ax.plot([x1m, x2m], [y1m, y2m], color='green', marker='x',label = \"machine\")\n","    ax.scatter(x1[:, 0], x1[:, 1], c = conv_to_color(mists1), cmap='RdBu',marker ='o' )\n","    ax.scatter(x2[:, 0], x2[:, 1], c = conv_to_color(mists2), cmap='RdBu',marker ='x' )\n","    ax.scatter(x3[:, 0], x3[:, 1], c = conv_to_color(mists3), cmap='RdBu', marker ='o' )\n","    ax.scatter(x4[:, 0], x4[:, 1], c = conv_to_color(mists4), cmap='RdBu', marker ='x' )\n","\n","    ax.scatter(Extract(points_chosen,0), Extract(points_chosen,1), label = \"points chosen\", marker = \"X\", color=\"green\",s=70 )\n","    gs = np.array(gammas)\n","    #gs = -np.log(expit(gs))\n","    gs = np.sqrt(-2*np.log(gs))\n","    for i in range(len(gs)):\n","        if i == 0:\n","            circle = plt.Circle((points_chosen[i][0], points_chosen[i][1]), gs[i], color='b', fill=False, label =\"radius\")\n","        else:\n","            circle = plt.Circle((points_chosen[i][0], points_chosen[i][1]), gs[i], color='b', fill=False)\n","\n","        ax.add_patch(circle)\n","\n","    plt.legend()\n","    #plt.savefig(\"figure\"+str(indexx)+\".pdf\")\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B8yqdjnbsGOm"},"source":["knn_learner = HumanLearner()\n","points_chosen = []\n","epsilon = 0.9\n","RESOLUTION = 1\n","MAX_SIZE = 5 # size of teaching set\n","MAX_TRIALS = 1\n","_, prior_preds = test_prior(net_human, net_machine, epsilon, data_x, data_y)\n","data_sizes  = []\n","indices_used = []\n","test_errors = [[] for _ in range(MAX_TRIALS)]\n","gammas = []\n","plot(1, knn_learner, points_chosen)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xER4KcAiBKfs"},"source":["\n","for itt in range(MAX_SIZE):\n","    best_index = -1\n","    best_value = 0\n","    if itt % RESOLUTION == 0:\n","        print(\"###########################\")\n","        print(\"evaluating at size \" + str(itt) )\n","        _, metrics, _b = test_posterior(net_human, net_machine, epsilon, knn_learner, data_x, data_y)\n","        print(metrics)\n","        print(\"###########################\")\n","        test_errors[0].append(metrics[\"system accuracy\"])\n","        data_sizes.append(itt)\n","\n","    if itt == 0:\n","        preds_teach = prior_preds\n","    else:\n","        _a, _b, preds_teach = test_posterior(net_human, net_machine, epsilon, knn_learner, data_x, data_y)\n","\n","    error_improvements = get_improvement_defer(preds_teach, opt_defer_teaching, optimal_gammas, data_x_np, predicted_mach, predicted_hum, data_y_np )\n","    print(f'index {np.argmax(error_improvements)} val {max(error_improvements)}')\n","    best_index = np.argmax(error_improvements)\n","    indices_used.append(best_index) # add found element to set used\n","    repr_x = data_x_np[best_index]\n","    target = data_y_np[best_index] \n","    points_chosen.append(repr_x)\n","\n","    knn_learner.add_to_teaching((repr_x, opt_defer_teaching[best_index], optimal_gammas[best_index]))\n","    gammas.append(optimal_gammas[best_index])\n","    points_chosen = [list(pp) for pp in points_chosen]\n","    plot(1, knn_learner, points_chosen)\n","_, metrics, _b = test_posterior(net_human, net_machine, epsilon, knn_learner, data_x, data_y)\n","print(metrics)\n"],"execution_count":null,"outputs":[]}]}