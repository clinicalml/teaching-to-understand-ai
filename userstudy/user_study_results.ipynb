{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import math\n",
    "import numpy as np\n",
    "import argparse\n",
    "from __future__ import print_function\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from scipy.stats import multivariate_normal\n",
    "import  scipy.stats as st\n",
    "from matplotlib import cm\n",
    "from __future__ import print_function\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilities for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    prediction_tokens = normalize_answer(prediction).split()\n",
    "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truth):\n",
    "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
    "\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_truth_pred(truths, preds):\n",
    "    '''\n",
    "    truths, preds: matched arrays of ground truth answers and predictions\n",
    "    '''\n",
    "    f1 = exact_match = total = 0\n",
    "    array_f1 = []\n",
    "    for i in range(len(truths)):\n",
    "        total += 1\n",
    "        if truths[i] in ['yes', \"no\"]:\n",
    "            continue\n",
    "        ground_truths = [truths[i]]\n",
    "        prediction = preds[i]\n",
    "        exact_match += metric_max_over_ground_truths(\n",
    "            exact_match_score, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(\n",
    "            f1_score, prediction, ground_truths)\n",
    "        array_f1.append(metric_max_over_ground_truths(\n",
    "            f1_score, prediction, ground_truths))\n",
    "    exact_match = 100.0 * exact_match / (total+ 0.00000000001)\n",
    "    f1 = 100.0 * f1 / (total+ 0.00000000001)\n",
    "\n",
    "    return {'exact_match': exact_match, 'f1': f1, 'array_f1':array_f1}\n",
    "\n",
    "\n",
    "def get_answer( model, tokenizer, context, question):\n",
    "    # 1. TOKENIZE THE INPUT\n",
    "    # note: if you don't include return_tensors='pt' you'll get a list of lists which is easier for\n",
    "    # exploration but you cannot feed that into a model.\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "    # 2. OBTAIN MODEL SCORES\n",
    "    # the AutoModelForQuestionAnswering class includes a span predictor on top of the model.\n",
    "    # the model returns answer start and end scores for each word in the text\n",
    "    answer_start_scores, answer_end_scores = model(**inputs, return_dict=False)\n",
    "    answer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n",
    "    # 3. GET THE ANSWER SPAN\n",
    "    # once we have the most likely start and end tokens, we grab all the tokens between them\n",
    "    # and convert tokens back to words!\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "    return answer\n",
    "# https://huggingface.co/transformers/migration.html\n",
    "def model_evaluation(model, tokenizer, questions, contexts, answers, to_print = False):\n",
    "    preds = []\n",
    "    my_list = list(range(len(questions)))\n",
    "    with tqdm(total=len(my_list)) as pbar:\n",
    "        for ex in range(len(questions)):\n",
    "            answer = get_answer(model, tokenizer, contexts[ex],questions[ex])\n",
    "            preds.append(answer)\n",
    "            if ex % 100 == 0 and to_print:\n",
    "                print(\"context \" +contexts[ex] )\n",
    "                print(\"quest \" +questions[ex] )\n",
    "                print(\"truth \" +answers[ex]['text'] )\n",
    "                print(\"pred \" + answer)\n",
    "            pbar.update(1)\n",
    "    truths = [answers[i]['text'] for i in range(len(answers))]\n",
    "    scores = evaluate_truth_pred(truths, preds)\n",
    "    print(scores)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def get_conf_interval(arr):\n",
    "    alpha_level = 0.95\n",
    "    err  = st.t.interval(alpha_level, len(arr)-1, loc=np.mean(arr), scale=st.sem(arr))[1]/2  - st.t.interval(alpha_level, len(arr)-1, loc=np.mean(arr), scale=st.sem(arr))[0]/2 \n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_saved = pickle.load(open(\"data_saved.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tasks = data_saved['tasks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No-Teaching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_0 = data_saved['responses_0']\n",
    "all_answers_0 = []\n",
    "all_truths_0 = []\n",
    "all_anwers_defer_0 = []\n",
    "all_truths_defer_0 = []\n",
    "all_truths_human_0 = []\n",
    "all_answers_human_0 = []\n",
    "times_0 = []\n",
    "all_defers_b_0 = []\n",
    "test_times_0 = []\n",
    "defers_0 = []\n",
    "for i in range(len(responses)):\n",
    "    #completed_task\n",
    "    #task_id = int(responses[i]['task_id'].split('task')[1])\n",
    "    if int(responses_0[i]['completed_task']) != 0:\n",
    "        raw_task = tasks[responses_0[i]['task_id']]\n",
    "        task_as = raw_task['test_as']\n",
    "        task_ai_as = raw_task['test_ai_as']\n",
    "        hum_answers = responses_0[i]['test_user_answers']\n",
    "        hum_defer = responses_0[i]['test_user_defers']\n",
    "        hum_ans = []\n",
    "        hum_ans_defer = []\n",
    "        truth_defer = []\n",
    "        truth_all = []\n",
    "        time = 0\n",
    "        teach_pressed_times = responses_0[i]['teach_pressed_times']\n",
    "        test_pressed_times = responses_0[i]['test_pressed_times']\n",
    "        if responses_0[i]['completed_task'] == 0:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        \n",
    "        for j in range(len(hum_answers)):\n",
    "            truth_all.append(task_as[j])\n",
    "            if hum_defer[j] == 0:\n",
    "                hum_ans.append(hum_answers[j])\n",
    "            else:\n",
    "                hum_ans.append(task_ai_as[j])\n",
    "                hum_ans_defer.append(task_ai_as[j])\n",
    "                truth_defer.append(task_as[j])\n",
    "        overall_f1 = evaluate_truth_pred(hum_ans, truth_all)['f1']\n",
    "\n",
    "\n",
    "        #print(responses[i]['user_lessons'])\n",
    "        \n",
    "        for i in range(1,len(teach_pressed_times)):\n",
    "            interval = teach_pressed_times[i] - teach_pressed_times[i-1]\n",
    "            if interval/60/1000 <= 4:\n",
    "                time += teach_pressed_times[i] - teach_pressed_times[i-1]\n",
    "\n",
    "        for i in range(1,len(test_pressed_times)):\n",
    "            interval = test_pressed_times[i] - test_pressed_times[i-1]\n",
    "\n",
    "            if interval/60/1000 <= 4:\n",
    "                test_times_0.append(interval/60/1000)\n",
    "                time += test_pressed_times[i] - test_pressed_times[i-1]\n",
    "        times_0.append(time/60/1000)\n",
    "        \n",
    "\n",
    "\n",
    "        for j in range(len(hum_answers)):\n",
    "            f1_ai = metric_max_over_ground_truths(f1_score, task_as[j], [task_ai_as[j]])\n",
    "            if hum_defer[j] == 0:\n",
    "                if f1_ai <= 0.5:\n",
    "                    all_defers_b_0.append(1)\n",
    "                else:\n",
    "                    all_defers_b_0.append(0)#-1)\n",
    "            else:        \n",
    "                if f1_ai >= 0.5:\n",
    "                    all_defers_b_0.append(1)\n",
    "                else:\n",
    "                    all_defers_b_0.append(0)#-1)\n",
    "                \n",
    "        for j in range(len(hum_answers)):\n",
    "            \n",
    "            truth_all.append(task_as[j])\n",
    "            defers_0.append( hum_defer[j])\n",
    "            if hum_defer[j] == 0:\n",
    "                hum_ans.append(hum_answers[j])\n",
    "                all_truths_human_0.append(task_as[j])\n",
    "                all_answers_human_0.append(hum_answers[j])\n",
    "            else:\n",
    "                hum_ans.append(task_ai_as[j])\n",
    "                hum_ans_defer.append(task_ai_as[j])\n",
    "                truth_defer.append(task_as[j])\n",
    "                all_anwers_defer_0.append(task_as[j])\n",
    "                all_truths_defer_0.append(task_ai_as[j])\n",
    "                \n",
    "            all_answers_0.append(hum_ans[j])\n",
    "            all_truths_0.append(task_as[j])\n",
    "    \n",
    "        a = evaluate_truth_pred(hum_ans, truth_all)['f1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"System F1\")\n",
    "print(evaluate_truth_pred(all_answers_0, all_truths_0)['f1'])\n",
    "print(get_conf_interval(evaluate_truth_pred(all_answers_0, all_truths_0)['array_f1']))\n",
    "print(\"Defer F1\")\n",
    "print(evaluate_truth_pred(all_anwers_defer_0, all_truths_defer_0)['f1'])\n",
    "print(get_conf_interval(evaluate_truth_pred(all_anwers_defer_0, all_truths_defer_0)['array_f1']))\n",
    "print(\"Non-Defer F1\")\n",
    "print(evaluate_truth_pred(all_answers_human_0, all_truths_human_0)['f1'])\n",
    "print(get_conf_interval(evaluate_truth_pred(all_answers_human_0, all_truths_human_0)['array_f1']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Teaching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses_1 = data_saved['responses_1']\n",
    "all_answers_1 = []\n",
    "all_truths_1 = []\n",
    "all_anwers_defer_1 = []\n",
    "all_truths_defer_1 = []\n",
    "all_truths_human_1 = []\n",
    "all_answers_human_1 = []\n",
    "times_1 = []\n",
    "test_times_1 = []\n",
    "all_defers_b_1 = []\n",
    "teach_all_answers_1 = []\n",
    "teach_all_truths_1 = []\n",
    "teach_all_anwers_defer_1 = []\n",
    "teach_all_truths_defer_1 = []\n",
    "teach_all_truths_human_1 = []\n",
    "teach_all_answers_human_1 = []\n",
    "seen_all_answers_1 = []\n",
    "seen_all_truths_1 = []\n",
    "notseen_all_answers_1 = []\n",
    "notseen_all_truths_1 = []\n",
    "seen_defers = []\n",
    "notseen_defers = []\n",
    "defers_1 = []\n",
    "user_lessons = {}\n",
    "for i in range(len(responses_1)):\n",
    "    #completed_task\n",
    "    #task_id = int(responses[i]['task_id'].split('task')[1])\n",
    "    if int(responses_1[i]['completed_task']) != 0:\n",
    "        raw_task = tasks[responses_1[i]['task_id']]\n",
    "        task_as = raw_task['test_as']\n",
    "        task_ai_as = raw_task['test_ai_as']\n",
    "        hum_answers = responses_1[i]['test_user_answers']\n",
    "        hum_defer = responses_1[i]['test_user_defers']\n",
    "        teach_task_as = raw_task['teach_as']\n",
    "        teach_task_ai_as = raw_task['teach_ai_as']\n",
    "        teach_hum_answers = responses_1[i]['teach_user_answers']\n",
    "        teach_hum_defer = responses_1[i]['teach_user_defers']\n",
    "        test_clusters = raw_task['test_clusters']\n",
    "        teach_clusters = raw_task['teaching_clusters']\n",
    "        hum_ans = []\n",
    "        hum_ans_defer = []\n",
    "        truth_defer = []\n",
    "        truth_all = []\n",
    "        time = 0\n",
    "        teach_pressed_times = responses_1[i]['teach_pressed_times']\n",
    "        test_pressed_times = responses_1[i]['test_pressed_times']\n",
    "        if responses_1[i]['completed_task'] == 0:\n",
    "            continue\n",
    "            \n",
    "        for j in range(len(hum_answers)):\n",
    "            truth_all.append(task_as[j])\n",
    "            if hum_defer[j] == 0:\n",
    "                hum_ans.append(hum_answers[j])\n",
    "            else:\n",
    "                hum_ans.append(task_ai_as[j])\n",
    "                hum_ans_defer.append(task_ai_as[j])\n",
    "                truth_defer.append(task_as[j])\n",
    "        overall_f1 = evaluate_truth_pred(hum_ans, truth_all)['f1']\n",
    "\n",
    "        rand_id = np.random.randint(10000)\n",
    "        user_lessons[rand_id]= responses_1[i]['user_lessons']\n",
    "\n",
    "        #print(responses[i]['user_lessons'])\n",
    "        for i in range(1,len(teach_pressed_times)):\n",
    "            interval = teach_pressed_times[i] - teach_pressed_times[i-1]\n",
    "            if interval/60/1000 <= 4:\n",
    "                time += teach_pressed_times[i] - teach_pressed_times[i-1]\n",
    "\n",
    "        for i in range(1,len(test_pressed_times)):\n",
    "            interval = test_pressed_times[i] - test_pressed_times[i-1]\n",
    "\n",
    "            if interval/60/1000 <= 4:\n",
    "                test_times_1.append(interval/60/1000)\n",
    "                time += test_pressed_times[i] - test_pressed_times[i-1]\n",
    "        times_1.append(time/60/1000)\n",
    "        \n",
    "\n",
    "\n",
    "        for j in range(len(hum_answers)):\n",
    "            f1_ai = metric_max_over_ground_truths(f1_score, task_as[j], [task_ai_as[j]])\n",
    "            if hum_defer[j] == 0:\n",
    "                if f1_ai <= 0.5:\n",
    "                    all_defers_b_1.append(1)\n",
    "                else:\n",
    "                    all_defers_b_1.append(0)#-1)\n",
    "            else:        \n",
    "                if f1_ai >= 0.5:\n",
    "                    all_defers_b_1.append(1)\n",
    "                else:\n",
    "                    all_defers_b_1.append(0)#-1)\n",
    "                \n",
    "        for j in range(len(hum_answers)):\n",
    "            defers_1.append(hum_defer[j])\n",
    "            truth_all.append(task_as[j])\n",
    "            if hum_defer[j] == 0:\n",
    "                hum_ans.append(hum_answers[j])\n",
    "                all_truths_human_1.append(task_as[j])\n",
    "                all_answers_human_1.append(hum_answers[j])\n",
    "            else:\n",
    "                hum_ans.append(task_ai_as[j])\n",
    "                hum_ans_defer.append(task_ai_as[j])\n",
    "                truth_defer.append(task_as[j])\n",
    "                all_anwers_defer_1.append(task_as[j])\n",
    "                all_truths_defer_1.append(task_ai_as[j])\n",
    "            if test_clusters[j] in teach_clusters:\n",
    "                seen_all_answers_1.append(hum_ans[j])\n",
    "                seen_all_truths_1.append(task_as[j])\n",
    "                seen_defers.append(hum_defer[j])\n",
    "            else:\n",
    "                notseen_all_answers_1.append(hum_ans[j])\n",
    "                notseen_all_truths_1.append(task_as[j])\n",
    "                notseen_defers.append(hum_defer[j])\n",
    "            all_answers_1.append(hum_ans[j])\n",
    "            all_truths_1.append(task_as[j])\n",
    "            \n",
    "            \n",
    "\n",
    "        for j in range(len(teach_hum_answers)):\n",
    "\n",
    "            if teach_hum_defer[j] == 0:\n",
    "                teach_all_truths_human_1.append(teach_task_as[j])\n",
    "                teach_all_answers_human_1.append(teach_hum_answers[j])\n",
    "                teach_all_answers_1.append(teach_hum_answers[j])\n",
    "\n",
    "            else:\n",
    "\n",
    "                teach_all_anwers_defer_1.append(teach_task_as[j])\n",
    "                teach_all_truths_defer_1.append(teach_task_ai_as[j])\n",
    "                teach_all_answers_1.append(teach_task_ai_as[j])\n",
    "\n",
    "            teach_all_truths_1.append(teach_task_as[j])\n",
    "        a = evaluate_truth_pred(hum_ans, truth_all)['f1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Post-Teaching\")\n",
    "print(\"System F1\")\n",
    "print(evaluate_truth_pred(all_answers_1, all_truths_1)['f1'])\n",
    "print(get_conf_interval(evaluate_truth_pred(all_answers_1, all_truths_1)['array_f1']))\n",
    "print(\"Defer F1\")\n",
    "print(evaluate_truth_pred(all_anwers_defer_1, all_truths_defer_1)['f1'])\n",
    "print(get_conf_interval(evaluate_truth_pred(all_anwers_defer_1, all_truths_defer_1)['array_f1']))\n",
    "\n",
    "print(\"Non-Defer F1\")\n",
    "print(evaluate_truth_pred(all_answers_human_1, all_truths_human_1)['f1'])\n",
    "print(get_conf_interval(evaluate_truth_pred(all_answers_human_1, all_truths_human_1)['array_f1']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coverage\")\n",
    "print(np.mean(defers_1))\n",
    "print(get_conf_interval(defers_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treat1 = evaluate_truth_pred(all_answers_human_1, all_truths_human_1)['array_f1']\n",
    "treat0 = evaluate_truth_pred(all_answers_human_0, all_truths_human_0)['array_f1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.ttest_ind(treat1,treat0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'exact_match': 49.16666666666585, 'f1': 54.330939213291245}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"not seen clusters\")\n",
    "print(evaluate_truth_pred(notseen_all_answers_1, notseen_all_truths_1)['f1'])\n",
    "print(\" seen clusters\")\n",
    "print(evaluate_truth_pred(seen_all_answers_1, seen_all_truths_1)['f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treat1 = evaluate_truth_pred(notseen_all_answers_1, notseen_all_answers_1)['array_f1']\n",
    "treat0 = evaluate_truth_pred(seen_all_answers_1, seen_all_truths_1)['array_f1']\n",
    "print(stats.ttest_ind(treat1,treat0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non defer\")\n",
    "treat1 = evaluate_truth_pred(all_anwers_defer_1, all_truths_defer_1)['array_f1']\n",
    "treat0 = evaluate_truth_pred(all_anwers_defer_0, all_truths_defer_0)['array_f1']\n",
    "print(stats.ttest_ind(treat1,treat0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" defer\")\n",
    "treat1 = evaluate_truth_pred(all_answers_human_1, all_truths_human_1)['array_f1']\n",
    "treat0 = evaluate_truth_pred(all_answers_human_0, all_truths_human_0)['array_f1']\n",
    "print(stats.ttest_ind(treat1,treat0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"overall\")\n",
    "treat1 = evaluate_truth_pred(all_answers_1, all_truths_1)['array_f1']\n",
    "treat0 = evaluate_truth_pred(all_answers_0, all_truths_0)['array_f1']\n",
    "print(stats.ttest_ind(treat1,treat0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"defer\")\n",
    "treat1 = defers_0\n",
    "treat0 = defers_1\n",
    "print(stats.ttest_ind(treat1,treat0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
